@inbook{068f3f9559274570aa6d39bc11083297,
  title     = "Comparing Formal Specification Languages for Complex Reasoning Systems",
  author    = "{van Harmelen}, Frank and {Lopez de Mantaras}, Ramon and Jacek Malec and Jan Treur",
  year      = "1993",
  pages     = "257--282",
  editor    = "J. Treur and T. Wetter",
  booktitle = "Formal Specification of Complex Reasoning Systems",
  publisher = "Ellis Horwood",
}


@inbook{d3f461ae667a44b3a0efed919dd410f6,
  title     = "A Purpose Driven Method for Language Comparison",
  abstract  = "Current efforts to compare knowledge engineering (KE) modelling languages have been limited to either rather shallow comparisons on a broad-set of languages, or to detailed comparisons with limited applicability to a narrow set of languages. In this paper we propose a novel way of organising language comparisons. This method is based on an alternating decomposition of the goals that a language tries to achieve and the linguistic methods it empIoys to achieve these goals. This new method for comparing languages allows a general comparison at high levels of abstraction, while not preventing more precise comparisons whenever possible. One result of our comparison method is an insight in the different assumptions that underly the languages to be compared. Two further consequences follow from the proposed comparison method, namely (i) a measure for the degree of similarity between languages, and (ii) a method for translating between languages. After describing our method, we apply it to a pair of KE modelling languages, and show how it yields insights in the assumptions underlying the languages and how it can be used to produce a translation procedure between the languages.",
  author    = "F.M. Brazier and {van Harmelen}, F.A.H. and R. Straatman and J. Treur and N.J.E. Wijngaards and M. Willems",
  note      = "Brazier.ea:96*6",
  year      = "1996",
  doi       = "10.1007/3-540-61273-4_5",
  isbn      = "3540612734",
  volume    = "1076",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "66--81",
  editor    = "N. Shadbolt and K. O'Hara and A.Th. Schreiber",
  booktitle = "Advances in Knowledge Acquisition - 9th European Knowledge Acquisition Workshop, EKAW 1996, Proceedings",
}


@article{07c69e105ab345f7b31ed183db9d2718,
  title     = "Summary of the KEML'96 workshop",
  author    = "R. Benjamins and {van Harmelen}, F.A.H. and N.J.E. Wijngaards",
  note      = "Benjamins.ea:96 The Knowledge Engineering and Modelling Language's Workshop 1996",
  year      = "1996",
  doi       = "10.1017/S0269888900007839",
  volume    = "11",
  pages     = "193--195",
  journal   = "Knowledge Engineering Review",
  issn      = "0269-8889",
  publisher = "Cambridge University Press",
  number    = "2",
}


@article{76a11dc0f09f4423bb76bab0709a691d,
  title     = "Evaluating a formal KBS specification language",
  abstract  = "Formal specification languages can improve the development of knowledge-based systems (KBS), but several problems limit their usefulness. (ML)2, a formal language based on the knowledge models used in the CommonKADS KBS development method, avoids many of these problems. (ML)2 specifically aims at formalizing the CommonKADS expertise model. To analyze (ML)2 usability, a set of evaluation criteria was designed. A small case study was then performed, constructing and expertise model in (ML)2, to test and refine this criteria.",
  author    = "{van Harmelen}, Frank and Manfred Aben and Fidel Ruiz and {van de Plassche}, Joke",
  year      = "1996",
  month     = "2",
  doi       = "10.1109/64.482959",
  volume    = "11",
  pages     = "56--62",
  journal   = "IEEE expert",
  issn      = "0885-9000",
  publisher = "Institute of Electrical and Electronics Engineers Inc.",
  number    = "1",
}


@article{daf4c16d543c4ce0aec82cc5f52ece68,
  title     = "Structure-preserving specification languages for knowledge-based systems",
  abstract  = "Much of the work on validation and verification of knowledge based systems (KBSs) has been done in terms of implementation languages (mostly rule-based languages). Recent papers have argued that it is advantageous to do validation and verification in terms of a more abstract and formal specification of the system. However, constructing such formal specifications is a difficult task. This paper proposes the use of formal specification languages for KBS-development that are closely based on the structure of informal knowledge-models. The use of such formal languages has as advantages that (i) we can give strong support for the construction of a formal specification, namely on the basis of the informal description of the system; and (ii) we can use the structural correspondence to verify that the formal specification does indeed capture the informally stated requirements.",
  author    = "{Van Harmelen}, Frank and Manfred Aben",
  year      = "1996",
  month     = "2",
  doi       = "10.1006/ijhc.1996.0010",
  volume    = "44",
  pages     = "187--212",
  journal   = "International Journal of Human-computer Studies",
  issn      = "1071-5819",
  publisher = "Academic Press Inc.",
  number    = "2",
}


@article{702aad7d1b6b4abdabb3e49ebf7bd553,
  title     = "Using reflection techniques for flexible problem solving (with examples from diagnosis)",
  abstract  = "Flexible problem solving consists of the dynamic selection and configuration of problem solving methods for a particular problem type, depending on the particular problem and the goal of problem solving. In this paper, we propose an architecture that supports such flexible problem solving automatically. For this purpose, problem solving methods are described in a uniform way, by an abstract model of components, which together define the functionality of the methods. Such an abstract model is used for dynamic selection and configuration of the problem solving methods. The proposed architecture for flexible problem solving consists of well-known reflection techniques: two object-meta relations, a definable naming mechanism and the axiomhood and theoremhood reflection rules. We have succeeded in using standard meta-architecture techniques to enable flexible problem solving.",
  keywords  = "Applications of meta-reasoning, Architectures for meta-reasoning, Configuration of problem solving methods, Diagnosis",
  author    = "{Ten Teije}, Annette and {Van Harmelen}, Frank",
  year      = "1996",
  month     = "9",
  doi       = "10.1016/0167-739X(96)88794-2",
  volume    = "12",
  pages     = "217--234",
  journal   = "Future Generation Computer Systems",
  issn      = "0167-739X",
  publisher = "Elsevier",
  number    = "2-3 SPEC. ISS.",
}


@article{7482b42880d043558aa294350e9d0f7a,
  title     = "Applying rule-base anomalies to KADS inference structures",
  abstract  = "The literature on validation and verification of knowledge-based systems contains a catalouge of anomalies for knowledge-based systems, such as redundant, contradictory or deficient knowledge. Detecting such anomalies is a method for verifying knowledge-based systems. (continued)",
  keywords  = "validation, verfication, knowledge-based systems,",
  author    = "{van Harmelen}, F",
  year      = "1997",
  volume    = "21",
  pages     = "271--280",
  journal   = "Decision Support Systems",
  issn      = "0167-9236",
  publisher = "North Holland",
}


@article{b24000c856414830948c326f9708ff73,
  title     = "Formalisation for decision support in anaesthesiology",
  abstract  = "This paper reports on research for decision support for anaesthesiologists at the University Hospital in Groningen, the Netherlands. Based on CAROLA, an existing automated operation documentation system, we designed a support environment that will assist in real-time diagnosis. The core of the work presented here consists of a knowledge base (containing anaesthesiological knowledge) and a diagnosis system. The knowledge base is specified in the logic-based formal specification language AFSL. This leads to a powerful and precise treatment of knowledge structuring and data abstraction.",
  keywords  = "Anesthesiology, Artificial Intelligence, Body Surface Area, Body Temperature, Decision Support Techniques, Hematocrit, Humans, Models, Theoretical, Journal Article",
  author    = "{Renardel de Lavalette}, {G R} and R. Groenboom and E Rotterdam and {van Harmelen}, F and {ten Teije}, A and {de Geus}, F.",
  year      = "1997",
  month     = "11",
  volume    = "11",
  pages     = "189--214",
  journal   = "Artificial Intelligence in Medicine",
  issn      = "0933-3657",
  publisher = "Elsevier",
  number    = "3",
}


@article{6dd3f5c9499c462fab1d070bc644f5e2,
  title     = "Applying rule-base anomalies to KADS inference structures",
  abstract  = "The literature on validation and verification of knowledge-based systems contains a catalogue of anomalies for knowledge-based systems, such as redundant, contradictory or deficient knowledge. Detecting such anomalies is a method for verifying knowledge-based systems. Unfortunately, the traditional formulation of the anomalies in the literature is very specific to a rule-based knowledge representation, which greatly restricts their applicability. In this paper, we show how the traditional anomalies can be reinterpreted in terms of conceptual models (in particular KADS inference structures). For this purpose, we present a formalisation of KADS inference structures which enables us to apply the traditional rule-base anomalies to these inference structures. This greatly improves the usefulness of the anomalies, since they can now be applied to a much wider class of knowledge-based systems. Besides this reformulation and wider applicability of the traditional anomalies, further contributions of this paper are a novel formalisation of KADS inference structures and a number of improvements to the existing formalisation of the traditional anomalies.",
  keywords  = "Anomalies, Inference structures, Knowledge-based systems, Validation, Verification",
  author    = "{Van Harmelen}, Frank",
  year      = "1997",
  month     = "12",
  volume    = "21",
  pages     = "271--280",
  journal   = "Decision Support Systems",
  issn      = "0167-9236",
  publisher = "North Holland",
  number    = "4",
}


@article{d896c05d87e04e30adbac060c7b9303b,
  title     = "Formal support for development of knowledge-based systems",
  abstract  = "This article provides an approach for developing reliable knowledge-based systems. Its main contributions are: Specification is done at an architectural level that abstracts from a specific implementation formalism. The model of expertise of CommonKADS distinguishs different types of knowledge and describes their interaction. Our architecture refines this model and adds an additional level of formalization. The formal specification and verification system KIV is used for specifying and verifying such architectures. We have chosen KIV for four reasons: (1) it provides the formal means required for specifying the dynamics of knowledge-based systems (i.e., dynamic logic), (2) it provides compositional specifications, (3) it provides an interactive theorem prover, and (4) last but not least it comes with a sophisticated tool environment developed in several realistic application projects.",
  author    = "Dieter Fensel and {van Harmelen}, Frank and Wolfgang Reif and {ten Teije}, Annette",
  year      = "1998",
  pages     = "1--18",
  journal   = "Failure and Lessons …",
  issn      = "1088-128X",
  publisher = "Cognizant Communication Corporation",
}


@article{45834b0c245a47b1b63bd545cc2054ed,
  title     = "Formal Support for Development of Knowledge-Based Systems",
  abstract  = "This article provides an approach for developing reliable knowledge-based systems. Its main contributions are: Specification is done at an architectural level that abstracts from a specific implementation formalism. The model of expertise of CommonKADS distinguishs different types of knowledge and describes their interaction. Our architecture refines this model and adds an additional level of formalization. The formal specification and verification system KIV is used for specifying and verifying such architectures. We have chosen KIV for four reasons: (1) it provides the formal means required for specifying the dynamics of knowledge-based systems (i.e., dynamic logic), (2) it provides compositional specifications, (3) it provides an interactive theorem prover, and (4) last but not least it comes with a sophisticated tool environment developed in several realistic application projects.",
  keywords  = "Formal methods, Knowledge-based systems, Validation, Verification",
  author    = "Dieter Fensel and {Van Harmelen}, Frank and Wolfgang Reif and {Ten Teije}, Annette",
  year      = "1998",
  volume    = "2",
  pages     = "173--182",
  journal   = "Failure and Lessons …",
  issn      = "1088-128X",
  publisher = "Cognizant Communication Corporation",
  number    = "4",
}


@inbook{6ab2f7260ef1493f81a644c5346238e4,
  title     = "Specification of dynamics for knowledge-based systems",
  abstract  = "During the last years, a number of formal specification languages for knowledge-based systems have been developed. Characteristic for knowledge-based systems are a complex knowledge base and an inference engine which uses this knowledge to solve a given problem. Specification languages for knowledge-based systems have to cover both aspects: they have to provide means to specify a complex and large amount of knowledge and they have to provide means to specify the dynamic reasoning behaviour of a knowledge-based system. This paper will focus on the second aspect, which is an issue considered to be unsolved. For this purpose, we have surveyed existing approaches in related areas of research. We have taken approaches for the specification of information systems (i.e., Language for Conceptual Modelling and Troll), approaches for the specification of database updates and the dynamics of logic programs (Transaction Logic and Dynamic Database Logic), and the approach of Abstract State Machines.",
  author    = "{Van Eck}, Pascal and Joeri Engelfriet and Dieter Fensel and {Van Harmelen}, Frank and Yde Venema and Mark Willems",
  year      = "1998",
  isbn      = "3540653058",
  volume    = "1472",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "37--68",
  booktitle = "Transactions and Change in Logic Databases - International Seminar on Logic Databases and the Meaning of Change and ILPS 1997 Post-Conference Workshop on (Trans)Actions and Change in Logic Programming and Deductive Databases, DYNAMICS 1997, Invited Surveys and Selected Papers",
}


@article{b4cbf2d229bc4a48b95f11cb0062e82c,
  title     = "Construction of problem-solving methods as parametric design",
  abstract  = "The knowledge-engineering literature contains a number of approaches for constructing or selecting problem solvers. Some of these approaches are based on indexing and selecting a problem solver from a library, others are based on a knowledge acquisition process, or are based on search-strategies. None of these approaches sees constructing a problem solver as a configuration task that could be solved with an appropriate configuration method. We introduce a representation of the functionality of problem-solving methods that allows us to view the construction of problem solvers as a configuration problem, and specifically as a parametric design problem. From the available methods for parametric design, we use propose-critique-modify for the automated configuration of problem-solving methods. We illustrate this approach by a scenario in a small car domain example.",
  author    = "{Ten Teije}, A. and {Van Harmelen}, F. and Schreiber, {A. Th} and Wielinga, {B. J.}",
  year      = "1998",
  month     = "10",
  volume    = "49",
  pages     = "363--389",
  journal   = "International Journal of Human-computer Studies",
  issn      = "1071-5819",
  publisher = "Academic Press Inc.",
  number    = "4",
}


@article{a66b3c850b4c4b7f86cdc8cd854626f2,
  title     = "A study of PROforma, a development methodology for clinical procedures",
  abstract  = "Knowledge engineering has shown that besides the general methodologies from software engineering it is useful to develop special purpose methodologies for knowledge based systems (KBS). PROforma is a newly developed methodology for a specific type of knowledge based systems. PROforma is intended for decision support systems and in particular for clinical procedures in the medical domain. This paper reports on an evaluation study of PROforma, and on the trade-off that is involved between general purpose and special purpose development methods in Knowledge Engineering and Medical AI. Our method for evaluating PROforma is based on re-engineering a realistic system in two methodologies: the new and special purpose KBS methodology PROforma and the widely accepted, and more general KBS methodology CommonKADS. The four most important results from our study are as follows. Firstly, PROforma has some strong points which are also strong related to requirements of medical reasoning. Secondly, PROforma has some weak points, but none of them are in any way related to the special purpose nature of PROforma. Thirdly, a more general method like CommonKADS works better in the analysis phase than the more special purpose method PROforma. Finally, to support a complementary use of the methodologies, we propose a mapping between their respective languages.",
  author    = "A. Vollebregt and {ten Teije}, A.C.M. and {van Harmelen}, F.A.H. and M. Mosseveld and {van der Lei}, J.",
  year      = "1999",
  doi       = "10.1016/S0933-3657(99)00016-0",
  volume    = "17",
  pages     = "195--221",
  journal   = "Artificial Intelligence in Medicine",
  issn      = "0933-3657",
  publisher = "Elsevier",
}


@inbook{6e9811354906411c922ecd9efcc7357d,
  title     = "Describing Problem Solving Methods using Anytime Performance profiles",
  author    = "{ten Teije}, A.C.M. and {van Harmelen}, F.A.H.",
  year      = "1999",
  booktitle = "Proceedings of the IJCAI'99 Workshop on Ontologies and Problem Solving Methods, 1999",
}


@article{e07551b6d2fc42a0a550841929623a9d,
  title     = "Formally verifying dynamic properties of KBS",
  author    = "P. Groot and {ten Teije}, A.C.M. and {van Harmelen}, F.A.H.",
  note      = "Proceedings title: Proceedings of the 11th European Workshop on Knowledge Acquisition, Modeling, and Management (EKAW99)",
  year      = "1999",
  volume    = "1621",
  pages     = "157--172",
  journal   = "Lecture Notes in Computer Science",
  issn      = "0302-9743",
  publisher = "Springer Verlag",
}


@inbook{ef03bb2b7c734d5faa3854cd3d8f70e7,
  title     = "Practical Knowledge Representation for the Web.",
  author    = "{van Harmelen}, F.A.H. and D.A. Fensel",
  year      = "1999",
  booktitle = "In Proceedings of the Workshop on Intelligent Information Integration (III99) during IJCAI-99, Stockholm, Sweden, August 1999.",
}


@inbook{8c393c3e396248beb8b440e5eb9b0a77,
  title     = "Practical Knowledge Representation for the Web.",
  author    = "{van Harmelen}, F.A.H. and D.A. Fensel",
  year      = "1999",
  booktitle = "Proceedings of the IJCAI'99 Workshop on Intelligent Information Integration{"}, 1999",
}


@article{8487c8b823404e36948f2d8cef8bae21,
  title     = "WebMaster: Knowledge-based Verification of Web-pages",
  author    = "{van Harmelen}, F.A.H. and {van der Meer}, J.",
  note      = "Proceedings title: Twelfth International Conference on Industrial & Engineering Applications of Artificial Intelligence & Expert Systems IEA/AIE'99",
  year      = "1999",
  journal   = "Lecture Notes in Computer Science",
  issn      = "0302-9743",
  publisher = "Springer Verlag",
}


@inbook{2f1bbce2d1864657a800da7b06893a22,
  title     = "{"}Anytime Diagnostic Reasoning using Approximate Boolean Constraint Propagation.",
  author    = "A.S. Verberne and {van Harmelen}, F.A.H. and {ten Teije}, A.C.M.",
  year      = "2000",
  editor    = "F. Giunchiglia and B. Selman",
  booktitle = "Proceedings of the Seventh Fifth International Conference on Principles of Knowledge Representation and Reasoning ({KR'00}).",
  publisher = "KR Inc.",
}


@inbook{cdc0d89208c44693a1382ba8e53adf8f,
  title     = "Describing Problem Solving Methods using Anytime Performance profiles.",
  author    = "{van Harmelen}, F.A.H. and {ten Teije}, A.C.M.",
  year      = "2000",
  editor    = "W. Horn",
  booktitle = "Proceedings of 14th European Conference on AI, ECAI'00.",
  publisher = "IOS Press",
}


@inbook{594679e8aad64a948799f257863ad1a9,
  title     = "Knowledge-Based Meta-Data Validation: Analyzing a Web-Based Information System.",
  author    = "{van Harmelen}, F.A.H. and A. Kampman and H. Stuckenschmidt and T. Vogele",
  year      = "2000",
  editor    = "K. Greve",
  booktitle = "Fourtheenth International Symposium Informatics for Environmental Protection.",
  publisher = "German Computer Society",
}


@inbook{8c9d10ad7da54fc4872261098bf76a4f,
  title     = "Knowledge Representation on the Web",
  author    = "S. Decker and D.A. Fensel and {van Harmelen}, F.A.H. and I. Horrocks and S. Melnik and M. Klein and J. Broekstra",
  year      = "2000",
  editor    = "F. Baader",
  booktitle = "International Workshop on Description Logics ({DL}'00).",
  publisher = "University of Aachen",
}


@inbook{cf1ddbe918a748d6863ef9adfa82c9f1,
  title     = "Knowledge Representation on the Web.",
  author    = "D.A. Fensel and {van Harmelen}, F.A.H. and I. Horrocks and S. Melnik and M.C.A. Klein and J. Broekstra",
  year      = "2000",
  booktitle = "In Proceedings of the 2000 International Workshop on Description Logics (DL2000):89-97.",
}


@inbook{316fd65572bf405190f0e1ba8e4dfd35,
  title     = "Maintenance of KBS’s by domain experts the holy grail in practice",
  abstract  = "Enabling a domain expert to maintain his own knowledge in a Knowledge Based System has long been an ideal for the Knowledge Engineering community. In this paper we report on our experience with trying to achieve this ideal in a practical setting, by building a maintenance tool for an existing KBS. After a brief survey of various approaches to this problem described in literature, we select a domain-and task-specific modelling approach as the most promising and appropriate. First, we construct a domain ontology and a task model for the KBS system to be maintained, as well as a task analysis of the maintenance tool itself. The maintenance tool is subsequently implemented using a two layer architecture which seperates domain and system concepts. Although no full-scale evaluation has been undertaken, we report on our initial experience with this approach and present our conclusions.",
  author    = "Arne Bultman and Joris Kuipers and {Van Harmelen}, Frank",
  year      = "2000",
  doi       = "10.1007/3-540-45049-1_17",
  isbn      = "3540676899",
  volume    = "1821",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "139--149",
  booktitle = "Intelligent Problem Solving: Methodologies and Approaches - 13th International Conference on Industrial and Engineering Applications of Artificial Intelligence and Expert Systems, IEA/AIE 2000, Proceedings",
}


@inbook{9bf7b60928bd4a53a1f916beea7507f9,
  title     = "Maintenance of KBS's by Domain Experts: The Holy Grail in Practice.",
  author    = "{van Harmelen}, F.A.H. and J. Kuipers and A. Bultman",
  year      = "2000",
  editor    = "R. Logananthara and G. Palm and M. Ali",
  booktitle = "Thirtheenth International Conference on Industrial & Engineering Applications of Artificial Intelligence & Expert Systems IEA/AIE'00",
  publisher = "Springer Verlag",
}


@inbook{30ba82d8f9194084abb8a0986017eab9,
  title     = "OIL & UPML: A Unifying Framework for the Knowledge Web.",
  author    = "D.A. Fensel and M. Crubezy and {van Harmelen}, F.A.H. and I. Horrocks",
  year      = "2000",
  pages     = "4.1--4.12",
  booktitle = "Proceedings of the Workshop on Applications of Ontologies and Problem-solving Methods, 14th European Conference on Artificial Intelligence ECAI’00",
}


@inbook{e31cfed642aa4ba2b6531f79f2b63078,
  title     = "OIL and UPML: a unifying framework for the Knowledge Web.",
  author    = "D.A. Fensel and M. Cruzeby and {van Harmelen}, F.A.H. and I. Horrocks",
  year      = "2000",
  editor    = "R. Benjamins",
  booktitle = "Proceedings of the {ECAI}'00 workshop on application of ontologies and problem-solving methods.",
  publisher = "ECCAI",
}


@inbook{2795b64e6f7047bc9cbabf078152e1bb,
  title     = "OIL in a nutshell.",
  author    = "D.A. Fensel and I. Horrocks and {van Harmelen}, F.A.H. and S. Decker and M. Erdmann and M.C.A. Klein",
  year      = "2000",
  editor    = "R. Dieng",
  booktitle = "Proceedings of the 12th European Workshop on Knowledge Acquisition, Modeling, and Management ({EKAW}'00).",
  publisher = "Springer-Verlag",
}


@inbook{fe092d68f44e4369b2f1acdbdb25f183,
  title     = "OIL in a nutshell.",
  author    = "D.A. Fensel and I. Horrocks and {van Harmelen}, F.A.H. and S. Decker and M. Erdmann and M.C.A. Klein",
  note      = "Gebeurtenis: European Knowledge Acquisition Conference (EKAW-2000)",
  year      = "2000",
  series    = "Lecture Notes in Artificial Intelligence, LNAI",
  publisher = "Springer-Verlag",
  pages     = "1--16",
  editor    = "R. Dieng",
  booktitle = "Knowledge Acquisition, Modeling, and Management, Proceedings of the European Knowledge Acquisition Conference (EKAW-2000).",
}


@inbook{700a5a2579ea463ca3f0126d041a3127,
  title     = "On-To-Knowledge: Ontology-based Tools for Knowledge Management",
  author    = "D.A. Fensel and {van Harmelen}, F.A.H. and M. Klein and J.M. Akkermans and J. Broekstra and C. Fluit and {van der Meer}, J. and H.-P. Schnurr and R. Studer and J. Hughes and U. Krohn and J. Davies and R. Engels and B. Bremdal and F. Ygge and T. Lau and B. Novotny and U. Reimer and I. Horrocks",
  year      = "2000",
  booktitle = "eBusiness and eWork",
}


@inbook{24b23fc6df9140f6ac4c8ab8cadd0869,
  title     = "OnToKnowledge: Ontology-based Tools for Knowledge Management.",
  author    = "D.A. Fensel and {van Harmelen}, F.A.H. and J.M. Akkermans and M.C.A. Klein and J. Broekstra and C. Fluit and {van der Meer}, J. and H.-P. Schnurr and R. Studer and J. Davies and J. Hughes and U. Krohn and R. Engels and B. Bremdahl and F. Ygge and U. Reimer and I. Horrocks",
  year      = "2000",
  booktitle = "Proceedings of the eBusiness and eWork 2000 (EMMSEC 2000) Conference.",
}


@inbook{58b2ebd56e954255a09dca030270641c,
  title     = "Ontology-based Tools for Knowledge Management.",
  author    = "D.A. Fensel and {van Harmelen}, F.A.H. and J.M. Akkermans and M. Klein and J. Broekstra and C. Fluit and {van der Meer}, J. and H.-P. Schnurr and R. Studer and J. Davies and J. Hughes and U. Krohn and R. Engels and B. Bremdahl and F. Ygge and U. Reimer and I. Horrocks",
  year      = "2000",
  pages     = "1105--1110",
  booktitle = "Proceedings of the eBusiness and eWork 2000 (EMMSEC 2000) Conference.",
}


@article{ec54b2c793dd4e0abdb28b49ed4541c3,
  title     = "Questions and answers OIL.",
  author    = "{van Harmelen}, F.A.H. and I. Horrocks",
  year      = "2000",
  volume    = "15",
  pages     = "69--72",
  journal   = "IEEE Intelligent Systems",
  issn      = "1541-1672",
  publisher = "Institute of Electrical and Electronics Engineers Inc.",
  number    = "6",
}


@article{24590824e19943c092430458283ff48b,
  title     = "The DARPA agent markup language",
  abstract  = "The DARPA Agent Markup Language (DAML) program is a United States government sponsored endeavor aimed at providing the foundation for the next web evolution – the semantic web. The program is funding critical research to develop languages, tools and techniques for making considerably more of the content on the web machine-understandable. We believe that this will lead to the next major generation of web technology, and will enable considerably more “machine to machine” (agent-based) communication. The program includes participation from academic researchers, government agencies, software development companies, and industrial organizations such as the World Wide Web consortium (W3C). The DAML project is also working closely with other efforts, including European Union funded Semantic Web projects (e.g. On-to-Knowledge[3] and Ibrow[4]), and the ongoing W3C RDF recommendation effort[5]. In the remainder of this report, we provide a short motivation, description, and status report about the program.",
  author    = "O Lassila and {van Harmelen}, F and I. Horrocks and J. Hendler and DL McGuinness",
  year      = "2000",
  volume    = "15",
  pages     = "67--73",
  journal   = "IEEE Intelligent Systems & their Applications",
  issn      = "1094-7167",
  publisher = "Institute of Electrical and Electronics Engineers Inc.",
  number    = "6",
}


@inbook{c2da169ab78c4e2e9f4dfe4cb7fc6ef3,
  title     = "The relation between ontologies and schema-languages: Translating OIL-specifications in XML-schema.",
  author    = "{van Harmelen}, F.A.H. and M. Klein and D.A. Fensel and I. Horrocks",
  year      = "2000",
  editor    = "R. Benjamins",
  booktitle = "Proceedings of the {ECAI}'00 workshop on applications of ontologies and problem-solving methods.",
  publisher = "ECCAI",
}


@inbook{4da4b8b93a4f4fd79783aba36ef46cb6,
  title     = "The relation between ontologies and schema-languages: Translating OIL-specifications in XML Schema.",
  author    = "M.C.A. Klein and D.A. Fensel and {van Harmelen}, F.A.H. and I. Horrocks",
  year      = "2000",
  pages     = "7.1--7.12",
  booktitle = "Proceedings of the Workshop on Applications of Ontologies and Problem-solving Methods, 14th European Conference on Artificial Intelligence ECAI’00",
}


@article{d297fe9213f24ac0a3b435eef68dfe7b,
  title     = "The semantic Web and its languages",
  abstract  = "The Web has drastically changed the availability of electronic information, but its success and exponential growth have made it increasingly difficult to find, access, present and maintain such information for a wide variety of users. In reaction to this bottleneck many new research initiatives and commercial enterprises have been set up to enrich available information with machine-processable semantics. The paper considers how the semantic Web will provide intelligent access to heterogeneous and distributed information, enabling software products (agents) to mediate between user needs and available information sources. The paper discusses the Resource Description Framework, XML and other languages",
  author    = "Ora Lassila and {van Harmelen}, F. and Ian Horrocks and Hendler, {James A.} and McGuinness, {Deborah L}",
  year      = "2000",
  doi       = "10.1109/5254.895864",
  volume    = "15",
  pages     = "67--73",
  journal   = "IEEE Intelligent Systems & their Applications",
  issn      = "1094-7167",
  publisher = "Institute of Electrical and Electronics Engineers Inc.",
  number    = "6",
}


@article{aaba624f55914153afad03ae3bb25441,
  title     = "The Semantic Web - on the respective Roles of XML and RDF.",
  abstract  = "The role of ontologies in the architecture of the Semantic Web was described. Extensible markup language (XML) and resource description framework (RDF) are the current standards for establishing semantic interoperability on the Web, but XML addresses only document structure. RDF better facilitates interoperation because it provides a data model that can be extended to address sophisticated ontology representation techniques. A general method for encoding ontology representation languages into RDF/RDF schema was proposed to establish a inference layer on top of the Web's current layers.",
  author    = "S. Decker and {van Harmelen}, F.A.H. and J. Broekstra and M. Erdmann and D.A. Fensel and I. Horrocks and M.C.A. Klein and S. Melnik",
  year      = "2000",
  doi       = "10.1109/4236.877487",
  pages     = "63--74",
  journal   = "IEEE Internet Computing",
  issn      = "1089-7801",
  publisher = "Institute of Electrical and Electronics Engineers Inc.",
}


@article{e55a7ace13014475a6fba9d2db3b27a4,
  title     = "The Semantic Web: The roles of XML and RDF",
  author    = "S. Decker and S. Melnik and {van Harmelen}, F.A.H. and D.A. Fensel and M. Klein and J. Broekstra and M. Erdmann and I. Horrocks",
  year      = "2000",
  volume    = "15",
  journal   = "IEEE Intelligent Systems & their Applications",
  issn      = "1094-7167",
  publisher = "Institute of Electrical and Electronics Engineers Inc.",
  number    = "3",
}


@article{d9ea380d0eb449e488a721a0fa927058,
  title     = "The Semantic Web: The roles of XML and RDF.",
  abstract  = "The role of ontologies in the architecture of the Semantic Web was described. Extensible markup language (XML) and resource description framework (RDF) are the current standards for establishing semantic interoperability on the Web, but XML addresses only document structure. RDF better facilitates interoperation because it provides a data model that can be extended to address sophisticated ontology representation techniques. A general method for encoding ontology representation languages into RDF/RDF schema was proposed to establish a inference layer on top of the Web's current layers.",
  author    = "S. Decker and S. Melnik and {van Harmelen}, F.A.H. and D.A. Fensel and M. Klein and J. Broekstra and M. Erdmann and I. Horrocks",
  year      = "2000",
  doi       = "10.1109/4236.877487",
  volume    = "15",
  pages     = "63--74",
  journal   = "IEEE Internet Computing",
  issn      = "1089-7801",
  publisher = "Institute of Electrical and Electronics Engineers Inc.",
  number    = "3",
}


@inbook{4984f511d2eb42c0b05a7b88a506ba1a,
  title     = "Torture tests: a quantitative analysis for the robustness of Knowledge-Based Systems.",
  author    = "P.C. Groot and {van Harmelen}, F.A.H. and {ten Teije}, A.C.M.",
  year      = "2000",
  pages     = "403--418",
  editor    = "R. Dieng",
  booktitle = "Proceedings of the 12th European Workshop on Knowledge Acquisition, Modeling, and Management ({EKAW}'00).",
  publisher = "Springer-Verlag",
}


@inbook{48232687b6044cb498dc7e46b4054a96,
  title     = "Torture tests: A quantitative analysis for the robustness of knowledge-based systems",
  author    = "P Groot and {Van Harmelen}, F and {ten Teije}, Annette",
  year      = "2000",
  isbn      = "3-540-41119-4",
  volume    = "1937",
  series    = "LECTURE NOTES IN ARTIFICIAL INTELLIGENCE",
  pages     = "403--418",
  booktitle = "KNOWLEDGE ENGINEERING AND KNOWLEDGE MANAGEMENT, PROCEEDINGS",
}


@inbook{695bd5862fba4dd3b33c964dcab0c8b3,
  title     = "Knowledge-based validation, aggregation, and visualization of meta-data: Analyzing a web-based information system",
  abstract  = "As meta-data become of increasing importance to the Web, we will need to start managing such meta-data. We argue that there is a strong need for meta-data validation and aggregation. We introduce the Spectacle Workbench for verifying semi-structured information and show how it can be used to validate, aggregate and visualize the metadata of an existing Information System. We conclude that the possibility to verify and aggregate meta-data is an added value with respect to contents-based access to information.",
  author    = "Heiner Stuckenschmidt and {Van Harmelen}, Frank",
  year      = "2001",
  isbn      = "3540427309",
  volume    = "2198",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "217--226",
  booktitle = "Web Intelligence: Research and Development - 1st Asia-Pacific Conference, WI 2001, Proceedings",
}


@article{6c2d3aaf30574c7783a1d47dfe23d493,
  title    = "OIL Ontology Infrastructure to Enable the Semantic Web",
  abstract = "Currently computers are changing from single isolated devices to entry points into a worldwide network of information exchange and business transactions. Therefore, support in the exchange of data, information, and knowledge is becoming the key issue in computer technology today. Ontologies provide a shared and common understanding of a domain that can be communicated between people and across application systems. Ontologies will play a major role in supporting information exchange processes in various areas. A prerequisite for such a role is the development of a joint standard for specifying and exchanging Ontologies well-integrated with existing web standards. This paper deals with precisely this necessity. We will present OIL which is a proposal for such a standard enabling the semantic web, i.e. information with machine processable semantics. It is based on existing proposals such as OKBC, XOL and RDFS, and enriches them with necessary features for expressing rich ontologies. The paper presents the motivation, underlying rationale, modeling primitives, syntax, semantics, tool environment, and applications of OIL",
  author   = "Dieter Fensel and Ian Horrocks and Harmelen, {Frank Van} and McGuinness, {Deborah L}",
  year     = "2001",
  doi      = "10.1109/5254.920598",
  volume   = "16",
  pages    = "38--45",
  journal  = "Intelligent Systems, IEEE",
  number   = "2",
}


@misc{218fe7547df84745a4ebdb5b317e312f,
  title    = "Ontology-based Information Visualisation",
  abstract = "The main contribution of this paper is to show how vi- sual representations of information can be based on onto- logical classifications of that information. We first discuss the central role of ontologies on the SemanticWeb. We sub- sequently outline our general approach to the construction of ontology-based visualisations of data. This is followed by a set of examples of ontology-based visualisations which all differ in interesting respects. The paper concludes with a brief discussion of related work.",
  author   = "Harmelen, {Frank Van} and Jeen Broekstra and Christiaan Fluit and {ter Horst}, Herko and Arjohn Kampman and {Van Der Meer}, Jos and Marta Sabou",
  year     = "2001",
}


@inbook{296aea5f4f60437bad004b44f8994791,
  title     = "Ontology-based metadata generation from semi-structured information",
  abstract  = "Content-related metadata plays an important role in intelligent information systems. Especially on the world-wide web meaningful metadata describing the contents of a web-site is the key to intelligent retrieval and access of information. Metadata description standards like RDF and RDF schema have been developed and work in progress addresses the use of ontologies to provide a logical foundation for metadata. However, the acquisition of appropriate metadata is still a problem. The main part of the paper is concerned with the specification of ontologies and metadata models. We describe the Spectacle approach, a knowledge-based approach for metadata validation and generation as well as tools related to the ontology language OIL. We conclude that the specification of ontologies and the generation of metadata models are processes that supplement each other and propose a method for semi-automatic generation of metadata models on the basis of ontologies.",
  author    = "Heiner Stuckenschmidt and {Van Harmelen}, Frank",
  year      = "2001",
  isbn      = "1581133804",
  pages     = "163--170",
  booktitle = "Proceedings of the First International Conference on Knowledge Capture",
}


@inbook{3050714490394955af00ed895a24a2b4,
  title     = "Using critiquing for improving medical protocols: Harder than it seems",
  abstract  = "Medical protocols are widely recognised to provide clinicians with high-quality and up-to-date recommendations. A critical condition for this is of course that the protocols themselves are of high quality. In this paper we investigate the use of critiquing for improving the quality of medical protocols. We constructed a detailed formal model of the jaundice protocol of the American Association of Pediatrics in the Asbru representation language. We recorded the actions performed by a pediatrician while solving a set of test cases. We then compared these expert actions with the steps recommended by the formalised protocol, and analysed the differences that we observed. Even our relatively small test set of 7 cases revealed many mismatches between the actions performed by the expert and the protocol recommendations, which suggest improvements of the protocol. A major problem in our case study was to establish a mapping between the actions performed by the expert and the steps suggested by the protocol. We discuss the reasons for this difficulty, and assess its consequences for the automation of the critiquing process.",
  author    = "Mar Marcos and Geert Berger and {van Harmelen}, Frank and {ten Teije}, Annette and Hugo Roomans and Silvia Miksch",
  year      = "2001",
  isbn      = "3540422943",
  volume    = "2101",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "431--441",
  booktitle = "Artificial Intelligence in Medicine - 8th Conference on Artificial Intelligence in Medicine in Europe, AIME 2001, Proceedings",
}


@article{98c93a250fa641278ebdde3e4dbcd1fe,
  title     = "A survey of languages for specifying dynamics: A knowledge engineering perspective",
  abstract  = "During the last years, a number of formal specification languages for knowledge-based systems has been developed. Characteristics for knowledge-based systems are a complex knowledge base and an inference engine which uses this knowledge to solve a given problem. Specification languages for knowledge-based systems have to cover both aspects. They have to provide the means to specify a complex and large amount of knowledge and they have to provide the means to specify the dynamic reasoning behavior of a knowledge-based system. This paper focuses on the second aspect. For this purpose, we survey existing approaches for specifying dynamic behavior in related areas of research. In fact, we have taken approaches for the specification of information systems (Language for Conceptual Modeling and TROLL), approaches for the specification of database updates and logic programming (Transaction Logic and Dynamic Database Logic) and the generic specification framework of Abstract State Machines.",
  keywords  = "Dynamics, Inference control, Knowledge-based systems, Specification languages, Update logics",
  author    = "{Van Eck}, Pascal and Joeri Engelfriet and Dieter Fensel and {Van Harmelen}, Frank and Yde Venema and Mark Willems",
  year      = "2001",
  month     = "5",
  doi       = "10.1109/69.929903",
  volume    = "13",
  pages     = "462--496",
  journal   = "IEEE Transactions on Knowledge and Data Engineering",
  issn      = "1041-4347",
  publisher = "IEEE Computer Society",
  number    = "3",
}


@inbook{3041593d264c478884e38f11e6e6e8fa,
  title     = "Approximating terminological queries",
  abstract  = "Current proposals for languages to encode terminological knowledge in intelligent systems support logical reasoning for answering user queries about objects and classes. An application of these languages on the World Wide Web, however, is hampered by the limitations of logical reasoning in terms ofefficiency and flexibility. In this paper we describe, how techniques from approximate reasoning can be used to overcome these problems. We discuss terminological knowledge and approximate reasoning in general and show the benefits ofapproximate reasoning using the example ofbuilding and maintaining semantic catalogues that can be used to query resource locations based on object classes.",
  author    = "Heiner Stuckenschmidt and {Van Harmelen}, Frank",
  year      = "2002",
  isbn      = "3540000747",
  volume    = "2522 LNAI",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  pages     = "329--343",
  booktitle = "Flexible Query Answering Systems - 5th International Conference, FQAS 2002, Proceedings",
}


@inbook{67261de6afa54370801ff6d951a47102,
  title     = "From informal knowledge to formal logic: A realistic case study in medical protocols",
  abstract  = "We report our experience in a case study with constructing fully formalised knowledge models of realistic, specialised medical knowledge. We have taken a medical protocol in daily use by medical specialists, modelled this knowledge in a specific-purpose knowledge representation language, and finally formalised this knowledge representation in terms of temporal logic and parallel programs. The value of this formalisation process is that each successive formalisation step has contributed to improving the quality of the original medical protocol, and that the final formalisation allows us to provide machine-assisted proofs of properties that are satisfied by the original medical protocol (or, alternatively, precise arguments why the original protocol fails to satisfy certain desirable properties). We believe that this the first time that a significant body of medical knowledge (in our case: a protocol for the management of jaundice in newborns) has been formalised to the extent that it becomes amenable to automated theorem proving, and that this has actually lead to improvement of the original body of medical knowledge.",
  author    = "Mar Marcos and Michael Balser and {Ten Teije}, Annette and {van Harmelen}, Frank",
  year      = "2002",
  isbn      = "3540442685",
  volume    = "2473",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "49--64",
  booktitle = "Knowledge Engineering and Knowledge Management: Ontologies and the Semantic Web - 13th International Conference, EKAW 2002, Proceedings",
}


@misc{1ed1d738b5ab4c4b9192563f10370569,
  title    = "Learning Structural Classification Rules for Web-page Categorization",
  abstract = "Content-related metadata plays an important role in the effort of developing intelligent web applications. One of the most established form of providing content-related metadata is the assignment of web-pages to content categories. We describe the Spectacle system for classifying individual web pages on the basis of their syntactic structure. This classification requires the spe-cification of classification rules associating common pa-ge structures with predefined classes. In this paper, we propose an approach for the automatic acquisition of these classification rules using techniques from inducti-ve logic programming and describe experiments in ap-plying the approach to an existing web-based informa-tion system.",
  author   = "Heiner Stuckenschmidt and Jens Hartmann and {Van Harmelen}, Frank",
  year     = "2002",
}


@inbook{0977e50ff58f4f31b7120f5b4c72e182,
  title     = "Ontology-based Information Visualisation",
  abstract  = "The main contribution of this paper is to show how visual representations of information can be based on ontological classifications of that information. We first discuss the central r ole of ontologies on the Semantic Web. We subsequently outline our general approach to the construction of ontology-based visualisations of data. This is followed by a set of examples of ontology-based visualisations which all differ in interesting respects. The paper concludes with a brief discussion of related work. 1",
  keywords  = "Semantic Web",
  author    = "Vladimir Geroimenko and Christiaan Fluit and Marta Sabou and {van Harmelen}, Frank",
  year      = "2002",
  isbn      = "0-7695-1195-3",
  series    = "Visualising the Semantic Web",
  publisher = "Springer/Verlag",
  editor    = "Vladimir Geroimenko",
  booktitle = "Visualising the Semantic Web",
}


@article{af387e71d0e5414b93b4ab7b7c66a69e,
  title    = "Ranking Agent Statements for Building Evolving Ontologies",
  abstract = "In this paper a methodology is described for ranking information received by different agents, based on previous experience with them. These rankings again are used for asking the right questions to the right agents. In this way agents can build up a reputation. The methods in this paper are strongly influenced on human heuristics regarding the assignment of confidence ratings on humans. The methods provide a solution to current problems with ontologies: (1) handling contradicting and sloppy information, (2) efficient network use instead of broadcasting information and (3) dealing with ontological drift.",
  author   = "Ronny Siebes and {van Harmelen}, Frank",
  year     = "2002",
  doi      = "10.1.1.6.8294",
  pages    = "2--4",
  journal  = "Workshop on Meaning Negotation, in conjunction with the Eighteenth National Conference on Artificial Intelligence",
}


@inbook{c20818146d15404d84428ec75b4f1497,
  title     = "Reviewing the design of DAML+OIL: An ontology language for the Semantic Web",
  abstract  = "In the current {"}Syntactic Web{"}, uninterpreted syntactic constructs are given meaning only by private off-line agreements that are inaccessible to computers. In the Semantic Web vision, this is replaced by a web where both data and its semantic definition are accessible and manipulable by computer software. DAML+OIL is an ontology language specifically designed for this use in the Web; it exploits existing Web standards (XML and RDF), adding the familiar ontological primitives of object oriented and frame based systems, and the formal rigor of a very expressive description logic. The definition of DAML+OIL is now over a year old, and the language has been in fairly widespread use. In this paper, we review DAML+OIL's relation with its key ingredients (XML, RDF, OIL, DAML-ONT, Description Logics), we discuss the design decisions and trade-offs that were the basis for the language definition, and identify a number of implementation challenges posed by the current language. These issues are important for designers of other representation languages for the Semantic Web, be they competitors or successors of DAML+OIL, such as the language currently under definition by W3C.",
  author    = "Ian Horrocks and Patel-Schneider, {Peter F.} and {Van Harmelen}, Frank",
  year      = "2002",
  pages     = "792--797",
  booktitle = "Proceedings of the National Conference on Artificial Intelligence",
}


@inbook{49ce1d5b12a640598767d522cd113302,
  title     = "Sesame: A generic architecture for storing and querying RDF and RDF Schema",
  abstract  = "RDF and RDF Schema are two W3C standards aimed at enriching the Web with machine-processable semantic data. We have developed Sesame, an architecture for efficient storage and expressive querying of large quantities of metadata in RDF and RDF Schema. Sesame's design and implementation are independent from any specific storage device. Thus, Sesame can be deployed on top of a variety of storage devices, such as relational databases, triple stores, or object-oriented databases, without having to change the query engine or other functional modules. Sesame offers support for concurrency control, independent export of RDF and RDFS information and a query engine for RQL, a query language for RDF that offers native support for RDF Schema semantics. We present an overview of Sesame as a generic architecture, as well as its implementation and our first experiences with this implementation.",
  author    = "Jeen Broekstra and Arjohn Kampman and {Van Harmelen}, Frank",
  year      = "2002",
  isbn      = "3540437606",
  volume    = "2342 LNCS",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  pages     = "54--68",
  booktitle = "The Semantic Web, ISWC 2002 - First International Semantic Web Conference, Proceedings",
}


@article{34b40f2bd03442f08fd4699c2281d9eb,
  title     = "How the semantic web will change KR: Challenges and opportunities for a new research agenda",
  abstract  = "Issues related to the application of semantic web to knowledge representation (KR) are discussed. Some of the assumptions underlying current KR technology that need to be revised when applied to semantic web are described. Areas which require future research include variable quality of knowledge, diversity of content, distributed authority, and multiple knowledge sources. Techniques for the local containment of inconsistencies will become much more important than they have been in current KR research.",
  author    = "{Van Harmelen}, Frank",
  year      = "2002",
  month     = "3",
  doi       = "10.1017/S0269888902000383",
  volume    = "17",
  pages     = "93--96",
  journal   = "Knowledge Engineering Review",
  issn      = "0269-8889",
  publisher = "Cambridge University Press",
  number    = "1",
}


@article{b5075dbf07fa45fa8173d5898b9a8972,
  title     = "Enabling knowledge representation on the Web by extending RDF Schema",
  abstract  = "Recently, a widespread interest has emerged in using ontologies on the Web. Resource Description Framework Schema (RDFS) is a basic tool that enables users to define vocabulary, structure and constraints for expressing meta data about Web resources. However, it includes no provisions for formal semantics, and its expressivity is not sufficient for full-fledged ontological modeling and reasoning. In this paper, we will show how RDFS can be extended to include a more expressive knowledge representation language. That, in turn, would enrich it with the required additional expressivity and the semantics of that language. We do this by describing the ontology language Ontology Inference Layer (OIL) as an extension of RDFS. An important advantage to our approach is that it ensures maximal sharing of meta data on the Web: even partial interpretation of an OIL ontology by less semantically aware processors will yield a correct partial interpretation of the meta data.",
  keywords  = "DAML, Knowledge representation, OIL, Ontologies, RDF, Semantic Web",
  author    = "Jeen Broekstra and Michel Klein and Stefan Decker and Dieter Fensel and {Van Harmelen}, Frank and Ian Horrocks",
  year      = "2002",
  month     = "8",
  doi       = "10.1016/S1389-1286(02)00217-7",
  volume    = "39",
  pages     = "609--634",
  journal   = "Computer Networks (1999)",
  issn      = "1389-1286",
  publisher = "Elsevier",
  number    = "5",
}


@article{a8f1c38ebbb94257ba54da65d5a7b1eb,
  title    = "A Metadata Model for Semantics-Based Peer-to-Peer Systems",
  abstract = "Peer-to-Peer systems are a new paradigm for information sharing and some systems have successfully been deployed. It has been argued that current Peer-to-Peer systems suffer from the lack of semantics. The SWAP project (Semantic Web and Peer-to-Peer) aims at overcoming this problem by combining the Peer-to-Peer paradigm with Semantic Web technologies. In the course of our investigations it turned out that the nature of Peer-to-Peer systems requires some compromises with respect to the use of semantic knowledge models. In particular, the notion of ontology does not really apply as we often do not find a shared understanding of the domain. In this paper, we propose a data model for encoding semantic information that combines features of ontology (concept hierarchies, relational structures) with a flexible description and rating model that allows us to handle heterogeneous and even contradictory views on the domain of interest. We discuss the role of this model in the SWAP environment and describe the model as well as its creation and access.",
  author   = "Marc Ehrig and Ch. Tempich and Jeen Broekstra and {van Harmelen}, Frank and Marta Sabou and Ronny Siebes and Steffen Staab and Heiner Stuckenschmidt",
  year     = "2003",
  pages    = "1--4",
  journal  = "Proceedings of the second Konferenz Professionelles Wissensmanagement",
}


@article{39a06dba90094459ad5e1f75241fa348,
  title     = "C-OWL: Contextualizing ontologies",
  abstract  = "Ontologies are shared models of a domain that encode a view which is common to a set of different parties. Contexts are local models that encode a party's subjective view of a domain. In this paper we show how ontologies can be contextualized, thus acquiring certain useful properties that a pure shared approach cannot provide. We say that an ontology is contextualized or, also, that it is a contextual ontology, when its contents are kept local, and therefore not shared with other ontologies, and mapped with the contents of other ontologies via explicit (context) mappings. The result is Context OWL (C-OWL), a language whose syntax and semantics have been obtained by extending the OWL syntax and semantics to allow for the representation of contextual ontologies.",
  author    = "Paolo Bouquet and Fausto Giunchiglia and {Van Harmelen}, Frank and Luciano Serafini and Heiner Stuckenschmidt",
  year      = "2003",
  volume    = "2870",
  pages     = "164--179",
  journal   = "Lecture Notes in Computer Science",
  issn      = "0302-9743",
  publisher = "Springer Verlag",
}


@article{43af14a00e83403a80a48fc3aa316052,
  title     = "Experiences in the formalisation and verification of medical protocols",
  abstract  = "Medical practice protocols or guidelines are statements to assist practitioners and patient decisions about appropriate health care for specific circumstances. In order to reach their potential benefits, protocols must fulfill strong quality requirements. Medical bodies worldwide have made efforts in this direction, mostly using informal methods such as peer review of protocols. We are concerned with, a different approach, namely the quality improvement of medical protocols by formal methods. In this paper we report on our experiences in the formalisation and verification of a real-world medical protocol. We have fully formalised a medical protocol in a two-stage formalisation process. Then, we have used a theorem prover to confirm whether the protocol formalisation complies with certain protocol properties. As a result, we have shown that formal verification can be used to analyse, and eventually improve, medical protocols.",
  author    = "Mar Marcos and Michael Balser and {Ten Teije}, Annette and {Van Harmelen}, Frank and Christoph Duelli",
  year      = "2003",
  volume    = "2780",
  pages     = "132--141",
  journal   = "Lecture Notes in Computer Science",
  issn      = "0302-9743",
  publisher = "Springer Verlag",
}


@article{fce48238792241078b29a4c4e4ba5aa7,
  title     = "From SHIQ and RDF to OWL:The Making of a Web Ontology Language",
  abstract  = "The OWL Web Ontology Language is a new formal language\nfor representing ontologies in the Semantic Web. OWL has features from\nseveral families of representation languages, including primarily Description\nLogics and frames. OWL also shares many characteristics with RDF,\nthe W3C base of the Semantic Web. In this paper we discuss how the\nphilosophy and features of OWL can be traced back to these older formalisms,\nwith modifications driven by several other constraints on OWL.\nSeveral interesting problems have arisen where these influences on OWL\nhave clashed.",
  keywords  = "description logics, frames, ontologies, semantic web",
  author    = "Ian Horrocks and Peter Patel-Schneider and {van Harmelen}, Frank",
  year      = "2003",
  doi       = "10.1016/j.websem.2003.07.001",
  volume    = "1",
  pages     = "7--26",
  journal   = "Journal of Web Semantics",
  issn      = "1570-8268",
  publisher = "Elsevier",
  number    = "1",
}


@article{82d380a0329944ba839d10f7d3e0d1f1,
  title     = "Informal and formal medical guidelines: Bridging the gap",
  abstract  = "The role of medical guidelines is becoming more and more important in the medical field. Within the Protocure project it has been shown that the quality of medical guidelines can be improved by formalisation. However formalisation turns out to be a very time-consuming task, resulting in a formal guideline that is much more complex than the original version and the relation with this original guideline is often unclear. This paper presents a case study where the relation between the informal medical guideline and its formal counterpart is investigated. This has been used to determine the gaps between the formal and informal guidelines and the cause of the size explosion of the formal guidelines.",
  author    = "Marije Geldof and {Ten Teije}, Annette and {Van Harmelen}, Frank and Mar Marcos and Peter Votruba",
  year      = "2003",
  volume    = "2780",
  pages     = "173--178",
  journal   = "Lecture Notes in Computer Science",
  issn      = "0302-9743",
  publisher = "Springer Verlag",
}


@book{904bcc8992c849a0b31cccc6ca65d5f6,
  title     = "Information Sharing on the Semantic Web",
  abstract  = "The large-scale and almost ubiquitous availability of information has become as much of a curse as it is a blessing. The more information is available, the harder it is to locate any particular piece of it. And even when it has been successfully found, it is even harder still to usefully combine it with other information we may already possess. This problem occurs at many different levels, ranging from the overcrowded disks of our own PCs to the mass of unstructured information on the World Wide Web. It is commonly understood that this problem of information sharing can only be solved by giving computers better access to the semantics of the information. While it has been recognized that ontologies play a crucial role in solving the open problems, most approaches rely on the existence of well-established data structures. To overcome these shortcomings, Stuckenschmidt and van Harmelen describe ontology-based approaches for resolving semantic heterogeneity in weakly structured environments, in particular the World Wide Web. Addressing problems like missing conceptual models, unclear system boundaries, and heterogeneous representations, they design a framework for ontology-based information sharing in weakly structured environments like the Semantic Web. For researchers and students in areas related to the Semantic Web, the authors provide not only a comprehensive overview of the State of the art, but also present in detail recent research in areas like ontology design for information integration, metadata generation and management, and representation and management of distributed ontologies. For professionals in areas such as e-commerce (e.g., the exchange of product knowledge) and knowledge management (e.g., in large and distributed organizations), the book provides decision support on the use of novel technologies, information about potential problems, and guidelines for the successful application of existing technologies.",
  author    = "Heiner Stuckenschmidt and Harmelen, {Frank Van}",
  year      = "2003",
  isbn      = "3540205942",
  publisher = "Springer/Verlag",
}


@inbook{8e6f8c918eb94fbcb1d304e80ea5c2e1,
  title     = "Supporting User Tasks through Visualisation of Light-weight Ontologies",
  abstract  = "Introduction\n\nAs is abundantly clear from the other chapters in this volume, ontologies will\nplay a central role in the development and deployment of the Semantic Web.\nThey will be used for many di#erent purposes, ranging across information\nlocalisation, integration, querying, presentation and navigation.\n\nExperiences in other fields (Data Mining, Scientific Computing) have\nshown that visualisation techniques can be successfully employed to support\nmany of these tasks in those areas. The...",
  author    = "Christiaan Fluit and Marta Sabou and {van Harmelen}, Frank",
  year      = "2003",
  doi       = "10.1.1.15.5304",
  isbn      = "3540408347",
  series    = "Handbook on Ontologies in Information Systems",
  pages     = "1--20",
  booktitle = "Handbook on Ontologies in Information Systems",
}


@book{1be539fb07104c5eb9904af692ac7392,
  title     = "Towards the Semantic Web",
  abstract  = "With the current changes driven by the expansion of the World Wide Web, this book uses a different approach from other books on the market: it applies ontologies to electronically available information to improve the quality of knowledge management in large and distributed organizations. Ontologies are formal theories supporting knowledge sharing and reuse. They can be used to explicitly represent semantics of semi-structured information. These enable sophisticated automatic support for acquiring, maintaining and accessing information. Methodology and tools are developed for intelligent access to large volumes of semi-structured and textual information sources in intra- and extra-, and internet-based environments to employ the full power of ontologies in supporting knowledge management from the information client perspective and the information provider. The aim of the book is to support efficient and effective knowledge management and focuses on weakly-structured online information sources. It is aimed primarily at researchers in the area of knowledge management and information retrieval and will also be a useful reference for students in computer science at the postgraduate level and for business managers who are aiming to increase the corporations information infrastructure. The Semantic Web is a very important initiative affecting the future of the WWW that is currently generating huge interest. The book covers several highly significant contributions to the semantic web research effort, including a new language for defining ontologies, several novel software tools and a coherent methodology for the application of the tools for business advantage. It also provides 3 case studies which give examples of the real benefits to be derived from the adoption of semantic-web based ontologies in {"}real world{"} situations. As such, the book is an excellent mixture of theory, tools and applications in an important area of WWW research. Provides guidelines for introducing knowledge management concepts and tools into enterprises, to help knowledge providers present their knowledge efficiently and effectively. Introduces an intelligent search tool that supports users in accessing information and a tool environment for maintenance, conversion and acquisition of information sources. Discusses three large case studies which will help to develop the technology according to the actual needs of large and or virtual organisations and will provide a testbed for evaluating tools and methods. The book is aimed at people with at least a good understanding of existing WWW technology and some level of technical understanding of the underpinning technologies (XML/RDF). It will be of interest to graduate students, academic and industrial researchers in the field, and the many industrial personnel who are tracking WWW technology developments in order to understand the business implications. It could also be used to support undergraduate courses in the area but is not itself an introductory text. {"}Generating huge interest and backed by the global WorldWideWeb consortium the semantic web is the key initiative driving the future of the World Wide Web. Towards the Semantic Web focuses on the application of Semantic Web technology and ontologies in particular to electronically available information to improve the quality of knowledge management in large and distributed organizations. Ontologies are formal structures supporting knowledge sharing and reuse. They can be used to represent explicitly the semantics of structured and semi-structured information which enable sophisticated automatic support for acquiring, maintaining and accessing information. Covering the key technologies for the next generation of the WWW, this book is an excellent mixture of theory, tools and applications in an important area of WWW research. Aims to support more efficient and effective knowledge management and focuses on weakly-structured online information sources. Covers highly significant contributions to the semantic web research effort, including a new language for defining ontologies, several novel software tools and a coherent methodology for the application of the tools for business advantage. Provides guidelines for introducing knowledge management concepts and tools into businesses. Introduces an intelligent search tool that supports users in accessing information and a tool environment for maintenance, conversion and acquisition of information sources. Also describes information visualisation and knowledge sharing tools. Describes a state-of-the-art system for storage and retrieval of metadata expressed in RDF and RDF Schema. Also discusses a system for automatic metadata extraction. Examines three significant case studies providing examples of the real benefits to be derived from the adoption of semantic-web based ontologies in {"}{"}real world{"}{"} situations.",
  author    = "John Davies and Dieter Fensel and Harmelen, {Frank Van}",
  year      = "2003",
  isbn      = "0470848677",
  publisher = "John Wiley",
}


@article{29b9ba977f0d4002be3128941590021e,
  title     = "A tool for gene expression based PubMed search through combining data sources.",
  abstract  = "We present a new tool for the semi-automated querying of PubMed using a batch of tens to thousands of GenBank accession numbers or UniGene cluster ids. By combining information from UniGene and SWISS-PROT, microGENIE obtains information on the biological relevance of expressed genes, as identified by micro-array experiments, with minimal user intervention and time investment. © Oxford University Press 2004; all rights reserved.",
  author    = "M. Korotkiy and R.A. Middelburg and H. Dekker and {van Harmelen}, F.A.H. and J. Lankelma",
  year      = "2004",
  doi       = "10.1093/bioinformatics/bth183",
  volume    = "20",
  pages     = "1980--1982",
  journal   = "Bioinformatics",
  issn      = "1367-4803",
  publisher = "Oxford University Press",
  number    = "12",
}


@article{d6c8740d8ca54972a7d6a18fbd06255e,
  title     = "Configuration of Web services as parametric design",
  abstract  = "The configuration of Web services is particularly hard given the heterogeneous, unreliable and open nature of the Web. Furthermore, such composite Web services are likely to be complex services, that will require adaptation for each specific use. Current approaches to Web service configuration are often based on pre/post-condition-style reasoning, resulting in a planning-style approach to service configuration, configuring a composite web service {"}from scratch{"} every time. In this paper, we propose instead a knowledge-intensive brokering approach to the creation of composite Web services. In our approach, we describe a complex Web service as a fixed template, which must be configured for each specific use. Web service configuration can then be regarded as parametric design, in which the parameters of the fixed template have to be instantiated with appropriate component services. During the configuration process, we exploit detailed knowledge about the template and the components, to obtain the required composite web service. We illustrate our proposal by applying it to a specific family of Web services, namely {"}heuristic classification services{"}. We have implemented a proto-type of our knowledge-intensive broker and describe its execution in a concrete scenario.",
  author    = "{Ten Teije}, Annette and {Van Harmelen}, Frank and Bob Wielinga",
  year      = "2004",
  volume    = "3257",
  pages     = "321--336",
  journal   = "Lecture Notes in Computer Science",
  issn      = "0302-9743",
  publisher = "Springer Verlag",
}


@article{fb3d79a95a754d18bf359662e52be6c8,
  title     = "Exploring Large Document Repositories with RDF Technology: The DOPE Project",
  author    = "Heiner Stuckenschmidt and {Van Harmelen}, Frank and {De Waard}, Anita and Scerri Tony and Ravinder Bhogal and {Van Buel}, Jan and Ian Crowlesmith and Christiaan Fluit and Arjohn Kampman and Jeen Broekstra and {van Mulligen}, Erik",
  year      = "2004",
  doi       = "10.1109/MIS.2004.9",
  volume    = "19",
  pages     = "34--40",
  journal   = "IEEE Intelligent Systems",
  issn      = "1541-1672",
  publisher = "Institute of Electrical and Electronics Engineers Inc.",
  number    = "3",
}


@inbook{93c27f7f2bc24f96a2ee217a3f5eca7e,
  title     = "OWL Web Ontology Language",
  abstract  = "The OWL Web Ontology Language is designed for use by applications that need to process the content of information instead of just presenting information to humans. OWL facilitates greater machine interpretability of Web content than that supported by XML, RDF, and RDF Schema (RDF-S) by providing additional vocabulary along with a formal semantics. OWL has three increasingly-expressive sublanguages: OWL Lite, OWL DL, and OWL Full. This document is written for readers who want a first impression of the capabilities of OWL. It provides an introduction to OWL by informally describing the features of each of the sublanguages of OWL. Some knowledge of RDF Schema is useful for understanding this document, but not essential. After this document, interested readers may turn to the OWL Guide for more detailed descriptions and extensive examples on the features of OWL. The normative formal definition of OWL can be found in the OWL Semantics and Abstract Syntax.",
  author    = "S. Staab and R. Studer and Grigoris Antoniou and {Van Harmelen}, Frank",
  year      = "2004",
  doi       = "10.1145/1295289.1295290",
  isbn      = "9781605660264",
  series    = "Ubiquity",
  publisher = "Springer",
  pages     = "1--1",
  editor    = "S Staab and R Studer",
  booktitle = "Ubiquity",
}


@book{1cbe43aedce045e8ab7d79f1a0b8bd41,
  title     = "OWL Web Ontology Language Reference",
  author    = "M. Dean and A.T. Schreiber and S. Bechofer and {van Harmelen}, F.A.H. and J. Hendler and I. Horrocks and D. MacGuinness and P. Patel-Schneider and Stein, {L. A.}",
  note      = "Dean04a",
  year      = "2004",
  publisher = "World Wide Web Consortium",
}


@article{3af4f0f0c17a4dd599da2009622732a4,
  title    = "OWL Web Ontology Language - Reference",
  abstract = "The {W}eb {O}ntology {L}anguage {OWL} is a semantic markup language for publishing and sharing ontologies on the {W}orld {W}ide {W}eb. {OWL} is developed as a vocabulary extension of {RDF} (the {R}esource {D}escription {F}ramework) and is derived from the {DAML}+{OIL} {W}eb {O}ntology {L}anguage. {T}his document contains a structured informal description of the full set of {OWL} language constructs and is meant to serve as a reference for {OWL} users who want to construct {OWL} ontologies.",
  keywords = "OWL Spec",
  author   = "McGuinness, {Deborah L} and Harmelen, {Frank van}",
  year     = "2004",
  pages    = "1--53, [Accessed: 02 February 2016]",
  journal  = "W3C Recommendation [Online], Available at: http://www.w3.org/TR/2004/REC-owl",
}


@article{cdb20c7e491b4996b8e1352c48fe9d9e,
  title     = "Peer selection in peer-to-peer networks with semantic topologies",
  abstract  = "Peer-to-Peer systems have proven to be an effective way of sharing data. Modern protocols are able to efficiently route a message to a given peer. However, determining the destination peer in the first place is not always trivial. We propose a model in which peers advertise their expertise in the Peer-to-Peer network. The knowledge about the expertise of other peers forms a semantic topology. Based on the semantic similarity between the subject of a query and the expertise of other peers, a peer can select appropriate peers to forward queries to, instead of broadcasting the query or sending it to a random set of peers. To calculate our semantic similarity measure we make the simplifying assumption that the peers share the same ontology. We evaluate the model in a bibliographic scenario, where peers share bibliographic descriptions of publications among each other. In simulation experiments we show how expertise based peer selection improves the performance of a Peer-to-Peer system with respect to precision, recall and the number of messages.",
  author    = "Peter Haase and Ronny Siebes and {Van Harmelen}, Frank",
  year      = "2004",
  volume    = "3226",
  pages     = "108--125",
  journal   = "Lecture Notes in Computer Science",
  issn      = "0302-9743",
  publisher = "Springer Verlag",
}


@inbook{e22328f7b53b47eb93625bfeff139c61,
  title     = "Peer Selection in Peer-to-Peer Networks with Semantic Topologies",
  abstract  = "Peer-to-Peer systems have proven to be an effective way of sharing data. Modern protocols are able to efficiently route a message to a given peer. However, determining the destination peer in the first place is not always trivial.\nWe propose a model in which peers advertise their expertise in the Peer-to-Peer network. The knowledge about the expertise of other peers forms a semantic topology. Based on the semantic similarity between the subject of a query and the expertise of other peers, a peer can select appropriate peers to forward queries to, instead of broadcasting the query or sending it to a random set of peers. To calculate our semantic similarity measure we make the simplifying assumption that the peers share the same ontology. We evaluate the model in a bibliographic scenario, where peers share bibliographic descriptions of publications among each other. In simulation experiments we show how expertise based peer selection improves the performance of a Peer-to-Peer system with respect to precision, recall and the number of messages.",
  author    = "Peter Haase and Ronny Siebes and {van Harmelen}, Frank",
  year      = "2004",
  doi       = "10.1007-978-3-540-30145-5_7",
  isbn      = "978-3-540-23609-2",
  series    = "Proceedings of the International Conference on Semantics in a Networked World ({ICNSW'04})",
  pages     = "108--125",
  booktitle = "Proceedings of the International Conference on Semantics in a Networked World ({ICNSW'04})",
}


@article{bd30f9749bc0415aa960a33637b93d1a,
  title     = "Protocure: supporting the development of medical protocols through formal methods",
  abstract  = "Medical guidelines and protocols describe the optimal care for a specific group of patients and therefore, when properly applied, improve the quality of patient care. During the last decade, a large number of medical guidelines and protocols have been published. However, the work done on developing and disseminating them far outweighs the efforts on guaranteeing their quality. Indeed, anomalies like ambiguity and incompleteness are frequent in medical guidelines and protocols. An approach grounded on a formal representation, can answer these needs, as we have demonstrated in the Protocure project'. The Protocure II project will aim at integrating formal methods in the life cycle of guidelines.",
  keywords  = "Clinical Protocols, Decision Support Techniques, Evidence-Based Medicine, Humans, Planning Techniques, Practice Guidelines as Topic, Programming Languages, Software, Journal Article, Research Support, Non-U.S. Gov't",
  author    = "Michael Balser and Oscar Coltell and {van Croonenborg}, Joyce and Christoph Duelli and {van Harmelen}, Frank and Albert Jovell and Peter Lucas and Mar Marcos and Silvia Miksch and Wolfgang Reif and Rosenbrand, {Kitty C J G M} and Andreas Seyfang and {ten Teije}, Annette",
  year      = "2004",
  volume    = "101",
  pages     = "103--107",
  journal   = "Studies in Health Technology and Informatics",
  issn      = "0926-9630",
  publisher = "IOS Press",
}


@book{98baea1ecbb34018b717b1de16de5a5b,
  title     = "Semantic Web Primer",
  abstract  = "The development of the Semantic Web, with machine-readable content, has the potential to revolutionize the World Wide Web and its use. A Semantic Web Primer provides an introduction and guide to this still emerging field, describing its key ideas, languages, and technologies. Suitable for use as a textbook or for self-study by professionals, it concentrates on undergraduate-level fundamental concepts and techniques that will enable readers to proceed with building applications on their own and includes exercises, project descriptions, and annotated references to relevant online materials. A Semantic Web Primer provides a systematic treatment of the different languages (XML, RDF, OWL, and rules) and technologies (explicit metadata, ontologies, and logic and inference) that are central to Semantic Web development as well as such crucial related topics as ontology engineering and application scenarios. This substantially revised and updated second edition reflects recent developments in the field, covering new application areas and tools. The new material includes a discussion of such topics as SPARQL as the RDF query language; OWL DLP and its interesting practical and theoretical properties; the SWRL language (in the chapter on rules); OWL-S (on which the discussion of Web services is now based). The new final chapter considers the state of the art of the field today, captures ongoing discussions, and outlines the most challenging issues facing the Semantic Web in the future. Supplementary materials, including slides, online versions of many of the code fragments in the book, and links to further reading, can be found at http://www.semanticwebprimer.org.Grigoris Antoniou is Professor at the Institute for Computer Science, FORTH (Foundation for Research and Technology-Hellas), Heraklion, Greece. Frank van Harmelen is Professor in the Department of Artificial Intelligence at the Vrije Universiteit, Amsterdam, the Netherlands.",
  author    = "Grigoris Antoniou and Harmelen, {Frank van}",
  year      = "2004",
  isbn      = "0262012103",
  publisher = "MIT Press",
}


@inbook{7b70f7f48ea9415c8ee7ff1a75259f6c,
  title     = "The SemanticWeb – ISWC 2004: Third International SemanticWeb Conference Hiroshima, Japan, November 7-11, 2004 Proceedings",
  author    = "Sheila McIlraith and Dimitris Plexousakis and {van Harmelen}, Frank",
  year      = "2004",
  isbn      = "3540237984",
  volume    = "3298",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  booktitle = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
}


@misc{e8adce6202874f8283a147f388e811fc,
  title    = "Using C-OWL for the Alignment and Merging of Medical Ontologies",
  abstract = "A number of sophisticated medical ontologies have been created over the past years. With their de-velopment the need for supporting the alignment of different ontologies is gaining importance. We proposed C-OWL, an extension of the Web Ontology Language OWL that supports alignment mappings between different, possibly incompatible ontologies on a semantic level. In this paper we report experiences from using C-OWL for the alignment of medical ontologies. We briefly review key concepts of the C-OWL semantics, explain the setting of the case study including some examples from the alignment and discuss the possibility of reasoning about the mapping based on the C-OWL semantics We conclude by arguing that C-OWL provides an adequate frame-work for aligning complex ontologies in the medical domain.",
  keywords = "Biomedical Knowledge representation, Knowledge Representa-tion Languages, Terminology Integration, validation and maintenance",
  author   = "Heiner Stuckenschmidt and {van Harmelen}, Frank and Luciano Serafini and Paolo Bouquet and Fausto Giunchiglia",
  year     = "2004",
}


@inbook{35cebc494cf0476b8cccc7a872efef48,
  title     = "Web Ontology Language",
  abstract  = "McGuinness, D., van Harmelen, F. (2004). OWL Web Ontology Language - W3C Recommendation. Retrieved June 14, 2009, http://www.w3.org/TR/owl-features/",
  author    = "DL McGuinness and {van Harmelen}, Frank",
  year      = "2004",
  doi       = "10.1007/978-1-4614-6170-8_113",
  isbn      = "978-1-4614-6170-8",
  series    = "W3C Recommendation.",
  booktitle = "W3C Recommendation.",
}


@inbook{606c3c9c70224bcb9a8f00b4fc935f63,
  title     = "Web ontology language: Owl",
  abstract  = "Summary. The expressivity of RDF and RDF Schema that was described in 12 is deliberately very limited: RDF is (roughly) limited to binary ground predicates, and RDF Schema is (again roughly) limited to a subclass hierarchy and a property hierarchy, with domain and range",
  author    = "G. Antoniou and {Van Harmelen}, F.",
  year      = "2004",
  doi       = "10.1.1.6.9313",
  isbn      = "9783540709992",
  series    = "Handbook on ontologies",
  publisher = "Springer_Verlag",
  pages     = "45–60",
  booktitle = "Handbook on ontologies",
}


@article{5b84ef08d70b4ef484d5260fb246b88a,
  title     = "The Semantic Web: What, why, how, and when",
  abstract  = "The Semantic Web aims to enrich the Web with a layer of machine-interpretable metadata so that computer programs can predictably derive new information. This goal will require the development of metadata syntax and vocabularies, and the creation of metadata for lots of Web pages.",
  keywords  = "Data integration, Intelligent support, Semantic Web",
  author    = "{Van Harmelen}, Frank",
  year      = "2004",
  month     = "3",
  volume    = "5",
  journal   = "Distributed Systems Online",
  issn      = "1541-4922",
  publisher = "Institute of Electrical and Electronics Engineers Inc.",
  number    = "3",
}


@article{5c4909d3d0af4ce2b8e1e705ffc371b8,
  title     = "Generating and managing metadata for Web-based information systems",
  abstract  = "As metadata become of increasing importance to the Web, we will need to start managing such metadata. We argue that there is a strong need for metadata management. We introduce the Spectacle Workbench for verifying semi-structured information and show how it can be used to validate, aggregate and visualize the metadata of an existing information system. We conclude that the possibility to verify and aggregate metadata is an added value with respect to contents-based access to information and that it is possible to generate it on the basis of Web contents. ?? 2004 Elsevier B.V. All rights reserved.",
  keywords  = "Content-analysis, Knowledge-based methods, Metadata management",
  author    = "Heiner Stuckenschmidt and {Van Harmelen}, Frank",
  year      = "2004",
  month     = "8",
  doi       = "10.1016/j.knosys.2004.04.004",
  volume    = "17",
  pages     = "201--206",
  journal   = "Knowledge-Based Systems",
  issn      = "0950-7051",
  publisher = "Elsevier",
  number    = "5-6",
}


@article{ef5224f999734bafbf57098c8ef44c8e,
  title     = "Contextualizing ontologies",
  abstract  = "Ontologies are shared models of a domain that encode a view which is common to a set of different parties. Contexts are local models that encode a party's subjective view of a domain. In this paper, we show how ontologies can be contextualized, thus acquiring certain useful properties that a pure shared approach cannot provide. We say that an ontology is contextualized or, also, that it is a contextual ontology, when its contents are kept local, and therefore not shared with other ontologies, and mapped with the contents of other ontologies via explicit (context) mappings. The result is Context OWL (C-OWL), a language whose syntax and semantics have been obtained by extending the OWL syntax and semantics to allow for the representation of contextual ontologies.",
  keywords  = "Compatibilities, Context OWL, Contextual ontology",
  author    = "Paolo Bouquet and Fausto Giunchiglia and {Van Harmelen}, Frank and Luciano Serafini and Heiner Stuckenschmidt",
  year      = "2004",
  month     = "10",
  doi       = "10.1016/j.websem.2004.07.001",
  volume    = "1",
  pages     = "325--343",
  journal   = "Journal of Web Semantics",
  issn      = "1570-8268",
  publisher = "Elsevier",
  number    = "4",
}


@article{bf6b51361ff04c95bfd87977043a9ef8,
  title     = "Bibster-a semantics-based bibliographic peer-to-peer system",
  abstract  = "This paper describes Bibster, a Peer-to-Peer system for exchanging bibliographic metadata among researchers. We show how Bibster exploits ontologies in data-representation, query formulation, query routing, and query result presentation. The Bibster system is freely available and is used by researchers across multiple organizations.",
  keywords  = "Knowledge management, Peer-to-peer, Semantic web ontologies",
  author    = "Peter Haase and Björn Schnizler and Jeen Broekstra and Marc Ehrig and {Van Harmelen}, Frank and Maarten Menken and Peter Mika and Michal Plechawski and Pawel Pyszlak and Ronny Siebes and Steffen Staab and Christoph Tempich",
  year      = "2004",
  month     = "12",
  doi       = "10.1016/j.websem.2004.09.006",
  volume    = "2",
  pages     = "99--103",
  journal   = "Journal of Web Semantics",
  issn      = "1570-8268",
  publisher = "Elsevier",
  number    = "1",
}


@inbook{c1cba8093b6c4b39a1e7594aacf502af,
  title     = "A framework for handling inconsistency in changing ontologies",
  abstract  = "One of the major problems of large scale, distributed and evolving ontologies is the potential introduction of inconsistencies. In this paper we survey four different approaches to handling inconsistency in DL-based ontologies: consistent ontology evolution, repairing inconsistencies, reasoning in the presence of inconsistencies and multi-version reasoning. We present a common formal basis for all of them, and use this common basis to compare these approaches. We discuss the different requirements for each of these methods, the conditions under which each of them is applicable, the knowledge requirements of the various methods, and the different usage scenarios to which they would apply.",
  author    = "Peter Haase and {Van Harmelen}, Frank and Zhisheng Huang and Heiner Stuckenschmidt and York Sure",
  year      = "2005",
  doi       = "10.1007/11574620_27",
  isbn      = "3540297545",
  volume    = "3729 LNCS",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  pages     = "353--367",
  booktitle = "The Semantic Web, ISWC 2005 - 4th International Semantic Web Conference, ISWC 2005, Proceedings",
}


@inbook{c0344399fef54f54882db3a71064640f,
  title     = "Formalising medical quality indicators to improve guidelines",
  abstract  = "Medical guidelines can significantly improve quality of medical care and reduce costs. But how do we get sound and well-structured guidelines? This paper investigates the use of quality indicators that are formulated by medical institutions to evaluate medical care. The main research questions are (i) whether it is possible to formalise those indicators in a specific knowledge representation language for medical guidelines, and (ii) whether it is possible to verify whether such guidelines do indeed satisfy these indicators. In a case study on two real-life guidelines (Diabetes and Jaundice) we have studied 35 indicators, that were developped independently from these guidelines. Of these 25 (71%!) suggested anomalies in one of the guidelines in our case study.",
  author    = "{Van Gendt}, Marjolein and {Ten Teije}, Annette and Radu Serban and {Van Harmelen}, Frank",
  year      = "2005",
  isbn      = "3540278311",
  volume    = "3581 LNAI",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  pages     = "201--210",
  booktitle = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
}


@inbook{bc56cc474e5740f99e47d0cd22d404be,
  title     = "Introduction to Semantic Web Ontology Languages",
  abstract  = "The aim of this chapter is to give a general introduction to some of the ontology languages that play a prominent role on the Semantic Web, and to discuss the formal foundations of these languages. Web ontology languages will be the main carriers of the information that we will want to share and integrate.",
  author    = "Grigoris Antoniou and Enrico Franconi and {Van Harmelen}, Frank",
  year      = "2005",
  doi       = "10.1007/11526988_1",
  isbn      = "3540278281",
  series    = "Reasoning Web",
  pages     = "1–21",
  booktitle = "Reasoning Web",
}


@article{2416a0dc6c8b4b078331ca2654b6716a,
  title   = "Ontology-driven extraction of linguistic patterns for modelling clinical guidelines",
  author  = "Radu Serban and {Ten Teije}, Annette and {Van Harmelen}, Frank and Mar Marcos and Cristina Polo-Conde",
  year    = "2005",
  pages   = "381--382",
  journal = "Belgian/Netherlands Artificial Intelligence Conference",
  issn    = "1568-7805",
}


@inbook{4c48cb1e8d0c41189180f415718623c3,
  title     = "Ontology-driven extraction of linguistic patterns for modelling clinical guidelines",
  abstract  = "Evidence-based clinical guidelines require frequent updates duo to research and technology advances. The quality of guideline updates can be improved if the knowledge underlying the guideline text is explicitly modelled using the so-called guideline patterns (GPs), mappings between a text fragment arid a formal representation of its corresponding medical knowledge. Ontology-driven extraction of linguistic patterns is a method to automatically reconstruct the control knowledge captured in guidelines, which facilitates a more effective modelling and authoring of clinical guidelines. We illustrate by examples the use of a method for generating and searching for linguistic guideline patterns in the text of a guideline for treatment of breast cancer, and provide a general evaluation of usefulness of these patterns in the modelling of the guideline analyzed.",
  author    = "Radu Serban and Teije, {Annette Ten} and {Van Harmelen}, Frank and Mar Marcos and Cristina Polo-Conde",
  year      = "2005",
  isbn      = "3540278311",
  volume    = "3581 LNAI",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  pages     = "191--200",
  booktitle = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
}


@inbook{a0c3c45a88394f33837eef6806985fda,
  title     = "Ontology mapping: A way out of the medical tower of babel?",
  author    = "{Van Harmelen}, Frank",
  year      = "2005",
  isbn      = "3540278311",
  volume    = "3581 LNAI",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  pages     = "3--6",
  booktitle = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
}


@inbook{d1b1ca8ecf1e4b0881d54557d518f794,
  title     = "Query Processing in Ontology-Based Peer-to-Peer Systems",
  abstract  = "The unstructured, heterogeneous and dynamic nature of the Web poses a new challenge to query-answering over multiple data sources. The so-called Semantic Web aims at providing more and semantically richer structures in terms of ontologies and meta-data. A problem that remains is the combined use of heterogeneous sources. In a dynamic environment, it is no longer realistic to assume that the involved data sources act as if they were a single (virtual) source, modelled as a global schema, as is done in classical data integration approaches. In this paper, we propose an alternative approach where we replace the role of a single virtual data source schema with a peer-to-peer approach relying on limited shared (or: overlapping) vocabularies between peers. Since overlaps between vocabularies of peers will be limited and the dynamic nature of the system prohibits the design of accurate mappings, query processing will have to be approximate. We provide a formal model for such approximate query processing based on limited shared vocabularies between peers, and we show how the quality of the approximation can be adjusted in a gradual manner. The result is a flexible architecture for query-processing in heterogenous and dynamic environments, based on a formal foundation. We present the approach and discuss it on the basis of a case study.",
  keywords  = "knowledge sharing, knowledge-based mediation architectures, methods and formalisms for, semantic web",
  author    = "Heiner Stuckenschmidt and Harmelen, {Frank Van} and Fausto Giunchiglia",
  year      = "2005",
  doi       = "10.1007/3-7643-7361-X_7",
  isbn      = "978-3-7643-7361-0",
  series    = "Ontologies for Agents Theory and Experiences",
  pages     = "145--167",
  booktitle = "Ontologies for Agents Theory and Experiences",
}


@article{691b809a87584cfb83812192f176a047,
  title    = "Reasoning with inconsistent ontologies",
  abstract = "In this paper we present a framework of reasoning with inconsistent ontologies, in which pre-defined selection functions are used to deal with concept relevance. We examine how the notion of {"}concept relevance{"} can be used for reasoning with inconsistent ontologies. We have implemented a prototype called PION (Processing Inconsistent ONtologies), which is based on a syntactic relevance-based selection function. In this paper, we also report the experiments with PION.",
  author   = "Zhisheng Huang and {Van Harmelen}, Frank and {Ten Teije}, Annette",
  year     = "2005",
  pages    = "454--459",
  journal  = "IJCAI International Joint Conference on Artificial Intelligence",
  issn     = "1045-0823",
}


@article{48688278661c4d0db7292bfcf5fb7ce6,
  title     = "A quantitative analysis of the robustness of knowledge-based systems through degradation studies",
  abstract  = "The overall aim of this paper is to provide a general setting for quantitative quality measures of knowledge-based system behaviour that is widely applicable to many knowledge-based systems. We propose a general approach that we call degradation studies: an analysis of how system output changes as a function of degrading system input, such as incomplete or incorrect data or knowledge. To show the feasibility of our approach, we have applied it in a case study. We have taken a large and realistic vegetation-classification system, and have analysed its behaviour under various varieties of incomplete and incorrect input. This case study shows that degradation studies can reveal interesting and surprising properties of the system under study.",
  keywords  = "Knowledge-based systems, Quantitative analysis, Robust behaviour, Validation",
  author    = "Perry Groot and {ten Teije}, Annette and {van Harmelen}, Frank",
  year      = "2005",
  month     = "2",
  doi       = "10.1007/s10115-003-0140-7",
  volume    = "7",
  pages     = "224--245",
  journal   = "Knowledge and Information Systems",
  issn      = "0219-1377",
  publisher = "Springer London",
  number    = "2",
}


@inbook{8c39755717ec492b8f29f86f653d2ade,
  title     = "Approximate semantic matching of music classes on the internet",
  abstract  = "We address the problem of semantic matching, which concerns the search for semantic agreement between heterogeneous concept hierarchies. We propose a new approximation method to discover and assess the {"}strength{"} (preciseness) of a semantic match between concepts from two such concept hierarchies. We apply the method in the music domain, and present the results of preliminary tests on concept hierarchies from actual sites on the Internet.",
  keywords  = "approximation, Music genre, music style, ontology, semantic matching, semantic web",
  author    = "Zharko Aleksovski and Kate, {Warner Ten} and {Van Harmelen}, Frank",
  year      = "2006",
  doi       = "10.1007/1-4020-4995-1_9",
  isbn      = "9781402049538",
  pages     = "133--147",
  booktitle = "Intelligent Algorithms in Ambient and Biomedical Computing",
  publisher = "Springer Netherlands",
}


@inbook{cf8614b39e9f4861839dd9ad3025e60f,
  title     = "Bibster-a semantics-based bibliographic peer-to-peer system",
  abstract  = "This chapter describes the design, implementation, and evaluation of Bibster, a Peer-to-Peer system for exchanging bibliographic data among researchers. Bibster exploits ontologies in data-storage, query formulation, query-routing and answer presentation: When bibliographic entries are made available for use in Bibster, they are structured and classified according to two different ontologies. This ontological structure is then exploited to help users formulate their queries. Subsequently, the ontologies are used to improve query routing across the Peer-to-Peer network. Finally, the ontologies are used to post-process the returned answers in order to do duplicate detection. The chapter describes each of these ontology-based aspects of Bibster.",
  author    = "Peter Haase and Björn Schnizler and Jeen Broekstra and Marc Ehrig and {Van Harmelen}, Frank and Maarten Menken and Peter Mika and Michal Plechawski and Pawel Pyszlak and Ronny Siebes and Steffen Staab and Christoph Tempich",
  year      = "2006",
  doi       = "10.1007/3-540-28347-1_19",
  isbn      = "3540283463",
  pages     = "349--363",
  booktitle = "Semantic Web and Peer-to-Peer: Decentralized Management and Exchange of Knowledge and Information",
  publisher = "Springer Berlin / Heidelberg",
}


@inbook{f8169997e7ad44b59c25de04c6391b3a,
  title     = "Expertise-based peer selection",
  abstract  = "Peer-to-Peer systems have proven to be an effective way of sharing data. Finding the data in an efficient and robust manner still is a challenging problem. We propose a model in which peers advertise their expertise in the Peer-to-Peer network. The knowledge about the expertise of other peers forms a semantic overlay network (SON). Based on the semantic similarity between the subject of a query and the expertise of other peers, a peer can select appropriate peers to forward queries to, instead of broadcasting the query or sending it to a random set of peers. We evaluate the model in a bibliographic scenario, where peers share bibliographic descriptions of publications among each other. In simulation experiments complemented with a real-world field experiment we show how expertise based peer selection improves the performance of a Peer-to-Peer system with respect to precision, recall and the number of messages.",
  author    = "Ronny Siebes and Peter Haase and {Van Harmelen}, Frank",
  year      = "2006",
  doi       = "10.1007/3-540-28347-1_7",
  isbn      = "3540283463",
  pages     = "125--142",
  booktitle = "Semantic Web and Peer-to-Peer: Decentralized Management and Exchange of Knowledge and Information",
  publisher = "Springer Berlin / Heidelberg",
}


@article{29c60e65ac184520ae61b41b5302c78a,
  title     = "Exploiting the structure of background knowledge used in ontology matching",
  abstract  = "We investigate the use of a background knowledge ontology in ontology matching. We conducted experiments on matching two medical ontologies using a third extensive one as background knowledge, and compare the results with directly matching the two ontologies. Our results indicate that using background knowledge, in particular the exploitation of its structure, has enormous benefits on the matching. The structure of the background ontology needs closer examination to determine how to use it in order to obtain maximal benefit.",
  author    = "Zharko Aleksovski and {Ten Kate}, Warner and {Van Harmelen}, Frank",
  year      = "2006",
  volume    = "225",
  journal   = "CEUR workshop proceedings",
  issn      = "1613-0073",
  publisher = "CEUR Workshop Proceedings",
}


@inbook{3f8e8b2550a542fe90820d10ce0ce4c2,
  title     = "From natural language to formal proof goal : Structured goal formalisation applied to medical guidelines",
  abstract  = "The main problem encountered when starting verification of goals for some formal system, is the ambiguity of those goals when they are specified in natural language. To verify goals given in natural language, a translation of those goals to the formalism of the verification tool is required. The main concern is to assure equivalence of the final translation and the original. A structured method is required to assure equivalence in every case. This article proposes a goal formalisation method in five steps, in which the domain expert is involved in such a way that the correctness of the result can be assured. The contribution of this article is a conceptual goal model, a formal expression language for this model, and a structured method which transforms any input goal to a fully formalised goal in the required target formalism. The proposed formalisation method guarantees essential properties like correctness, traceability, reduced variability and reusability.",
  author    = "S. Staab and V. Svatek and Ruud Stegers and {ten Teije}, Annette and {van Harmelen}, Frank",
  year      = "2006",
  doi       = "10.1007/11891451",
  isbn      = "3540463631",
  series    = "EKAW",
  publisher = "Springer",
  pages     = "51--58",
  editor    = "S Staab and V Svatek",
  booktitle = "EKAW",
}


@inbook{b9e4e042dd474445b581ce0311d52bd8,
  title     = "From natural language to formal proof goal* structured goal formalisation applied to medical guidelines (extended abstract)",
  abstract  = "The main problem encountered when starting verification of goals for some formal system, is the ambiguity of those goals when they are specified in natural language. To verify goals given in natural language, a translation of those goals to the formalism of the verification tool is required. The main concern is to assure equivalence of the final translation and the original. A structured method is required to assure equivalence in every case. This article proposes a goal formalisation method in five steps, in which the domain expert is involved in such a way that the correctness of the result can be assured. The contribution of this article is a conceptual goal model, a formal expression language for this model, and a structured method which transforms any input goal to a fully formalised goal in the required target formalism. The proposed formalisation method guarantees essential properties like correctness, traceability, reduced variability and reusability.",
  author    = "Ruud Stegers and Teije, {Annette Ten} and {Van Harmelen}, Frank",
  year      = "2006",
  isbn      = "3540463631",
  volume    = "4248 LNAI",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "51--58",
  booktitle = "Managing Knowledge in a World of Networks - 15th International Conference, EKAW 2006, Proceedings",
}


@book{349d90feb5a84c57874612916185947c,
  title     = "Fuzzy Logic and the Semantic Web",
  abstract  = "The publication Capturing Intelligence, Volume 1, focuses on research from all disciplines related to the issue of understanding and reproducing intelligent artificial systems. The volume presents a classical, rich and well-established area—namely the representation of fuzzy, noncrisp concepts, with a new and highly exciting challenge—namely the vision of the Semantic Web. The insight that any realistic approach to the Semantic Web will have to take into account the lessons from fuzzy logic approaches is gaining ground in a wide community. Higher precision of search engines, searching with semantic instead of syntactic queries, information integration among different web-resources, personalization, and semantic web services, are all part of the promises of this vision. The publication discusses a number of ways in which current Semantic Web technology can be “fuzzified,” and a number of ways in which such fuzzy representations can be used for tasks, such as search and information retrieval.",
  author    = "{van Harmelen}, Frank",
  year      = "2006",
  doi       = "10.1016/S1574-9576(06)80001-8",
  isbn      = "9780444519481",
  volume    = "1",
  publisher = "Elsevier",
}


@inbook{5c8046aa6f4542cfaf255e4fcc6c590c,
  title     = "Matching unstructured vocabularies using a background ontology",
  abstract  = "Existing ontology matching algorithms use a combination of lexical and structural correspondence between source and target ontologies. We present a realistic case-study where both types of overlap are low: matching two unstructured lists of vocabulary used to describe patients at Intensive Care Units in two different hospitals. We show that indeed existing matchers fail on our data. We then discuss the use of background knowledge in ontology matching problems. In particular, we discuss the case where the source and the target ontology are of poor semantics, such as flat lists, and where the background knowledge is of rich semantics, providing extensive descriptions of the properties of the concepts involved. We evaluate our results against a Gold Standard set of matches that we obtained from human experts.",
  author    = "Zharko Aleksovski and Michel Klein and {Ten Kate}, Warner and {Van Harmelen}, Frank",
  year      = "2006",
  doi       = "10.1007/11891451_18",
  isbn      = "3540463631",
  volume    = "4248 LNAI",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "182--197",
  booktitle = "Managing Knowledge in a World of Networks - 15th International Conference, EKAW 2006, Proceedings",
}


@inbook{ff0bc3281717488393369ce4d765b3e4,
  title     = "Meaning on the web: Evolution vs intelligent design?",
  abstract  = "It is a truism that as the Web grows in size and scope, it becomes harder to find what we want, to identify like-minded people and communities, to find the best ads to offer, and to have applications work together smoothly. Services don't interoperate; queries yield long lists of results, most of which seem to miss the point. If the Web were a person, we would expect richer and more successful interactions with it - interactions that were, quite literally, more meaningful. That's because in human discourse, it is shared meaning that gives us real communication. Yet with the current Web, meaning cannot be found.Much recent work has aspired to change this, both for human-machine interchange and machine-machine synchronization. Certainly the {"}semantic web{"} looks to add meaning to our current simplistic matching of mere strings of characters against mere {"}bags{"} of words. But can we legislate meaning from on high? Isn't meaning organic and determined by use, a moving and context-dependent target? But if meaning is an evolving organic soup, how are humans able to get anything done with one another? Don't we love to {"}define our terms{"}? But then again, is real definition even possible?These questions have daunted philosophers for years, and we probably won't solve them here. But we'll try to understand what's at the root of our own current religious debate: should meaning on the Web be evolutionary, driven organically through the bottom-up human assignment of tags? Or does it need to be carefully crafted and managed by a higher authority, using structured representations with defined semantics? Without picket signs or violence (we hope), our panelists will explore the two extreme ends of the spectrum - and several points in between.",
  author    = "Ron Brachman and Dan Connolly and Rohit Khare and Frank Smadja and {Van Harmelen}, Frank",
  year      = "2006",
  doi       = "10.1145/1135777.1135886",
  isbn      = "1595933239",
  pages     = "745",
  booktitle = "Proceedings of the 15th International Conference on World Wide Web",
}


@inbook{6b853186f81545c598c481232c118f98,
  title     = "Ontology-based information visualization: Toward semantic web applications",
  abstract  = "This chapter has demonstrated an elegant way to visually represent ontological data. We have described how the Cluster Map visualization can use ontologies to create expressive information visualizations, with the attractive property that classes and objects that are semantically related are also spatially close in the visualization. Another key aspect of the visualization is that it focuses on visualizing instances rather than ontological models, thereby making it very useful for information retrieval purposes. A number of applications developed in the past few years have been described that prominently incorporate the Cluster Map visualization. Based on these descriptions, we could distinguish a number of generic information retrieval tasks that are well supported by the visualization. These applications prove the usability and usefulness of the Cluster Map in real-life scenarios. Furthermore, these applications show the applicability of the visualization in Semantic Web-based environments, where lightweight ontologies are playing a crucial role in organizing and accessing heterogeneous and decentralized information sources.",
  author    = "Christiaan Fluit and Marta Sabou and {Van Harmelen}, Frank",
  year      = "2006",
  doi       = "10.1007/1-84628-290-X_3",
  isbn      = "1852339764",
  pages     = "45--58",
  booktitle = "Visualizing the Semantic Web: XML-Based Internet and Information Visualization",
  publisher = "Springer London",
}


@inbook{82e49ce7bc05475ebaa9832d22a040e3,
  title     = "Ontology-Based Information Visualization: Toward Semantic Web Applications",
  abstract  = "The Semantic Web is an extension of the current World Wide Web, based on the idea of exchanging information with explicit, formal, and machine-accessible descriptions of meaning. Providing information with such semantics will enable the construction of applications that have an increased awareness of what is contained in the information they process and that can therefore operate more accurately. This has the potential of improving the way we deal with information in the broadest sense possible, for example, better search engines, mobile agents for various tasks, or even applications yet unheard of. Rather than being merely a vision, the Semantic Web has significant backing from various institutes such as DARPA, the European Union, and the W3C, all of which have performed a variety of Semantic Web activities. In order to be able to exchange the semantics of information, one first needs to agree on how to explicitly model it. Ontologies are a mechanism for representing such formal and shared domain descriptions. They can be used to annotate data with labels (metadata) indicating their meaning, thereby making their semantics explicit and machine-accessible. Many Semantic Web initiatives emphasize the capability of machines to exchange the meaning of information. Although their efforts will lead to an increased quality of the application's results, their user interfaces often take little or no advantage of the increased semantics. For example, an ontology-based search engine could use its ontology to enrich the presentation of the resulting list to the end user, for example, by replacing the endless list of hits with a navigation structure based on the semantics of the hits. Visualization is becoming increasingly important in Semantic Web tools. In par-ticular, visualization is used in tools that support the development of ontologies, such as ontology extraction tools (OntoLift, Text-to-Onto) or ontology editors (Protégé, OntoLift). The intended users of these tools are ontology engineers that need to gain an insight into the complexity of the ontology. Therefore, these tools employ schema visualization techniques that primarily focus on the structure of the ontology (i.e., its concepts and their relationships). We presented a detailed overview of these tools in Fluit et al. (2003). The Cluster Map visualization technique, developed by the Dutch software vendor Aduna (http://aduna.biz), bridges the gap between complex semantic structures and 45 46 Visualizing the Semantic Web their simple, intuitive user-oriented presentation. It presents semantic data to end users who want to leverage the benefits of Semantic Web technology without being burdened with the complexity of the underlying metadata. For end users, instance information is often more important than the structure of the ontology that is used to describe these instances. Accordingly, the Cluster Map technique focuses on visualizing instances and their classifications according to the concepts of the ontology. We have reported in previous work (Fluit et al., 2002; 2003) on case studies that exploit the expressive power of this technique. Since then, the growth of the Semantic Web has made it possible to take this technology a step further and integrate it in three different applications. Two of them are employed within Semantic Web research projects. The third is a commercial information retrieval application. These appli-cations exhibit the characteristics of a typical Semantic Web tool: they provide easy (visual) access to a set of heterogeneous, distributed data sources and rely on Semantic Web encoding languages and storage facilities for the manipulation of the visualized data. This chapter is centered on the description of these three applications. First, we will explain the contents of the Cluster Map visualization and the kind of ontologies it visualizes in Section 3.2. Section 3.3 presents the three real-life applications that incorporate the visualization. These two sections lead to a discussion in Section 3.4 on how the visualization can support several user tasks, such as analysis, search, and exploration. Some considerations for future work and a summary conclude this chapter. 3.2 Cluster Map Basics",
  author    = "Christiaan Fluit and Marta Sabou and Harmelen, {Frank van}",
  year      = "2006",
  doi       = "10.1109/IV.2001.942109",
  isbn      = "0-7695-1195-3",
  series    = "Visualizing the Semantic Web",
  pages     = "45--58",
  booktitle = "Visualizing the Semantic Web",
}


@inbook{7be36f1ce0cd45189c97468ea5ffb6ed,
  title     = "Peer-to -peer and semantic web",
  abstract  = "Just as the industrial society of the last century depended on natural resources, today's society depends on information. A lack of resources in the industrial society hindered development just as a lack of information hinders development in the information society. Consequently, the exchange of information becomes essential for more and more areas of society: Companies announce their products in online marketplaces and exchange electronic orders with their suppliers; in the medical area patient information is exchanged between general practitioners, hospitals and health insurances; public administration receive tax information from employers and offer online services to their citizens. As a reply to this increasing importance of information exchange, new technologies supporting a fast and accurate information exchange are being developed. Prominent examples of such new technologies are so-called Semantic Web and Peer-to-Peer technologies. These technologies address different aspects of the inherit complexity of information exchange. Semantic Web Technologies address the problem of information complexity by providing advanced support for representing and processing information. Peer-to-Peer technologies, on the other hand, address system complexity by allowing flexible and decentralized information storage and processing.",
  author    = "Heiner Stuckenschmidt and {Van Harmelen}, Frank and Wolf Siberski and Steffen Staab",
  year      = "2006",
  doi       = "10.1007/3-540-28347-1_1",
  isbn      = "3540283463",
  pages     = "1--17",
  booktitle = "Semantic Web and Peer-to-Peer: Decentralized Management and Exchange of Knowledge and Information",
  publisher = "Springer Berlin / Heidelberg",
}


@inbook{5e8f12c030ee4ef3b38f85a725924c7e,
  title     = "Semantic Web research anno 2006: Main streams, popular fallacies, current status and future challenges",
  abstract  = "In this topical paper we try to give an analysis and overview of the current state of Semantic Web research. We point to different interpretations of the Semantic Web as the reason underlying many controversies, we list (and debunk) four false objections which are often raised against the Semantic Web effort. We discuss the current status of the Semantic Web work by reviewing the current answers to four central research questions that need to be answered, and by surveying the uptake of Semantic Web technology in different application areas. Finally, we try to identify the main challenges facing the Semantic Web community.",
  author    = "{Van Harmelen}, Frank",
  year      = "2006",
  isbn      = "354038569X",
  volume    = "4149 LNAI",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "1--7",
  booktitle = "Cooperative Information Agents X - 10th International Workshop, CIA 2006. Proceedings",
}


@inbook{c98302a518384013910c9263c39e6d7a,
  title     = "Where does it break? or: Why the semantic web is not just {"}research as usual{"}",
  abstract  = "Work on the Semantic Web is all too often phrased as a technological challenge: how to improve the precision of search engines, how to personalise web-sites, how to integrate weakly-structured data-sources, etc. This suggests that we will be able to realise the Semantic Web by merely applying (and at most refining) the results that are already available from many branches of Computer Science. I will argue in this talk that instead of (just) a technological challenge, the Semantic Web forces us to rethink the foundations of many subfields of Computer Science. This is certainly true for my own field (Knowledge Representation), where the challenge of the Semantic Web continues to break many often silently held and shared assumptions underlying decades of research. With some caution, I claim that this is also true for other fields, such as Machine Learning, Natural Language Processing, Databases, and others. For each of these fields, I will try to identify silently held assumptions which are no longer true on the Semantic Web, prompting a radical rethink of many past results from these fields.",
  author    = "{Van Harmelen}, Frank",
  year      = "2006",
  isbn      = "3540345442",
  volume    = "4011 LNCS",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "1",
  booktitle = "The Semantic Web: Research and Applications: 3rd European Semantic Web Conference, ESWC 2006 Proceedings",
}


@article{2e2cfe4d2d6840a9af6f68e04ad13615,
  title     = "Improving medical protocols by formal methods",
  abstract  = "OBJECTIVES: During the last decade, evidence-based medicine has given rise to an increasing number of medical practice guidelines and protocols. However, the work done on developing and distributing protocols outweighs the efforts on guaranteeing their quality. Indeed, anomalies like ambiguity and incompleteness are frequent in medical protocols. Recent efforts have tried to address the problem of protocol improvement, but they are not sufficient since they rely on informal processes and notations. Our objective is to improve the quality of medical protocols.APPROACH: The solution we suggest to the problem of quality improvement of protocols consists in the utilisation of formal methods. It requires the definition of an adequate protocol representation language, the development of techniques for the formal analysis of protocols described in that language and, more importantly, the evaluation of the feasibility of the approach based on the formalisation and verification of real-life medical protocols. For the first two aspects we rely on earlier work from the fields of knowledge representation and formal methods. The third aspect, i.e. the evaluation of the use of formal methods in the quality improvement of protocols, constitutes our main objective. The steps with which we have carried out this evaluation are the following: (1) take two real-life reference protocols which cover a wide variety of protocol characteristics; (2) formalise these reference protocols; (3) check the formalisation for the verification of interesting protocol properties; and (4) determine how many errors can be uncovered in this way.RESULTS: Our main results are: a consolidated formal language to model medical practice protocols; two protocols, each both modelled and formalised; a list of properties that medical protocols should satisfy; verification proofs for these protocols and properties; and perspectives of the potentials of this approach. Our results have been evaluated by a panel of medical experts, who judged that the problems we detected in the protocols with the help of formal methods were serious and should be avoided.CONCLUSIONS: We have succeeded in demonstrating the feasibility of formal methods for improving medical protocols.",
  keywords  = "Artificial Intelligence, Clinical Protocols, Feasibility Studies, Humans, Infant, Newborn, Jaundice, Neonatal, Practice Guidelines as Topic, Programming Languages, Quality Assurance, Health Care, Journal Article, Research Support, Non-U.S. Gov't",
  author    = "{ten Teije}, Annette and Mar Marcos and Michel Balser and {van Croonenborg}, Joyce and Christoph Duelli and {van Harmelen}, Frank and Peter Lucas and Silvia Miksch and Wolfgang Reif and Rosenbrand, {Kitty C J G M} and Andreas Seyfang",
  year      = "2006",
  month     = "3",
  doi       = "10.1016/j.artmed.2005.10.006",
  volume    = "36",
  pages     = "193--209",
  journal   = "Artificial Intelligence in Medicine",
  issn      = "0933-3657",
  publisher = "Elsevier",
  number    = "3",
}


@inbook{bb952e2524ac44c3a57a5fd2d13f4f8d,
  title     = "Reasoning With Inconsistent Ontologies: Framework, Prototype, and Experiment",
  keywords  = "Classical logical inference engines, DICE (Diagnoses for Intensive Care Evaluation), Inconsistency reasoner and selection function, Linear extension and linear reduction strategy, Logical entailment, Polysemy, Prototype of PION using SWI-Prolog, Reasoning with inconsistent ontologies, Syntactic relevance-based selection functions",
  author    = "Zhisheng Huang and Harmelen, {Frank Van} and {Ten Teije}, Annette",
  year      = "2006",
  month     = "7",
  doi       = "10.1002/047003033X.ch5",
  isbn      = "0470025964",
  pages     = "71--93",
  booktitle = "Semantic Web Technologies: Trends and Research in Ontology-based Systems",
  publisher = "John Wiley & Sons, Ltd",
}


@inbook{b324c489cc7c404a8db70f02dab314dd,
  title     = "Thesaurus-based Retrieval of Case Law",
  abstract  = "In the context of intelligent disclosure of case law, we report on our findings on methods for retrieving relevant case law within the domain of tort law from a repository of 68.000 court verdicts. We apply a thesaurus-based technique to find specific legal situations. It appears that statistical measures of term relevance are not sufficient, but that explicit knowledge about specific formulations used in law and case law are required to distinguish relevant case law from irrelevant. In addition, we found out that the retrieving legal concepts with an ``interpretive'' character requires a different method than concepts do not require additional interpretation.",
  author    = "Klein, {Michel C. A.} and {van Steenbergen}, Wouter and Uijttenbroek, {Elisabeth M.} and Lodder, {Arno R.} and Harmelen, {Frank van}",
  year      = "2006",
  month     = "12",
  isbn      = "978-1-58603-698-0",
  pages     = "61--70",
  editor    = "Engers, {Tom van}",
  booktitle = "Proceedings of the 19th International JURIX conference: JURIX 2006",
  publisher = "IOS Press",
}


@article{e648354f2c7c42ea84dd9e30a3b23574,
  title     = "Anytime classification by ontology approximation",
  abstract  = "Reasoning with large or complex ontologies is one of the bottle-necks of the Semantic Web. In this paper we present an anytime algorithm for classification based on approximate subsumption. We give the formal definitions for approximate subsumption, and show its monotonicity and soundness; we show how it can be computed in terms of classical subsumption; and we study the computational behaviour of the algorithm on a set of realistic benchmarks. The most interesting finding is that anytime classification works best on ontologies where classical subsumption is hardest to compute.",
  author    = "S. Schlobach and E. Blaauw and {El Kebir}, M. and {Ten Teije}, A. and {Van Harmelen}, F. and S. Bortoli and M.C. Hobbelman and K. Millian and Y. Ren and S. Stam and P. Thomassen and {Van Het Schip}, R. and {Van Willigem}, W.",
  year      = "2007",
  volume    = "291",
  journal   = "CEUR workshop proceedings",
  issn      = "1613-0073",
  publisher = "CEUR Workshop Proceedings",
}


@inbook{ab77811a4b954f9a992b4347c8b7bd11,
  title     = "Case law retrieval by concept search and visualization",
  abstract  = "The BEST-project (BATNA Establishment using Semantic web Technology, http://best-project.nl) strives to provide disputing parties with information about their legal position in a liability case. Our assumption is that through intelligent disclosure of Dutch Tort Law cases, laymen can estimate their chances: information derived from previous court decisions can help to obtain insight into BATNAs (Best Alternative to a Negotiated Agreement), alternatives a party has if negotiation fails. Information BATNAs also contributes to determining the room left for negotiation.",
  author    = "Uijttenbroek, {Elisabeth M.} and Klein, {Michel C.A.} and Lodder, {Arno R.} and {Van Harmelen}, Frank",
  year      = "2007",
  doi       = "10.1145/1276318.1276336",
  isbn      = "1595936807",
  pages     = "95--96",
  booktitle = "Proceedings of the Eleventh International Conference on Artificial Intelligence and Law",
}


@inbook{a0f178afe3e64189b27e076b91f617bd,
  title     = "Dynamic aspects of OPJK legal ontology",
  abstract  = "The OPJK (Ontology of Professional Judicial Knowledge) is a legal ontology developed to map questions of junior judges to a set of stored frequently asked questions. In this paper, we investigate dynamic and temporal aspects of one of the SEKT legal ontologies, by subjecting the ontology OPJK to MORE, a multi-version ontologies reasoning Sys-tem. MORE is based on a temporal logic approach. We show how the temporal logic approach can be used to obtain a better understanding of dynamic and temporal evolution of legal ontologies.",
  author    = "Zhisheng Huang and Stefan Schlobach and {Van Harmelen}, Frank and Núria Casellas and Pompeu Casanovas",
  year      = "2007",
  doi       = "10.1007/978-3-540-85569-9-8",
  isbn      = "3540855688",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  pages     = "113--129",
  booktitle = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
}


@inbook{a3658c341e5b4fb19372c0d8f012020e,
  title     = "Media, politics and the semantic web",
  abstract  = "The media play an important role in the functioning of our society. This role is extensively studied by Communication Scientists, requiring a systematic analysis of media content. The methods developed in this field utilize complex data models and background knowledge. This data is generally represented ad hoc, making it difficult to analyze, combine and share data sets. In this paper we present our work on formalizing this representation using RDF(S). We discuss the requirements for a good representation, highlighting a number of non-trivial modeling decisions. We conclude with a description of the resulting system and the benefits for a recent investigation of the 2006 Dutch parliamentary campaign. This case study shows concrete improvements for annotating, querying, and analyzing data, but also indicates a number of aspects that were more difficult to model in RDF(S), contributing to the discussion on modeling with and improving RDF(S) and associated tools.",
  author    = "{Van Atteveldt}, Wouter and Stefan Schlobach and {Van Harmelen}, Frank",
  year      = "2007",
  isbn      = "3540726667",
  volume    = "4519 LNCS",
  pages     = "205--219",
  booktitle = "The Semantic Web: Research and Applications - 4th European Semantic Web Conference, ESWC 2007, Proceedings",
}


@inbook{c8660026c38c49ad891322ef6124401f,
  title     = "The OpenKnowledge system: An interaction-centered approach to knowledge sharing",
  abstract  = "The information that is made available through the semantic web will be accessed through complex programs (web-services, sensors, etc.) that may interact in sophisticated ways. Composition guided simply by the specifications of programs' inputs and outputs is insufficient to obtain reliable aggregate performance - hence the recognised need for process models to specify the interactions required between programs. These interaction models, however, are traditionally viewed as a consequence of service composition rather than as the focal point for facilitating composition. We describe an operational system that uses models of interaction as the focus for knowledge exchange. Our implementation adopts a peer to peer architecture, thus making minimal assumptions about centralisation of knowledge sources, discovery and interaction control.",
  author    = "Ronny Siebes and Dave Dupplaw and Spyros Kotoulas and {De Pinninck}, {Adrian Perreau} and {Van Harmelen}, Frank and David Robertson",
  year      = "2007",
  isbn      = "9783540768463",
  volume    = "4803 LNCS",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  number    = "PART 1",
  pages     = "381--390",
  booktitle = "On the Move to Meaningful Internet Systems 2007: CoopIS, DOA, ODBASE, GADA, and IS - OTM Confederated International Conferences CoopIS, DOA, ODBASE, GADA, and IS 2007, Proceedings",
  edition   = "PART 1",
}


@inbook{fe168c99784142baaf7ab0ded58564ca,
  title     = "The role of model checking in critiquing based on clinical guidelines",
  abstract  = "Medical critiquing systems criticise clinical actions performed by a physician. In order to provide useful feedback, an important task is to find differences between the actual actions and a set of 'ideal' actions as described by a clinical guideline. In case differences exist, insight to which extent they are compatible is provided by the critiquing system. We propose a methodology for such critiquing, where the ideal actions are given by a formal model of a clinical guideline, and where the actual actions are derived from real world patient data. We employ model checking to investigate whether a part of the actual treatment is consistent with the guideline. Furthermore, it is shown how critiquing can be cast in terms of temporal logic, and what can be achieved by using model checking. The methodology has been applied to a clinical guideline of breast cancer in conjunction with breast cancer patient data.",
  author    = "Perry Groot and Arjen Hommersom and Peter Lucas and Radu Serban and {Ten Teije}, Annette and {Van Harmelen}, Frank",
  year      = "2007",
  isbn      = "3540735984",
  volume    = "4594 LNAI",
  pages     = "411--420",
  booktitle = "Artificial Intelligence in Medicine - 11th Conference on Artificial Intelligence in Medicine, AIME 2007, Proceedings",
}


@inbook{5f5655376ba6488f9ac5430a4320ed17,
  title     = "Two obvious intuitions: Ontology-mapping needs background knowledge and approximation",
  abstract  = "Ontology mapping (or: ontology alignment, or integration) is one of the most active areas the Semantic Web area. An increasing amount of ontologies are becoming available in recent years, and if the Semantic Web is to be taken seriously, the problem of ontology mapping must be solved. Numerous approaches are being proposed, a yearly competition is being organized, and a number of survey papers have appeared. Nevertheless, with only a few exceptions, two obvious intuitions on ontology mapping have been overlooked: if humans perform {"}ontology mapping{"} in their daily life (a task we all solve every day), they do not do this in a vacuum. Instead, they exploit a rich body of background knowledge already shared by both agents involved in the mapping process. Similarly, humans do not expect that their daily-life ontology mapping is perfect. We can very well cope with approximate translations between concepts used by different agents (in fact, we are so good at it that we barely notice that we do this). In this talk I will discuss recent work where we have quantitatively shown that indeed, ontology mapping can benefit from background knowledge, and that, somewhat surprisingly, more background knowledge leads to continuously improving results. We also discuss how the use of such background knowledge can be exploited to find approximate mappings when perfect mappings cannot be found.",
  author    = "{Van Harmelen}, Frank",
  year      = "2007",
  doi       = "10.1109/IAT.2006.127",
  isbn      = "9780769527482",
  pages     = "11",
  booktitle = "Proceedings - 2006 IEEE/WIC/ACM International Conference on Intelligent Agent Technology (IAT 2006 Main Conference Proceedings), IAT'06",
}


@inbook{1b123029e3cf4002b156f95cf12584d8,
  title     = "Using Google distance to weight approximate ontology matches",
  abstract  = "Discovering mappings between concept hierarchies is widely regarded as one of the hardest and most urgent problems facing the Semantic Web. The problem is even harder in domains where concepts are inherently vague and ill-defined, and cannot be given a crisp definition. A notion of approximate concept mapping is required in such domains, but until now, no such notion is vailable. The first contribution of this paper is a definition for approximate mappings between concepts. Roughly, a mapping between two concepts is decomposed into a number of submappings, and a sloppiness value determines the fraction of these submappings that can be ignored when establishing the mapping. A potential problem of such a definition is that with an increasing sloppiness value, it will gradually allow mappings between any two arbitrary concepts. To improve on this trivial behaviour, we need to design a heuristic weighting which minimises the sloppiness required to conclude desirable matches, but at the same time maximises the sloppiness required to conclude undesirable matches. The second contribution of this paper is to show that a Google based similarity measure has exactly these desirable properties. We establish these results by experimental validation in the domain of musical genres. We show that this domain does suffer from ill-defined concepts. We take two real-life genre hierarchies from the Web, we compute approximate mappings between them at varying levels of sloppiness, and we validate our results against a handcrafted Gold Standard. Our method makes use of the huge amount of knowledge that is implicit in the current Web, and exploits this knowledge as a heuristic for establishing approximate mappings between ill-defined concepts.",
  keywords  = "Approximation, Google distance",
  author    = "Risto Gligorov and {Ten Kate}, Warner and Zharko Aleksovski and {Van Harmelen}, Frank",
  year      = "2007",
  doi       = "10.1145/1242572.1242676",
  isbn      = "1595936548",
  pages     = "767--776",
  booktitle = "16th International World Wide Web Conference, WWW2007",
}


@article{283379e3418b40d8b9471f6e8e144a1a,
  title     = "Extraction and use of linguistic patterns for modelling medical guidelines",
  abstract  = "Objective: The quality of knowledge updates in evidence-based medical guidelines can be improved and the effort spent for updating can be reduced if the knowledge underlying the guideline text is explicitly modelled using the so-called linguistic guideline patterns, mappings between a text fragment and a formal representation of its corresponding medical knowledge. Methods and material: Ontology-driven extraction of linguistic patterns is a method to automatically reconstruct the control knowledge captured in guidelines, which facilitates a more effective modelling and authoring of medical guidelines. We illustrate by examples the use of this method for generating and instantiating linguistic patterns in the text of a guideline for treatment of breast cancer, and evaluate the usefulness of these patterns in the modelling of this guideline. Results: We developed a methodology for extracting and using linguistic patterns in guideline formalization, to aid the human modellers in guideline formalization and reduce the human modelling effort. Using automatic transformation rules for simple linguistic patterns, a good recall (between 72% and 80%) is obtained in selecting the procedural knowledge relevant for the guideline model, even though the precision of the guideline model generated automatically covers only between 20% and 35% of the human-generated guideline model. These results indicate the suitability of our method as a pre-processing step in medical guideline formalization. Conclusions: Modelling and authoring of medical texts can benefit from our proposed method. As pre-requisites for generating automatically a skeleton of the guideline model from the procedural part of the guideline text, to aid the human modeller, the medical terminology used by the guideline must have a good overlap with existing medical thesauri and its procedural knowledge must obey linguistic regularities that can be mapped into the control constructs of the target guideline modelling language.",
  keywords  = "Knowledge engineering, Medical guideline formalization, Ontologies, Semantic mark-up",
  author    = "Radu Serban and {ten Teije}, Annette and {van Harmelen}, Frank and Mar Marcos and Cristina Polo-Conde",
  year      = "2007",
  month     = "2",
  doi       = "10.1016/j.artmed.2006.07.012",
  volume    = "39",
  pages     = "137--149",
  journal   = "Artificial Intelligence in Medicine",
  issn      = "0933-3657",
  publisher = "Elsevier",
  number    = "2",
}


@article{13f19f42684b4319a665bedb844b5cda,
  title     = "Debugging incoherent terminologies",
  abstract  = "In this paper we study the diagnosis and repair of incoherent terminologies. We define a number of new nonstandard reasoning services to explain incoherence through pinpointing, and we present algorithms for all of these services. For one of the core tasks of debugging, the calculation of minimal unsatisfiability preserving subterminologies, we developed two different algorithms, one implementing a bottom-up approach using support of an external description logic reasoner, the other implementing a specialized tableau-based calculus. Both algorithms have been prototypically implemented. We study the effectiveness of our algorithms in two ways: we present a realistic case study where we diagnose a terminology used in a practical application, and we perform controlled benchmark experiments to get a better understanding of the computational properties of our algorithms in particular and the debugging problem in general.",
  keywords  = "Debugging, Description logics, Diagnosis",
  author    = "Stefan Schlobach and Zhisheng Huang and Ronald Cornet and {Van Harmelen}, Frank",
  year      = "2007",
  month     = "10",
  doi       = "10.1007/s10817-007-9076-z",
  volume    = "39",
  pages     = "317--349",
  journal   = "Journal of Automated Reasoning",
  issn      = "0168-7433",
  publisher = "Springer Netherlands",
  number    = "3",
}


@article{f1ad47c1ddaf494dbee0e6b53873d059,
  title     = "Where is the Web in the Semantic Web?",
  author    = "{van Harmelen}, Frank and Michael Uschold",
  year      = "2007",
  month     = "12",
  doi       = "10.1016/j.websem.2007.09.003",
  volume    = "5",
  pages     = "225--226",
  journal   = "Journal of Web Semantics",
  issn      = "1570-8268",
  publisher = "Elsevier",
  number    = "4",
}


@inbook{d164f9a305d14844a2ee4f8e479f5fba,
  title     = "Chapter 21 The Semantic Web: Webizing Knowledge Representation",
  abstract  = "The World Wide Web opens up new opportunities for the use of knowledge representation: a formal description of the semantic content of Web pages can allow better processing by computational agents. Further, the naming scheme of the Web, using Universal Resource Indicators, allows KR systems to avoid the ambiguities of natural language and to allow linking between semantic documents. These capabilities open up a raft of new possibilities for KR, but also present challenges to some traditional KR assumptions.",
  author    = "Jim Hendler and {van Harmelen}, Frank",
  year      = "2008",
  doi       = "10.1016/S1574-6526(07)03021-0",
  isbn      = "9780444522115",
  volume    = "3",
  series    = "Foundations of Artificial Intelligence",
  pages     = "821--839",
  booktitle = "Handbook of Knowledge Representation",
}


@inbook{bf4d9dc0db0f47759bf872c849fc8b69,
  title     = "Models of interaction as a grounding for Peer to peer knowledge sharing",
  abstract  = "Most current attempts to achieve reliable knowledge sharing on a large scale have relied on pre-engineering of content and supply services. This, like traditional knowledge engineering, does not by itself scale to large, open, peer to peer systems because the cost of being precise about the absolute semantics of services and their knowledge rises rapidly as more services participate. We describe how to break out of this deadlock by focusing on semantics related to interaction and using this to avoid dependency on a priori semantic agreement; instead making semantic commitments incrementally at run time. Our method is based on interaction models that are mobile in the sense that they may be transferred to other components, this being a mechanism for service composition and for coalition formation. By shifting the emphasis to interaction (the details of which may be hidden from users) we can obtain knowledge sharing of sufficient quality for sustainable communities of practice without the barrier of complex meta-data provision prior to community formation.",
  author    = "David Robertson and Adam Barker and Paolo Besana and Alan Bundy and Chen-Burger, {Yun Heh} and David Dupplaw and Fausto Giunchiglia and {Van Harmelen}, Frank and Fadzil Hassan and Spyros Kotoulas and David Lambert and Guo-chao Li and Jarred McGinnis and Fiona McNeill and Nardine Osman and {De Pinninck}, {Adrian Perreau} and Ronny Siebes and Carles Sierra and Chris Walton",
  year      = "2008",
  doi       = "10.1007/978-3-540-89784-2_4",
  isbn      = "3540897836",
  volume    = "4891 LNCS",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  pages     = "81--129",
  booktitle = "Advances in Web Semantics I - Ontologies, Web Services and Applied Semantic Web",
}


@inbook{31ea4763a7934f2992fcc15a8a2a686a,
  title     = "Open knowledge coordinating knowledge sharing through peer-to-peer interaction",
  abstract  = "The drive to extend the Web by taking advantage of automated symbolic reasoning (the so-called Semantic Web) has been dominated by a traditional model of knowledge sharing, in which the focus is on task-independent standardisation of knowledge. It appears to be difficult, in practice, to standardise in this way because the way in which we represent knowledge is strongly influenced by the ways in which we expect to use it. We present a form of knowledge sharing that is based not on direct sharing of {"}true{"} statements about the world but, instead, is based on sharing descriptions of interactions. By making interaction specifications the currency of knowledge sharing we gain a context to interpreting knowledge that can be transmitted between peers, in a manner analogous to the use of electronic institutions in multi-agent systems. The narrower notion of semantic commitment we thus obtain requires peers only to commit to meanings of terms for the purposes and duration of the interactions in which they appear. This lightweight semantics allows networks of interaction to be formed between peers using comparatively simple means of tackling the perennial issues of query routing, service composition and ontology matching. A basic version of the system described in this paper has been built (via the OpenKnowledge project); all its components use established methods; many of these have been deployed in substantial applications; and we summarise a simple means of integration using the interaction specification language itself.",
  author    = "Dave Robertson and Fausto Giunchiglia and {Van Harmelen}, Frank and Maurizio Marchese and Marta Sabou and Marco Schorlemmer and Nigel Shadbolt and Ronnie Siebes and Carles Sierra and Chris Walton and Srinandan Dasmahapatra and Dave Dupplaw and Paul Lewis and Mikalai Yatskevich and Spyros Kotoulas and {De Pinninck}, {Adrian Perreau} and Antonis Loizou",
  year      = "2008",
  doi       = "10.1007/978-3-540-85058-8_1",
  isbn      = "3540850570",
  volume    = "5118 LNAI",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  pages     = "1--18",
  booktitle = "Languages, Methodologies and Development Tools for Multi-Agent Systems - First International Workshop, LADS 2007, Revised Selected and Invited Papers",
}


@inbook{5875181a1c2d4f28ac393beb7615ee96,
  title     = "Retrieval of Case Law to provide laymen with information about liability: Preliminary Results of the BEST-Project",
  abstract  = "This paper describes the experiments carried out in the context of the BEST-project, an interdisciplinary project with researchers from the Law faculty and the AI department of the VU University Amsterdam. The aim of the project is to provide laymen with information about their legal position in a liability case, based on retrieved case law. The process basically comes down to (1) analyzing the input of a layman in terms of a layman ontology, (2) mapping this ontology to a legal ontology, (3) retrieve relevant case law based, and finally (4) present the results in a comprehensible way to the layman. This paper describes the experiments undertaken regarding step 4, and in particular step 3.",
  author    = "E.M. Uijttenbroek and A.R. Lodder and M.C.A. Klein and {van Harmelen}, F.A.H. and G. Wildeboer and R.L.L. Sie",
  year      = "2008",
  doi       = "10.1007/978-3-540-85569-9_19",
  series    = "Lecture Notes in Computer Science",
  publisher = "Springer",
  pages     = "291--310",
  editor    = "P. Casanovas and G. Sartor and N. Casellas and R. Rubino",
  booktitle = "Computable Models of the Law",
}


@inbook{228407fb7d844ee49d8c8a52bfe35170,
  title     = "Semantic Web Technologies as the Foundation for the Information Infrastructure",
  abstract  = "The Semantic Web is arising over the pas few years as a realistic option for a world wide Information Infrastructure, with its promises of semantic interoperability and serendipitous reuse. In this paper we will analyse the essential ingredients of semantic technologies, what makes them suitable as the foundation for the Information Infrastructure, and what the alternatives to semantic technologies would be as foundations for the Information Infrastructure. We will make a survey of the most important achievements on semantic technologies in the past few years, and point to the most important challenges that remain to be solved. 3.1 Historical trend towards increasing demands on interoperability When Thomas Watson, the founder of IBM, was asked for his estimate of how many computers would be needed worldwide, his reply is widely claimed to have been: 'about five'",
  keywords  = "everythingimport",
  author    = "{Van Oosterom}, Peter and S. Zlatanova and {Van Harmelen}, Frank",
  year      = "2008",
  series    = "Creating Spatial Information Infrastructures Towards the Spatial Semantic Web",
  publisher = "CRC Press",
  editor    = "{Van Oosterom}, Peter and S Zlatanova",
  booktitle = "Creating Spatial Information Infrastructures Towards the Spatial Semantic Web",
}


@inbook{11c3609efb2848aa99fe2d3b2696fea1,
  title     = "Towards LarKC: A platform for Web-scale reasoning",
  abstract  = "Current Semantic Web reasoning systems do not scale to the requirements of their hottest applications, such as analyzing data from millions of mobile devices, dealing with terabytes of scientific data, and content management in enterprises with thousands of knowledge workers. In this paper, we present our plan of building the Large Knowledge Collider, a platform for massive distributed incomplete reasoning that will remove these scalability barriers. This is achieved by (i) enriching the current logic-based Semantic Web reasoning methods, (ii) employing cognitively inspired approaches and techniques, and (iii) building a distributed reasoning platform and realizing it both on a high-performance computing cluster and via {"}computing at home{"}. In this paper, we will discuss how the technologies of LarKC would move beyond the state-of-the-art of Web-scale reasoning.",
  author    = "Dieter Fensel and {Van Harmelen}, Frank and Bo Andersson and Paul Brennan and Hamish Cunningham and Valle, {Emanuele Delia} and Florian Fischer and Zhisheng Huang and Atanas Kiryakov and Lee, {Tony Kyung Il} and Lael Schooler and Volker Tresp and Stefan Wesner and Michael Witbrock and Ning Zhong",
  year      = "2008",
  doi       = "10.1109/ICSC.2008.41",
  isbn      = "9780769532790",
  pages     = "524--529",
  booktitle = "Proceedings - IEEE International Conference on Semantic Computing 2008, ICSC 2008",
}


@misc{ac157507808c4b649dab081c2d3106a9,
  title    = "Using multiple ontologies as background knowledge in ontology matching",
  abstract = "Using ontology as a background knowledge in ontology match- ing is being actively investigated. Recently the idea attracted attention because of the growing number of available ontologies, which in turn opens up new opportunities, and reduces the problem of nding candi- date background knowledge. Particularly interesting is the approach of using multiple ontologies as background knowledge, which we explore in this paper. We report on an experimental study conducted using real-life ontologies published online. The rst contribution of this paper is an exploration about how the matching performance behaves when multiple background ontologies are used cumulatively. As a second contribution, we analyze the impact that dierent types of background ontologies have to the matching perfor- mance. With respect to the precision and recall, more background knowl- edge monotonically increases the recall, while the precision depends on the quality of the added background ontology, with high quality tending to increase, and the low quality tending to decrease the precision.",
  author   = "Zharko Aleksovski and {Ten Kate}, Warner and {Van Harmelen}, Frank",
  year     = "2008",
}


@article{1849fcb892e14cb2aea6e787e31111ab,
  title     = "Expertise-based peer selection in Peer-to-Peer networks",
  abstract  = "Peer-to-Peer systems have proven to be an effective way of sharing data. Modern protocols are able to efficiently route a message to a given peer. However, determining the destination peer in the first place is not always trivial. We propose a model in which peers advertise their expertise in the Peer-to-Peer network. The knowledge about the expertise of other peers forms a semantic topology. Based on the semantic similarity between the subject of a query and the expertise of other peers, a peer can select appropriate peers to forward queries to, instead of broadcasting the query or sending it to a random set of peers. To calculate our semantic similarity measure, we make the simplifying assumption that the peers share the same ontology. We evaluate the model in a bibliographic scenario, where peers share bibliographic descriptions of publications among each other. In simulation experiments complemented with a real-world field experiment, we show how expertise-based peer selection improves the performance of a Peer-to-Peer system with respect to precision, recall and the number of messages.",
  keywords  = "Ontologies, P2P, Routing, Semantic overlays",
  author    = "Peter Haase and Ronny Siebes and {van Harmelen}, Frank",
  year      = "2008",
  month     = "4",
  doi       = "10.1007/s10115-006-0055-1",
  volume    = "15",
  pages     = "75--107",
  journal   = "Knowledge and Information Systems",
  issn      = "0219-1377",
  publisher = "Springer London",
  number    = "1",
}


@inbook{8634b9e67293479b90d66b5c54bd1110,
  title     = "Identifying Disease-Centric Subdomains in Very Large Medical Ontologies: A Case-Study on Breast Cancer Concepts in SNOMED CT. Or: Finding 2500 Out of 300.000",
  author    = "K. Milian and Z. Aleksovski and R. Vdovjak and {ten Teije}, A.C.M. and {van Harmelen}, F.A.H.",
  year      = "2009",
  pages     = "50--63",
  editor    = "D. Riano and {ten Teije}, A.C.M. and S. Miksch and M. Peleg",
  booktitle = "Knowledge Representation for Health-Care: Data, Processes and Guidelines, AIME 2009 Workshop KR4HC 2009",
}


@inbook{76b03bd74ff44365a419e715fdd7a63d,
  title     = "Knowledge engineering rediscovered: towards reasoning patterns for the semantic web",
  abstract  = "The extensive work on Knowledge Engineering in the 1990s has resulted in a systematic analysis of task-types, and the corresponding problem solving methods that can be deployed for different types of tasks. That anal- ysis was the basis for a sound and widely accepted methodology for building knowledge-based systems, and has made it is possible to build libraries of reusable models, methods and code.In this paper, we make a first attempt at a similar analy- sis for Semantic Web applications. We will show that it is possible to identify a relatively small number of task- types, and that, somewhat surprisingly, a large set of Semantic Web applications can be described in this ty- pology. Secondly, we show that it is possible to decom- pose these task-types into a small number of primitive (“atomic”) inference steps. We give semi-formal defini- tions for both the task-types and the primitive inference steps that we identify. We substantiate our claim that our task-types are sufficient to cover the vast majority of Semantic Web applications by showing that all en- tries of the Semantic Web Challenges of the last 3 years can be classified in these task-types.",
  author    = "{van Harmelen}, F.A.H. and {ten Teije}, A.C.M. and H. Wache",
  year      = "2009",
  isbn      = "9781605586588",
  pages     = "81--88",
  editor    = "Y Gil and N Fridman",
  booktitle = "Proceedings of the 5th International Conference on Knowledge Capture (K-CAP 2009)",
  publisher = "ACM",
}


@inbook{dabfb007e9524066a6c95e7dbff6fde9,
  title     = "MaRVIN: A platform for large-scale analysis of Semantic Web data",
  author    = "E. Oren and S. Kotoulas and G. Anadiotis and R.M. Siebes and {ten Teije}, A.C.M. and {van Harmelen}, F.A.H.",
  year      = "2009",
  booktitle = "Proceedings of the WebSci09: Society On-Line",
}


@article{dcf9139489264b0793a7e4646dea2a19,
  title     = "MARVIN: Distributed reasoning over large-scale Semantic Web data",
  abstract  = "Many Semantic Web problems are difficult to solve through common divide-and-conquer strategies, since they are hard to partition. We present Marvin, a parallel and distributed platform for processing large amounts of RDF data, on a network of loosely coupled peers. We present our divide-conquer-swap strategy and show that this model converges towards completeness. Within this strategy, we address the problem of making distributed reasoning scalable and load-balanced. We present SpeedDate, a routing strategy that combines data clustering with random exchanges. The random exchanges ensure load balancing, while the data clustering attempts to maximise efficiency. SpeedDate is compared against random and deterministic (DHT-like) approaches, on performance and load-balancing. We simulate parameters such as system size, data distribution, churn rate, and network topology. The results indicate that SpeedDate is near-optimally balanced, performs in the same order of magnitude as a DHT-like approach, and has an average throughput per node that scales with sqrt(i) for i items in the system. We evaluate our overall Marvin system for performance, scalability, load balancing and efficiency. © 2009 Elsevier B.V. All rights reserved.",
  author    = "E. Oren and S. Kotoulas and G. Anadiotis and R.M. Siebes and {ten Teije}, A.C.M. and {van Harmelen}, F.A.H.",
  year      = "2009",
  doi       = "10.1016/j.websem.2009.09.002",
  volume    = "7",
  journal   = "Journal of Web Semantics",
  issn      = "1570-8268",
  publisher = "Elsevier",
  number    = "4",
}


@inbook{f98679b520294b28bf556a6437d8749e,
  title     = "Reasoning about repairability of workflows at design time",
  abstract  = "This paper describes an approach for reasoning about the repairability of workflows at design time. We propose a heuristic-based analysis of a workflow that aims at evaluating its definition, considering different design aspects and characteristics that affect its repairability (called repairability factors), in order to determine if the workflow schema supports repairable executions of its activities through the application of repair actions. The analysis intents to identify and evaluate the impact of critical design flaws affecting the repairability of workflows. The results of this analysis are fed back to the workflow designer and used to improve the repairability of the workflow by making appropriate changes to its definition.",
  keywords  = "Repairability of Workflows, Self-healing Web services, Web Service Composition, Workflow Design",
  author    = "Gaston Tagni and {Ten Teije}, Annette and {Van Harmelen}, Frank",
  year      = "2009",
  doi       = "10.1007/978-3-642-00328-8_46",
  isbn      = "9783642003271",
  volume    = "17 LNBIP",
  series    = "Lecture Notes in Business Information Processing",
  publisher = "Springer/Verlag",
  pages     = "455--467",
  booktitle = "Business Process Management Workshops - BPM 2008 International Workshops - Revised Papers",
}


@article{36022f6e57c7436da7b0d0e61f5e799a,
  title     = "Research chapters in the area of stream reasoning",
  abstract  = "Data streams occur in a variety of modern applications. Specialized Stream Database Management Systems proved to be an optimal solution for on the y analysis of data streams, but they cannot perform complex reasoning tasks that requires to combine the streaming data with less time variant knowledge. At the same time, while reasoners are year after year scaling up in the classical, time invariant domain of ontological knowledge, reasoning upon rapidly changing information has been neglected or forgotten. We hereby propose stream reasoning - an unexplored, yet high impact, research area - as the new multi-disciplinary approach which will provide the abstractions, foundations, methods, and tools required to integrate data streams and reasoning systems. In particular the focus of this paper is to sketch the research chapters of Stream Reasoning.",
  author    = "{Della Valle}, E. and S. Ceri and Daniele Braga and I. Celino and D. Frensel and {Van Harmelen}, F. and G. Unel",
  year      = "2009",
  volume    = "466",
  journal   = "CEUR workshop proceedings",
  issn      = "1613-0073",
  publisher = "CEUR Workshop Proceedings",
}


@inbook{be965766aadc48ae9e446885e3e8fd49,
  title     = "Scalable Distributed Reasoning Using MapReduce",
  author    = "J. Urbani and S. Kotoulas and E. Oren and {van Harmelen}, F.A.H.",
  year      = "2009",
  booktitle = "The Semantic Web - ISWC 2009",
  publisher = "Springer",
}


@inbook{bdacb05355614467aa87cd49f4c5bdcd,
  title     = "The Free Speech Engine: Conversational Web Service Compatibility for Free",
  author    = "R.G.M. Stegers and {van Harmelen}, F.A.H. and {ten Teije}, A.C.M.",
  year      = "2009",
  isbn      = "1601321309",
  pages     = "53--59",
  editor    = "H.R. Arabnia and A. Marsh",
  booktitle = "Proceedings of the 2009 International Conference on Semantic Web & Web Services, SWWS 2009",
  publisher = "CSREA Press",
}


@article{d0f279ece3764d0db4671a586709882a,
  title     = "Using model checking for critiquing based on clinical guidelines",
  abstract  = "Objective: Medical critiquing systems compare clinical actions performed by a physician with a predefined set of actions. In order to provide useful feedback, an important task is to find differences between the actual actions and a set of 'ideal' actions as described by a clinical guideline. In case differences exist, the critiquing system provides insight into the extent to which they are compatible. Methods and material: We propose a computational method for such critiquing, where the ideal actions are given by a formal model of a clinical guideline, and where the actual actions are derived from real world patient data. We employ model checking to investigate whether a part of the actual treatment is consistent with the guideline. Results: We show how critiquing can be cast in terms of temporal logic, and what can be achieved by using model checking. Furthermore, a method is introduced for off-line computing relevant information which can be exploited during critiquing. The method has been applied to a clinical guideline of breast cancer in conjunction with breast cancer patient data. © 2008 Elsevier B.V. All rights reserved.",
  author    = "P. Groot and A. Hommersom and P.F. Lucas and R. Merk and {ten Teije}, A.C.M. and {van Harmelen}, F.A.H. and R.C. Serban",
  year      = "2009",
  doi       = "10.1016/j.artmed.2008.07.007",
  volume    = "46",
  pages     = "19--36",
  journal   = "Artificial Intelligence in Medicine",
  issn      = "0933-3657",
  publisher = "Elsevier",
  number    = "1",
}


@article{0930585942dc452e8573ed79204a43e3,
  title    = "Using semantic distances for reasoning with inconsistent ontologies",
  abstract = "Re-using and combining multiple ontologies on the Web is bound to lead to inconsistencies between the combined vocabularies. Even many of the ontologies that are in use today turn out to be inconsistent once some of their implicit knowledge is made explicit. However, robust and efficient methods to deal with inconsistencies are lacking from current Semantic Web reasoning systems, which are typically based on classical logic. In earlier papers, we have proposed the use of syntactic relevance functions as a method for reasoning with inconsistent ontologies. In this paper, we extend that work to the use of semantic distances. We show how Google distances can be used to develop semantic relevance functions to reason with inconsistent ontologies. In essence we are using the implicit knowledge hidden in theWeb for explicit reasoning purposes. We have implemented this approach as part of the PION reasoning system. We report on experiments with several realistic ontologies. The test results show that a mixed syntactic/semantic approach can significantly improve reasoning performance over the purely syntactic approach. Furthermore, our methods allow to trade-off computational cost for inferential completeness. Our experiment shows that we only have to give up a little quality to obtain a high performance gain.",
  author   = "Zhisheng Huang and {van Harmelen}, Frank",
  year     = "2009",
  pages    = "329--330",
  journal  = "Belgian/Netherlands Artificial Intelligence Conference",
  issn     = "1568-7805",
}


@article{3206aea94c0a4d6a9944c76dbd01c493,
  title     = "It's a streaming world! Reasoning upon rapidly changing information",
  abstract  = "Stream reasoning, an unexplored yet high impact research area, is a new multidisciplinary approach that can provide the abstractions, foundations, methods, and tools required to integrate data streams, the Semantic Web, and reasoning systems, thus providing a way to answer our initial questions and many others. Stream reasoning can benefit numerous areas, including traffic monitoring and traffic pattern detection that appear to provide a natural application area. Dealing with users' stream of experience, mobile applications must reason on what part of the streaming information is relevant and what its meaning is. Stream reasoning requires continuous processing, because queries are normally registered and remain continuously active while data streams into the stream-reasoning system. Streams can appear in multiple forms, ranging from relation data over binary messaging protocols, such as data streams originated by sensor networks, to text streams over Web protocols, such as blogs and microblogs.",
  author    = "{Della Valle}, Emanuele and Stefano Ceri and {Van Harmelen}, Frank and Dieter Fensel",
  year      = "2009",
  month     = "11",
  volume    = "24",
  pages     = "83--89",
  journal   = "IEEE Intelligent Systems",
  issn      = "1541-1672",
  publisher = "Institute of Electrical and Electronics Engineers Inc.",
  number    = "6",
}


@article{4e25e1cebdc04855bf38e9865b7ac7bb,
  title     = "A reasonable Semantic Web",
  abstract  = "The realization of Semantic Web reasoning is central to substantiating the Semantic Web vision. However, current mainstream research on this topic faces serious challenges, which forces us to question established lines of research and to rethink the underlying approaches. We argue that reasoning for the Semantic Web should be understood as {"}shared inference,{"} which is not necessarily based on deductive methods. Model-theoretic semantics (and sound and complete reasoning based on it) functions as a gold standard, but applications dealing with large-scale and noisy data usually cannot afford the required runtimes. Approximate methods, including deductive ones, but also approaches based on entirely different methods like machine learning or nature-inspired computing need to be investigated, while quality assurance needs to be done in terms of precision and recall values (as in information retrieval) and not necessarily in terms of soundness and completeness of the underlying algorithms.",
  keywords  = "Automated reasoning, Formal semantics, Knowledge representation, Linked Open Data, Semantic Web",
  author    = "Pascal Hitzler and {Van Harmelen}, Frank",
  year      = "2010",
  doi       = "10.3233/SW-2010-0010",
  volume    = "1",
  pages     = "39--44",
  journal   = "Semantic Web",
  issn      = "1570-0844",
  publisher = "IOS Press",
  number    = "1-2",
}


@inbook{fe98236aa398497c952b466badd6df3f,
  title     = "A workbench for anytime reasoning by ontology approximation: With a case study on instance retrieval",
  abstract  = "Reasoning is computationally expensive. This is especially true for reasoning on the Web, where data sets are very large and often described by complex terminologies. One way to reduce this complexity is through the use of approximate reasoning methods which trade one computational property (eg. quality of answers) for others, such as time and memory. Previous research into approximation on the Semantic Web has been rather ad-hoc, and we propose a framework for systematically studying such methods. We developed a workbench which allows the structured combination of different algorithms for approximation, reasoning and measuring in one single framework. As a case-study we investigate an incremental method for instance retrieval through ontology approximation, and we use our workbench to study the computational behaviour of several approximation strategies.",
  keywords  = "Anytime Reasoning, Approximate Reasoning, Description Logics, Ontologies, Semantic Web",
  author    = "Gaston Tagni and Stefan Schlobach and {Ten Teije}, Annette and {Van Harmelen}, Frank and Giorgios Karafotias",
  year      = "2010",
  doi       = "10.3233/978-1-60750-676-8-328",
  isbn      = "9781607506751",
  volume    = "222",
  series    = "Frontiers in Artificial Intelligence and Applications",
  publisher = "IOS Press",
  pages     = "328--340",
  booktitle = "STAIRS 2010 Proceedings of the Fifth Starting AI Researchers' Symposium",
}


@inbook{fe0bf63b9bbe45b2881141caa701e78f,
  title     = "Case Frames as Contextual Mappings to Case Law in BestPortal",
  author    = "R.J. Hoekstra and A.R. Lodder and {van Harmelen}, F.A.H.",
  year      = "2010",
  isbn      = "9781607506812",
  pages     = "77--86",
  editor    = "R.G.F. Winkels",
  booktitle = "Legal Knowledge and Information Systems - JURIX 2010: The Twenty-Third Annual Conference",
  publisher = "IOS Press",
}


@article{b12e115ad1d9474caa82e496bdfa0b59,
  title     = "Finding the Achilles Heel of the Web of Data : using network analysis for link-recommendation.",
  author    = "C.D.M. Gueret and P.T. Groth and {van Harmelen}, F.A.H. and S. Schlobach",
  note      = "Proceedings title: The Semantic Web - ISWC 2010 - 9th International Semantic Web Conference, ISWC 2010 Publisher: Springer ISBN: 978-3-642-17745-3 Editors: P.F. Patel-Schneider, Y. Pan, P Hitzler, P Mika, L Zhang, J.Z. Pan, I. Horrocks, B. Glimm",
  year      = "2010",
  volume    = "6496",
  journal   = "Lecture Notes in Computer Science",
  issn      = "0302-9743",
  publisher = "Springer Verlag",
}


@inbook{3c6ae4a4742a48c89d720d87edebe3f6,
  title     = "Identifying disease-centric subdomains in very large medical ontologies: A case-study on breast cancer concepts in SNOMED CT. Or: Finding 2500 out of 300.000",
  abstract  = "Modern medical vocabularies can contain up to hundreds of thousands of concepts. In any particular use-case only a small fraction of these will be needed. In this paper we first define two notions of a disease-centric subdomain of a large ontology. We then explore two methods for identifying disease-centric subdomains of such large medical vocabularies. The first method is based on lexically querying the ontology with an iteratively extended set of seed queries. The second method is based on manual mapping between concepts from a medical guideline document and ontology concepts. Both methods include concept-expansion over subsumption and equality relations. We use both methods to determine a breast-cancer-centric subdomain of the SNOMED CT ontology. Our experiments show that the two methods produce a considerable overlap, but they also yield a large degree of complementarity, with interesting differences between the sets of concepts that they return. Analysis of the results reveals strengths and weaknesses of the different methods.",
  keywords  = "Disease related concepts, Identifying ontology subdomain, Mapping medical terminologies, Medical guidelines, Ontology subsetting, Seed queries",
  author    = "Krystyna Milian and Zharko Aleksovski and Richard Vdovjak and {Ten Teije}, Annette and {Van Harmelen}, Frank",
  year      = "2010",
  doi       = "10.1007/978-3-642-11808-1_5",
  isbn      = "3642118070",
  volume    = "5943 LNAI",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  pages     = "50--63",
  booktitle = "Knowledge Representation for Health-Care: Data, Processes and Guidelines, AIME 2009, Workshop KR4HC 2009, Revised Selected and Invited Papers",
}


@inbook{4ad1c77d29c549bab4baeb9b9ab95ca6,
  title     = "OWL Reasoning with WebPIE: Calculating the Closure of 100 Billion Triples",
  author    = "J. Urbani and S. Kotoulas and J. Maassen and {van Harmelen}, F.A.H. and H.E. Bal",
  year      = "2010",
  booktitle = "The Semantic Web: Research and Applications, 7th Extended Semantic Web Conference, ESWC 2010",
  publisher = "Springer",
}


@article{aa7f74a475544d4682336fef34ef6c16,
  title    = "Towards expressive stream reasoning",
  abstract = "Stream Data processing has become a popular topic in database research addressing the challenge of efficiently answering queries over continuous data streams. Meanwhile data streams have become more and more important as a basis for higher level decision processes that require complex reasoning over data streams and rich background knowledge. In previous work the foundation for complex reasoning over streams and background knowledge was laid by introducing technologies for wrapping and querying streams in the RDF data format and by supporting simple forms of reasoning in terms of incremental view maintenance. In this paper, we discuss how this existing technologies should be extended toward richer forms of reasoning using Sensor Networks as a motivating example.",
  author   = "Heiner Stuckenschmidt and Stefano Ceri and {Della Valle}, Emanuele and {van Harmelen}, Frank and {di Milano}, P.",
  year     = "2010",
  pages    = "1--14",
  journal  = "Proceedings of the Dagstuhl Seminar on Semantic Aspects of Sensor Networks",
  issn     = "1862-4405",
}


@article{11589630c9e54c8cb7252a4a3c2e1213,
  title     = "Special issue on commonsense reasoning for the semantic web Preface",
  author    = "{van Harmelen}, Frank and Andreas Herzig and Pascal Hitzler and Guilin Qi",
  year      = "2010",
  month     = "2",
  doi       = "10.1007/s10472-010-9209-7",
  volume    = "58",
  pages     = "1--2",
  journal   = "Annals of Mathematics and Artificial Intelligence",
  issn      = "1012-2443",
  publisher = "Springer Netherlands",
  number    = "1-2",
}


@inbook{26372a47fbb442be9aeaa6d2fe04f1ea,
  title     = "Case Frames as Contextual Mappings to Case Law in BestPortal",
  author    = "R.J. Hoekstra and A.R. Lodder and {van Harmelen}, F.A.H.",
  year      = "2011",
  pages     = "393--394",
  editor    = "{De Causmaecker}, P. and J. Maervoet and T. Messelis and K. Verbeeck and T. Vermeulen",
  booktitle = "Proceedings of the 23rd Benelux Conference on Artificial Intelligence",
  publisher = "KAHO Sint-Lieven",
}


@inbook{80c2e23da4134deb83fa68924b4e5e75,
  title     = "Finding the Achilles Heel of the Web of Data using network analysis tools",
  author    = "C.D.M. Gueret and P.T. Groth and {van Harmelen}, F.A.H. and K.S. Schlobach",
  year      = "2011",
  booktitle = "The 23rd Benelux Conference on Artificial Intelligence (BNAIC 2011)",
}


@inbook{7e9dd006160747aa8af8896295057bdb,
  title     = "Knowledge Engineering Rediscovered: Towards Reasoning Patterns for the Semantic Web",
  author    = "{van Harmelen}, F.A.H. and {ten Teije}, A. and H. Wache",
  year      = "2011",
  isbn      = "9783642197963",
  pages     = "57--75",
  editor    = "D Fensel",
  booktitle = "Foundations for the Web of Information and Services - A Review of 20 Years of Semantic Web Research",
  publisher = "Springer Verlag",
}


@inbook{761f2b63531f48879f5696ec529fdecc,
  title     = "KR and Reasoning on the Semantic Web: Web-Scale Reasoning",
  abstract  = "Reasoning is a key element of the Semantic Web. For the Semantic Web to scale, it is required that reasoning also scales. This chapter focuses on two approaches to achieve this: The first deals with increasing the computational power available for a given task by harnessing distributed resources. These distributed resources refer to peer-to-peer networks, federated data stores, or cluster-based computing. The second deals with containing the set of axioms that need to be considered for a given task. This can be achieved by using intelligent selection strategies and limiting the scope of statements. The former is exemplified by methods substituting expensive web-scale reasoning with the cheaper application of heuristics while the latter by methods to control the quality of the provided axioms. Finally, future issues concerning information centralization and logics vs information retrieval-based methods, metrics, and benchmarking are considered.",
  author    = "Spyros Kotoulas and {van Harmelen}, Frank and Jesse Weaver",
  year      = "2011",
  doi       = "10.1007/978-3-540-92913-0_11",
  isbn      = "978-3-540-92912-3",
  series    = "Handbook of Semantic Web Technologies",
  pages     = "442--466",
  booktitle = "Handbook of Semantic Web Technologies",
}


@inbook{369c37b168844b9896cf5c533696c031,
  title     = "QueryPIE: Backward reasoning for OWL Horst over very large knowledge bases",
  author    = "J. Urbani and {van Harmelen}, F.A.H. and S. Schlobach and H.E. Bal",
  year      = "2011",
  booktitle = "10th Int. Semantic Web Conference (ISWC 2011)",
}


@article{75d0ad4ba8a64d0c965b2a5938e6fb62,
  title     = "User-centric Query Refinement and Processing Using Granularity Based Strategies",
  abstract  = "Under the context of large-scale scientific literatures, this paper provides a user-centric approach for refining and processing incomplete or vague query based on cognitive- and granularity-based strategies. From the viewpoints of user interests retention and granular information processing, we examine various strategies for user-centric unification of search and reasoning. Inspired by the basic level for human problem-solving in cognitive science, we refine a query based on retained user interests. We bring the multi-level, multi-perspective strategies from human problem-solving to large-scale search and reasoning. The power/exponential law-based interests retention modeling, network statistics-based data selection, and ontology-supervised hierarchical reasoning are developed to implement these strategies. As an illustration, we investigate some case studies based on a large-scale scientific literature dataset, DBLP. The experimental results show that the proposed strategies are potentially effective. © 2010 Springer-Verlag London Limited.",
  author    = "Y. Zeng and N. Zhong and Y. Wang and Y. Qin and Z. Huang and H Zhou and Y Yao and {van Harmelen}, F.A.H.",
  year      = "2011",
  doi       = "10.1007/s10115-010-0298-8",
  volume    = "27",
  pages     = "419--450",
  journal   = "Knowledge and Information Systems",
  issn      = "0219-1377",
  publisher = "Springer London",
  number    = "3",
}


@article{b5d6fab222b1402fa23b013753ace04e,
  title     = "WebPIE: A Web-scale parallel inference engine using MapReduce",
  author    = "J. Urbani and S. Kotoulas and J. Maassen and {van Harmelen}, F.A.H. and H.E. Bal",
  year      = "2011",
  journal   = "Journal of Web Semantics",
  issn      = "1570-8268",
  publisher = "Elsevier",
}


@article{44602747d9234130bed2a26f5c531e9b,
  title     = "The linked data benchmark council (LDBC)",
  author    = "Irini Fundulaki and Pey, {Josep Larriba} and David Dominguez-Sal and Ioan Toma and Dieter Fensel and Barry Bishop and Thomas Neumann and Orri Erling and Peter Neubauer and Paul Groth and {Van Harmelen}, Frank and Peter Boncz",
  year      = "2012",
  volume    = "877",
  journal   = "CEUR workshop proceedings",
  issn      = "1613-0073",
  publisher = "CEUR Workshop Proceedings",
}


@article{433d785b5e53418eb030d6052a5f8884,
  title     = "Theoretical and technological building blocks for an innovation accelerator",
  abstract  = "Modern science is a main driver of technological innovation. The efficiency of the scientific system is of key importance to ensure the competitiveness of a nation or region. However, the scientific system that we use today was devised centuries ago and is inadequate for our current ICT-based society: the peer review system encourages conservatism, journal publications are monolithic and slow, data is often not available to other scientists, and the independent validation of results is limited. The resulting scientific process is hence slow and sloppy. Building on the Innovation Accelerator paper by Helbing and Balietti [1], this paper takes the initial global vision and reviews the theoretical and technological building blocks that can be used for implementing an innovation (in first place: science) accelerator platform driven by re-imagining the science system. The envisioned platform would rest on four pillars: (i) Redesign the incentive scheme to reduce behavior such as conservatism, herding and hyping; (ii) Advance scientific publications by breaking up the monolithic paper unit and introducing other building blocks such as data, tools, experiment workflows, resources; (iii) Use machine readable semantics for publications, debate structures, provenance etc. in order to include the computer as a partner in the scientific process, and (iv) Build an online platform for collaboration, including a network of trust and reputation among the different types of stakeholders in the scientific system: scientists, educators, funding agencies, policy makers, students and industrial innovators among others. Any such improvements to the scientific system must support the entire scientific process (unlike current tools that chop up the scientific process into disconnected pieces), must facilitate and encourage collaboration and interdisciplinarity (again unlike current tools), must facilitate the inclusion of intelligent computing in the scientific process, must facilitate not only the core scientific process, but also accommodate other stakeholders such science policy makers, industrial innovators, and the general public. We first describe the current state of the scientific system together with up to a dozen new key initiatives, including an analysis of the role of science as an innovation accelerator. Our brief survey will show that there exist many separate ideas and concepts and diverse stand-alone demonstrator systems for different components of the ecosystem with many parts are still unexplored, and overall integration lacking. By analyzing a matrix of stakeholders vs. functionalities, we identify the required innovations. We (non-exhaustively) discuss a few of them: Publications that are meaningful to machines, innovative reviewing processes, data publication, workflow archiving and reuse, alternative impact metrics, tools for the detection of trends, community formation and emergence, as well as modular publications, citation objects and debate graphs. To summarize, the core idea behind the Innovation Accelerator is to develop new incentive models, rules, and interaction mechanisms to stimulate true innovation, revolutionizing the way in which we create knowledge and disseminate information. © The Author(s) 2012.",
  author    = "{van Harmelen}, F.A.H. and G Kampis and K. Borner and {van den Besselaar}, P.A.A. and S. Schultes and C.A. Goble and P.T. Groth and B Mons and S. Anderson and S. Decker and C. Hayes and T. Buecheler and D. Helbing",
  year      = "2012",
  doi       = "10.1140/epjst/e2012-01692-1",
  volume    = "214",
  pages     = "183--214",
  journal   = "European Physical Journal. Special Topics",
  issn      = "1951-6355",
  publisher = "Springer Verlag",
}


@article{34d46a307ff4440a8fb5efa26a3fb321,
  title    = "WebPIE: A web-scale parallel inference engine using MapReduce",
  abstract = "The large amount of Semantic Web data and its fast growth pose a significant computational challenge in performing efficient and scalable reasoning. On a large scale, the resources of single machines are no longer sufficient and we are required to distribute the process to improve performance. The article that we attach to our submission [1] tackles this problem proposing a methodology to perform inference materializing every possible consequence using the MapReduce programming model. We introduce a number of optimizations to address the issues that a naive implementation would raise and to improve the overall performance. We have implemented the presented techniques in a prototype called WebPIE and the evaluation shows that our approach is able to perform complex inference based on the OWL language over a very large input of about 100 billion triples. To the best of our knowledge, it is the only approach that demonstrates complex inference over an input of a hundred billion of triples.",
  author   = "Jacopo Urbani and Spyros Kotoulas and Jason Maassen and {Van Harmelen}, Frank and Henri Bal",
  year     = "2012",
  journal  = "Belgian/Netherlands Artificial Intelligence Conference",
  issn     = "1568-7805",
}


@article{456d66fe3a4346a6884c0eacd09cb67f,
  title     = "Reply to comment on {"}webPIE: A Web-scale parallel inference engine using MapReduce{"}",
  author    = "Jacopo Urbani and Spyros Kotoulas and J. Maassen and {Van Harmelen}, Frank and Henri Bal",
  year      = "2012",
  month     = "9",
  doi       = "10.1016/j.websem.2012.09.002",
  volume    = "15",
  pages     = "71--72",
  journal   = "Journal of Web Semantics",
  issn      = "1570-8268",
  publisher = "Elsevier",
}


@article{34a3d04075e549dbac0a02e26910a7c4,
  title     = "Erratum: WebPIE: A Web-scale Parallel Inference Engine using MapReduce (Journal of Web Semantics (2012) 10 (59-75))",
  author    = "Jacopo Urbani and Spyros Kotoulas and Jason Maassen and {Van Harmelen}, Frank and Henri Bal",
  year      = "2012",
  month     = "12",
  doi       = "10.1016/j.websem.2012.09.005",
  volume    = "17",
  pages     = "44",
  journal   = "Journal of Web Semantics",
  issn      = "1570-8268",
  publisher = "Elsevier",
}


@misc{fb48e56abc7544fcb0e24a476fbe8012,
  title  = "A Semantically-Enabled System for Clinical Trials",
  author = "Z. Huang and {ten Teije}, A.C.M. and {van Harmelen}, F.A.H.",
  note   = "Proceedings title: 25th Benelux Conference on Artificial Intelligence Editors: K Hindriks, M de Weerdt, B van Riemsdijk, M Warnier",
  year   = "2013",
}


@inbook{3160586e9d6846b9a8daa224e7d927e4,
  title     = "DynamiTE: Parallel Materialization of Dynamic RDF Data",
  author    = "J. Urbani and A. Margara and C.J.H. Jacobs and {van Harmelen}, F.A.H. and H.E. Bal",
  year      = "2013",
  pages     = "657--672",
  booktitle = "12th Int. Semantic Web Conference (ISWC 2013)",
  publisher = "Springer",
}


@article{7131b396f2fe4f1cb4832082e4619c82,
  title     = "Hybrid reasoning on OWL RL",
  author    = "J. Urbani and R Piro and {van Harmelen}, F.A.H. and H.E. Bal",
  year      = "2013",
  journal   = "Semantic Web",
  issn      = "1570-0844",
  publisher = "IOS Press",
}


@inbook{bb18383308bf44539aa052e4b17a02b3,
  title     = "Identifying most relevant concepts to describe clinical trial eligibility criteria",
  author    = "K. Milian and A. Bucur and {van Harmelen}, F.A.H. and {ten Teije}, A.C.M.",
  year      = "2013",
  booktitle = "International Conference on Health Informatics (HEALTHINF 2013)",
}


@article{371b66c6e07845768e482114ed32c7ca,
  title     = "Knowledge-based Patient Data Generation",
  author    = "Z. Huang and {van Harmelen}, F.A.H. and {ten Teije}, A.C.M. and K. Dentler",
  note      = "Proceedings title: Process Support and Knowledge Representation in Health Care Publisher: Springer Editors: D Riano, R Lenz, S Miksch, M Peleg, M Reichert, A.C.M. ten Teije",
  year      = "2013",
  volume    = "8268",
  pages     = "11--25",
  journal   = "Lecture Notes in Computer Science",
  issn      = "0302-9743",
  publisher = "Springer Verlag",
}


@inbook{d619971b9e074c95b786a3eca3b4ff1f,
  title     = "Rough Set Semantics for Identity on the Web",
  abstract  = "Identity relations are at the foundation of the Linked Open Data initiative and on the Semantic Web in general. They allow the interlinking of alternative descriptions of the same thing. However, many practical uses of owl:sameAs are known to violate its formal semantics. We propose a method that assigns meaning to (the subrelations of) an identity relation using the predicates of the dataset schema. Applications of this approach include automated suggestions for asserting/retracting identity pairs and quality assessment. We also describe an experimental design for this approach.",
  author    = "Wouter Beek and Stefan Schlobach and {van Harmelen}, Frank",
  year      = "2013",
  pages     = "10--13",
  editor    = "{van Harmelen}, F. and J. Hendler and P Hitzler and K Janowicz",
  booktitle = "AAAI Fall Symposium Semantics for Big Data",
  publisher = "AAAI",
}


@article{5c65a8ca57b84d3b8144d1238323ceca,
  title     = "Rule-based Formalization of Eligibility Criteria for Clinical Trials",
  author    = "Z. Huang and {ten Teije}, A.C.M. and {van Harmelen}, F.A.H.",
  note      = "Proceedings title: Artificial Intelligence in Medicine Publisher: Springer Editors: N Peek, R Morales, M Peleg",
  year      = "2013",
  volume    = "7885",
  pages     = "38--47",
  journal   = "Lecture Notes in Computer Science",
  issn      = "0302-9743",
  publisher = "Springer Verlag",
}


@article{827ae4d020e343c4863bad421de8613c,
  title     = "SemanticCT: A Semantically-Enabled System for Clinical Trials",
  author    = "Z. Huang and {ten Teije}, A.C.M. and {van Harmelen}, F.A.H.",
  note      = "Proceedings title: Process Support and Knowledge Representation in Health Care Publisher: Springer",
  year      = "2013",
  volume    = "8268",
  pages     = "11--25",
  journal   = "Lecture Notes in Computer Science",
  issn      = "0302-9743",
  publisher = "Springer Verlag",
  number    = "iss",
}


@inbook{d2310ad12c5c4c5db6aa7eca3c9ab94c,
  title     = "A Conceptual Model for Detecting Interactions among Medical Recommendations in Clinical Guidelines",
  abstract  = "Representation of clinical knowledge is still an open research topic. In particular, classical languages designed for representing clinical guidelines, which were meant for producing diagnostic and treatment plans, present limitations such as for re-using, combining, and reasoning over existing knowledge. In this paper, we address such limitations by proposing an extension of the TMR conceptual model to represent clinical guidelines that allows re-using and combining knowledge from several guidelines to be applied to patients with multimorbidities. We provide means to (semi)automatically detect interactions among recommendations that require some attention from experts, such as recommending more than once the same drug. We evaluate the model by applying it to a realistic case study involving 3 diseases (Osteoarthritis, Hypertension and Diabetes) and compare the results with two other existing methods.",
  keywords  = "Clinical knowledge representation, Combining medical guidelines, Multimorbidity, Reasoning",
  author    = "{Carretta Zamborlini}, Veruska and {Da Silveira}, Marcos and Cedric Pruski and Rinke Hoekstra and {ten Teije}, Annette and {van Harmelen}, Frank",
  note      = "Proceedings title: Proceedings of the 19th International Conference on Knowledge Engineering and Knowledge Management (EKAW 2014) Publisher: Springer Editors: K Janowicz, S Schlobach, S Lambrix, E Hyvonen",
  year      = "2014",
  volume    = "8876",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  pages     = "591--606",
  booktitle = "Knowledge Engineering and Knowledge Management - 19th International Conference, EKAW 2014, Proceedings",
}


@inbook{3fbc7f197cde4e729501efbdf251c04c,
  title     = "A Pragmatic Semantics for Identity in Linked Data",
  author    = "W.G.J. Beek and K.S. Schlobach and {van Harmelen}, F.A.H.",
  year      = "2014",
  booktitle = "The 14th International Conference on Principles of Knowledge Representation and Reasoning",
}


@inbook{9700d89776784a4d81bfe8f2985049a3,
  title     = "Evidence-based clinical guidelines in SemanticCT",
  author    = "Q. Hu and Z. Huang and {van Harmelen}, F.A.H. and {ten Teije}, A.C.M. and Jinguang Gu",
  year      = "2014",
  booktitle = "8th China Semantic Web Symposium & 3rd Web Science Conference (CSWS2014)",
}


@inbook{743083cf4b644e20a6146f1ac7d964c9,
  title     = "Feasibility Estimation for Clinical Trials",
  author    = "Z. Huang and {van Harmelen}, F.A.H. and {ten Teije}, A.C.M. and A. Dekker",
  year      = "2014",
  pages     = "68--77",
  booktitle = "Proceedings of the 7th International Conference on Health Informatics (HEALTHINF2014).",
  publisher = "scitepress digital library",
}


@article{91cb952159614b12b8d6ff7fff464f22,
  title     = "Reports on the 2013 AAAI fall symposium series",
  abstract  = "The Association for the Advancement of Artificial Intelligence was pleased to present the 2013 Fall Symposium Series, held Friday through Sunday, November 15-17, at the Westin Arlington Gateway in Arlington, Virginia, near Washington, D.C., USA. The titles of the five symposia were Discovery Informatics: AI Takes a Science- Centered View on Big Data (FS-13-01); How Should Intelligence Be Abstracted in AI Research: MDPs, Symbolic Representations, Artificial Neural Networks, or - ? (FS-13-02); Integrated Cognition (FS-13-03); Semantics for Big Data (FS- 13-04); and Social Networks and Social Contagion: Web Analytics and Computational Social Science (FS-13-05). The highlights of each symposium are presented in this report.",
  author    = "Burns, {Gully A P C} and Yolanda Gil and Natalia Villanueva-Rosales and Yan Liu and Sebastian Risi and Joel Lehman and Jeff Clune and Christian Lebiere and Rosenbloom, {Paul S.} and {Van Harmelen}, Frank and Hendler, {James A.} and Pascal Hitzler and Krzysztof Janowicz and Samarth Swarup",
  year      = "2014",
  volume    = "35",
  pages     = "69--74",
  journal   = "The AI Magazine",
  issn      = "0738-4602",
  publisher = "AI Access Foundation",
  number    = "2",
}


@inbook{56aeef4da7c8498eac1532de8dfedce5,
  title     = "Rough set semantics for identity on the Web",
  abstract  = "Identity relations are at the foundation of many logic-based knowledge representations. We argue that the traditional notion of equality, is unsuited for many realistic knowledge representation settings. The classical interpretation of equality is too strong when the equality statements are re-used outside their original context. On the Semantic Web, equality statements are used to interlink multiple descriptions of the same object, using owl:sameAs assertions. And indeed, many practical uses of owl:sameAs are known to violate the formal Leibniz-style semantics. We provide a more flexible semantics to identity by assigning meaning to the subrelations of an identity relation in terms of the predicates that are used in a knowledge-base. Using those indiscernability-predicates, we define upper and lower approximations of equality in the style of rought-set theory, resulting in a quality-measure for identity relations.",
  author    = "Wouter Beek and Stefan Schlobach and {van Harmelen}, Frank",
  year      = "2014",
  pages     = "587--589",
  booktitle = "14th International Conference on the Principles of Knowledge Representation and Reasoning, KR 2014",
  publisher = "AAAI Press",
}


@article{1165c9b0439c4faa87efa6098120a44a,
  title     = "Semantic Representation of Evidence-based Clinical Guidelines",
  author    = "Z. Huang and {ten Teije}, A.C.M. and {van Harmelen}, F.A.H. and S. Ait-Mokhtar",
  note      = "Proceedings title: 6th International Workshop on Knowledge Representation for Health Care (KR4HC2014) Publisher: Springer ISBN: 978-3-319-13281-5 Editors: S. Miksch, D. Riano",
  year      = "2014",
  volume    = "8903",
  journal   = "Lecture Notes in Computer Science",
  issn      = "0302-9743",
  publisher = "Springer Verlag",
}


@article{6570c360d0544bbdb575308926eb82cf,
  title    = "Semantic Representation of Evidence-based Clinical Guidelines",
  abstract = "Evidence-based Clinical Guidelines (EbCGs) are that the document or recommendation has been created using the best clinical research findings of the highest value to aid in the delivery of optimum clinical care to patients. In this paper, we propose a lightweight formal-ism of evidence-based clinical guidelines by introducing the Semantic Web Technology for it. With the help of the tools which have been de-veloped in the Semantic Web and Natural Language Processing (NLP), the generation of the formulations of evidence-based clinical guidelines become much easy. We will discuss several use cases of the semantic representation of EbCGs, and argue that it is potentially useful for the applications of the semantic web technology on the medical domain.",
  author   = "Zhisheng Huang and Harmelen, {Frank Van}",
  year     = "2014",
  doi      = "10.1007/978-3-319-13281-5_6",
  pages    = "78--94",
  journal  = "6th International Workshop on Knowledge Representation for Health Care {KR4HC}",
}


@article{29462648d9424241ae9f3c6d4eb31e12,
  title     = "Streaming the Web: Reasoning over dynamic data",
  abstract  = "In the last few years a new research area, called stream reasoning, emerged to bridge the gap between reasoning and stream processing. While current reasoning approaches are designed to work on mainly static data, the Web is, on the other hand, extremely dynamic: information is frequently changed and updated, and new data is continuously generated from a huge number of sources, often at high rate. In other words, fresh information is constantly made available in the form of streams of new data and updates. Despite some promising investigations in the area, stream reasoning is still in its infancy, both from the perspective of models and theories development, and from the perspective of systems and tools design and implementation. The aim of this paper is threefold: (i) we identify the requirements coming from different application scenarios, and we isolate the problems they pose; (ii) we survey existing approaches and proposals in the area of stream reasoning, highlighting their strengths and limitations; (iii) we draw a research agenda to guide the future research and development of stream reasoning. In doing so, we also analyze related research fields to extract algorithms, models, techniques, and solutions that could be useful in the area of stream reasoning. © 2014 Elsevier B.V. All rights reserved.",
  keywords  = "Complex Event Processing, Semantic Web, Stream processing, Stream reasoning, Survey",
  author    = "Alessandro Margara and Jacopo Urbani and {Van Harmelen}, Frank and Henri Bal",
  year      = "2014",
  doi       = "10.1016/j.websem.2014.02.001",
  volume    = "25",
  pages     = "24--44",
  journal   = "Journal of Web Semantics",
  issn      = "1570-8268",
  publisher = "Elsevier",
}


@inbook{fd6fc04a6c44499fa0a62ec0fe44c350,
  title     = "Towards a Conceptual Model for Enhancing Reasoning about Clinical Guidelines: A case-study on Comorbidity",
  abstract  = "Computer-Interpretable Guidelines (CIGs) are representations of Clinical Guidelines (CGs) in computer interpretable languages. CIGs have been pointed as an alternative to deal with the various limitations of paper based CGs to support healthcare activities. Although the improvements offered by existing CIG languages, the complexity of the medical domain requires advanced features in order to reuse, share, update, combine or personalize their contents. We propose a conceptual model for representing the content of CGs as a result from an iterative approach that take into account the content of real CGs, CIGs languages and foundational ontologies in order to enhance the reasoning capabilities required to address CIG use-cases. In particular, we apply our approach to the comorbidity use-case and illustrate the model with a realistic case study (Duodenal Ulcer and Transient Ischemic Attack) and compare the results against an existing approach.",
  author    = "{Carretta Zamborlini}, Veruska and {Da Silveira}, Marcos and Cedric Pruski and {ten Teije}, Annette and {van Harmelen}, Frank",
  note      = "Proceedings title: Proceedings of 6th International Workshop Knowledge Representation for Health-Care (KR4HC). Publisher: Springer Editors: S. Miksch, D. Riano, A. ten Teije",
  year      = "2014",
  doi       = "10.1007/978-3-319-13281-5_3",
  volume    = "8903",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "29--44",
  booktitle = "Knowledge Representation for Health Care - 6th International Workshop, KR4HC 2014, Revised Selected Papers",
}


@inbook{85421a7bb476400da0040eb31b196374,
  title     = "A Compact In-Memory Dictionary for RDF data",
  abstract  = "While almost all dictionary compression techniques focus on static RDF data, we present a compact in-memory RDF dictionary for dynamic and streaming data. To do so, we analysed the structure of terms in real-world datasets and observed a high degree of common prefixes. We studied the applicability of Trie data structures on RDF data to reduce the memory occupied by common prefixes and discovered that all existing Trie implementations lead to either poor performance, or an excessive memory wastage. In our approach, we address the existing limitations of Tries for RDF data, and propose a new variant of Trie which contains some optimizations explicitly designed to improve the performance on RDF data. Furthermore, we show how we use this Trie as an in-memory dictionary by using as numerical ID a memory address instead of an integer counter. This design removes the need for an additional decoding data structure, and further reduces the occupied memory. An empirical analysis on realworld datasets shows that with a reasonable overhead our technique uses 50–59% less memory than a conventional uncompressed dictionary.",
  author    = "Bazoubandi, {Hamid R.} and {de Rooij}, Steven and Jacopo Urbani and {ten Teije}, Annette and {van Harmelen}, Frank and Henri Bal",
  note      = "Proceedings title: Proceedings of the twelfth European Semantic Web Conference Publisher: Springer Place of publication: Berlin",
  year      = "2015",
  doi       = "10.1007/978-3-319-18818-8_13",
  volume    = "9088",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "205--220",
  booktitle = "The Semantic Web: Latest Advances and New Domains - 12th European Semantic Web Conference, ESWC 2015, Proceedings",
}


@inbook{efec5bf19fa345ff8264ce823017de8c,
  title     = "Analyzing Recommendations Interactions in Clinical Guidelines: Impact of action type hierarchies and causation beliefs",
  abstract  = "Accounting for patients with multiple health conditions is a complex task that requires analysing potential interactions among recommendations meant to address each condition. Although some approaches have been proposed to address this issue, important features still require more investigation, such as (re)usability and scalability. To this end, this paper presents an approach that relies on reusable rules for detecting interactions among recommendations coming from various guidelines. It extends previously proposed models by introducing the notions of action type hierarchy and causation beliefs, and provides a systematic analysis of relevant interactions in the context of multimorbidity. Finally, the approach is assessed based on a case-study taken from the literature to highlight the added value of the approach.",
  keywords  = "Clinical knowledge representation, Combining medical guidelines, Multimorbidity",
  author    = "{Carretta Zamborlini}, Veruska and {Da Silveira}, Marcos and Cedric Pruski and {ten Teije}, Annette and {van Harmelen}, Frank",
  year      = "2015",
  doi       = "10.1007/978-3-319-19551-3_40",
  isbn      = "9783319195506",
  volume    = "9105",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "317--326",
  booktitle = "Artificial Intelligence in Medicine - 15th Conference on Artificial Intelligence in Medicine, AIME 2015, Proceedings",
}


@book{dfa31ef6a2fd4867a6b27654aabff10c,
  title     = "ARIADNE: First Report on Data Mining",
  abstract  = "Recent years have witnessed a growing interest from archaeological communities in Linked Data. ARIADNE, the AdvancedResearch Infrastructure for Archaeological Data set Networking in Europe, facilitates a central web portal that providesaccess to archaeological data from various sources. Parts of these data have been being published as Linked Data, andare currently available in the Linked Open Data cloud. With it, the nature of these data has shifted from unstructuredto structured. This presents new opportunities for data mining. In this work, we investigate to what extend data mining can contribute to the understanding of linked archaeological data, and which form would best meet the communities' needs.",
  author    = "W.X. Wilcke and {de Boer}, Viktor and {van Harmelen}, F.A.H. and {de Kleijn}, Mauritius and M. Wansleeben",
  year      = "2015",
  series    = "Ariadne",
  publisher = "Ariadne",
  number    = "D16.1",
}


@inbook{1da843e38a53485383b9e72917ebd0b6,
  title     = "Detecting New Evidence for Evidence-based Guidelines Using a Semantic Distance Method",
  author    = "Q. Hu and Z. Huang and {ten Teije}, A.C.M. and {van Harmelen}, F.A.H.",
  year      = "2015",
  pages     = "307--316",
  booktitle = "Proceedings 15th Conference on Artificial Intelligence in Medicine",
  publisher = "Springer",
}


@article{aaf8c24aa8184513aff121db59682115,
  title     = "Enhancing reuse of structured eligibility criteria and supporting their relaxation.",
  abstract  = "Patient recruitment is one of the most important barriers to successful completion of clinical trials and thus to obtaining evidence about new methods for prevention, diagnostics and treatment. The reason is that recruitment is effort consuming. It requires the identification of candidate patients for the trial (the population under study), and verifying for each patient whether the eligibility criteria are met. The work we describe in this paper aims to support the comparison of population under study in different trials, and the design of eligibility criteria for new trials. We do this by introducing structured eligibility criteria, that enhance reuse of criteria across trials. We developed a method that allows for automated structuring of criteria from text. Additionally, structured eligibility criteria allow us to propose suggestions for relaxation of criteria to remove potentially unnecessarily restrictive conditions. We thereby increase the recruitment potential and generalizability of a trial.Our method for automated structuring of criteria enables us to identify related conditions and to compare their restrictiveness. The comparison is based on the general meaning of criteria, comprised of commonly occurring contextual patterns, medical concepts and constraining values. These are automatically identified using our pattern detection algorithm, state of the art ontology annotators and semantic taggers. The comparison uses predefined relations between the patterns, concept equivalences defined in medical ontologies, and threshold values. The result is a library of structured eligibility criteria which can be browsed using fine grained queries. Furthermore, we developed visualizations for the library that enable intuitive navigation of relations between trials, criteria and concepts. These visualizations expose interesting co-occurrences and correlations, potentially enhancing meta-research.The method for criteria structuring processes only certain types of criteria, which results in low recall of the method (18%) but a high precision for the relations we identify between the criteria (94%). Analysis of the approach from the medical perspective revealed that the approach can be beneficial for supporting trial design, though more research is needed.",
  author    = "K. Milian and Rinke Hoekstra and A. Bucur and {ten Teije}, A.C.M. and {van Harmelen}, F.A.H. and J. Paulissen",
  year      = "2015",
  doi       = "10.1016/j.jbi.2015.05.005",
  volume    = "56",
  pages     = "205--219",
  journal   = "Journal of Biomedical Informatics",
  issn      = "1532-0464",
  publisher = "Academic Press Inc.",
}


@inbook{cd7cd90ab9284d68ae98b8f54f65ca35,
  title     = "Identifying Evidence Quality for Updating Evidence-based Medical Guidelines",
  author    = "Z. Huang and Q. Hu and {ten Teije}, A.C.M. and {van Harmelen}, F.A.H.",
  year      = "2015",
  isbn      = "9783319265841",
  pages     = "51--64",
  editor    = "D Riano and R. Lenz and S. Miksch and M. Peleg and M. Reichert and {ten Teije}, A.C.M.",
  booktitle = "Knowledge Representation for Health Care, AIME 2015 International Joint Workshop, KR4HC/ProHealth 2015",
  publisher = "Springer",
}


@article{4db74fcd21e94f12a3087524690c0849,
  title     = "Semantics for Big Data",
  author    = "{van Harmelen}, F.A.H. and J.A. Hendler and P. Hitzler and K. Janowicz",
  note      = "PT: J; NR: 3; TC: 0; J9: AI MAG; PG: 2; GA: CG2BK; UT: WOS:000353079700001",
  year      = "2015",
  doi       = "10.1609/aimag.v36i1.2559",
  volume    = "36",
  pages     = "3--4",
  journal   = "The AI Magazine",
  issn      = "0738-4602",
  publisher = "AI Access Foundation",
  number    = "1",
}


@article{ce395ed7fb1e4849b0b97819b268f04f,
  title     = "Semantic Technologies for Historical Research: A Survey",
  abstract  = "During the nineties of the last century, historians and computer scientists created together a research agenda around the life cycle of historical information. It comprised the tasks of creation, design, enrichment, editing, retrieval, analysis and presentation of historical information with help of information technology. They also identified a number of problems and challenges in this field, some of them closely related to semantics and meaning. In this survey paper we study the joint work of historians and computer scientists in the use of Semantic Web methods and technologies in historical research. We analyse to what extent these contributions help in solving the open problems in the agenda of historians, and we describe open challenges and possible lines of research pushing further a still young, but promising, historical Semantic Web.",
  author    = "A. Merono and A. Ashkpour and {van Erp}, M.G.J. and K. Mandemakers and L. Breure and A. Scharnhorst and K.S. Schlobach and {van Harmelen}, F.A.H.",
  year      = "2015",
  doi       = "10.3233/SW-140158",
  volume    = "6",
  pages     = "539--564",
  journal   = "Semantic Web",
  issn      = "1570-0844",
  publisher = "IOS Press",
  number    = "6",
}


@book{874814a36ae64150a962b38011a681f1,
  title     = "The SMS platform - RISIS deliverable",
  author    = "{van den Besselaar}, P.A.A. and A. Khalili and A Idrissou and A. Loizou and {van Harmelen}, F.A.H.",
  year      = "2015",
  publisher = "Vrije Universiteit",
}


@article{3711c0eae7cb45b9bec21e26f594fb4b,
  title     = "Why the Data Train Needs Semantic Rails",
  abstract  = "While catchphrases such as big data, smart data, data-intensive science, or smart dust highlight different aspects, they share a common theme - namely, a shift toward a data-centered perspective in which the synthesis and analysis of data at an ever-increasing spatial, temporal, and thematic resolution promise new insights, while, at the same time, reduce the need for strong domain theories as starting points. In terms of the envisioned methodologies, those catchphrases tend to emphasize the role of predictive analytics, that is, statistical techniques including data mining and machine learning, as well as supercomputing. Interestingly, however, while this perspective takes the availability of data as a given, it does not answer the question how one would discover the required data in today's chaotic information universe, how one would understand which data sets can be meaningfully integrated, and how to communicate the results to humans and machines alike. The semantic web addresses these questions. In the following, we argue why the data train needs semantic rails. We point out that making sense of data and gaining new insights work best if inductive and deductive techniques go hand-in-hand instead of competing over the prerogative of interpretation.",
  author    = "Krzysztof Janowicz and Pascal Hitzler and Hendler, {James A.} and {van Harmelen}, Frank",
  year      = "2015",
  month     = "3",
  volume    = "36",
  pages     = "5--14",
  journal   = "The AI Magazine",
  issn      = "0738-4602",
  publisher = "AI Access Foundation",
  number    = "1",
}


@inbook{e7e1e9893fe14b078edbf1779f4b1c68,
  title     = "A Contextualised Semantics for owl: sameAs",
  abstract  = "Identity relations are at the foundation of the Semantic Web and the Linked Data Cloud. In many instances the classical interpretation of identity is too strong for practical purposes. This is particularly the case when two entities are considered the same in some but not all contexts. Unfortunately, modeling the specific contexts in which an identity relation holds is cumbersome and, due to arbitrary reuse and the Open World Assumption, it is impossible to anticipate all contexts in which an entity will be used. We propose an alternative semantics for owl:sameAs that partitions the original relation into a hierarchy of subrelations. The subrelation to which an identity statement belongs depends on the dataset in which the statement occurs. Adding future assertions may change the subrelation to which an identity statement belongs, resulting in a context-dependent and non-monotonic semantics. We show that this more fine-grained semantics is better able to characterize the actual use of owl:sameAs as observed in Linked Open Datasets.",
  author    = "W.G.J. Beek and K.S. Schlobach and {van Harmelen}, F.A.H.",
  year      = "2016",
  doi       = "10.1007/978-3-319-34129-3_25",
  isbn      = "9783319341286",
  volume    = "9678",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "405--419",
  booktitle = "The Semantic Web. Latest Advances and New Domains - 13th International Conference, ESWC 2016, Heraklion, Crete, Greece, May 29 - June 2, 2016, Proceedings",
}


@inbook{cec94fa27b524331a0c5f29e4d4c28c9,
  title     = "Adaptive Linked Data-Driven Web Components: Building Flexible and Reusable Semantic Web Interfaces - Building Flexible and Reusable Semantic Web Interfaces",
  abstract  = "Due to the increasing amount of Linked Data openly published on the Web, user-facing Linked Data Applications (LDAs) are gaining momentum. One of the major entrance barriers for Web developers to contribute to this wave of LDAs is the required knowledge of Semantic Web (SW) technologies such as the RDF data model and SPARQL query language. This paper presents an adaptive component-based approach together with its open source implementation for creating flexible and reusable SW interfaces driven by Linked Data. Linked Data-driven (LD-R) Web components abstract the complexity of the underlying SW technologies in order to allow reuse of existing Web components in LDAs, enabling Web developers who are not experts in SW to develop interfaces that view, edit and browse Linked Data. In addition to the modularity provided by the LD-R components, the proposed RDF-based configuration method allows application assemblers to reshape their user interface for different use cases, by either reusing existing shared configurations or by creating their proprietary configurations.",
  author    = "Ali Khalili and A. Loizou and {van Harmelen}, F.A.H.",
  year      = "2016",
  doi       = "10.1007/978-3-319-34129-3_41",
  isbn      = "978-3-319-34128-6",
  volume    = "9678",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer",
  pages     = "677--692",
  booktitle = "The Semantic Web. Latest Advances and New Domains - 13th International Conference, ESWC 2016, Heraklion, Crete, Greece, May 29 - June 2, 2016, Proceedings",
}


@misc{ca63a27c612148d2bf29a6b1283c8e58,
  title  = "A Deep Neural Network for Link Prediction on Knowledge Graphs",
  author = "W.X. Wilcke and {de Boer}, V. and {van Harmelen}, F.A.H. and {de Kleijn}, M.T.M.",
  year   = "2016",
}


@inbook{bd128ebb57df4bfdb31cc455c81cdbc6,
  title     = "Are names meaningful? Quantifying social meaning on the semantic web",
  abstract  = "According to its model-theoretic semantics, Semantic Web IRIs are individual constants or predicate letters whose names are chosen arbitrarily and carry no formal meaning. At the same time it is a well-known aspect of Semantic Web pragmatics that IRIs are often constructed mnemonically, in order to be meaningful to a human interpreter. The latter has traditionally been termed ‘social meaning’, a concept that has been discussed but not yet quantitatively studied by the Semantic Web community. In this paper we use measures of mutual information content and methods from statistical model learning to quantify the meaning that is (at least) encoded in Semantic Web names. We implement the approach and evaluate it over hundreds of thousands of datasets in order to illustrate its efficacy. Our experiments confirm that many Semantic Web names are indeed meaningful and, more interestingly, we provide a quantitative lower bound on how much meaning is encoded in names on a per-dataset basis. To our knowledge, this is the first paper about the interaction between social and formal meaning, as well as the first paper that uses statistical model learning as a method to quantify meaning in the Semantic Web context. These insights are useful for the design of a new generation of Semantic Web tools that take such social meaning into account.",
  author    = "{de Rooij}, Steven and Wouter Beek and Peter Bloem and {van Harmelen}, Frank and Stefan Schlobach",
  year      = "2016",
  doi       = "10.1007/978-3-319-46523-4_12",
  isbn      = "978-3-319-46522-7",
  volume    = "9981 LNCS",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "184--199",
  booktitle = "The Semantic Web - 15th International Semantic Web Conference, ISWC 2016, Proceedings",
}


@article{39ec163cbc9a41078f10c194ebcd2f3e,
  title     = "A Task-based Comparison of Linguistic and Semantic Document Retrieval Methods in the Medical Domain",
  abstract  = "Text-based and semantics-based methods are both studied intensively as methods for document retrieval. In order to gain insight in the respective merits of these two approaches, we have performed a controlled experiment where we executed a real-life task using both textbased and semantics-based techniques. To maximise the lessons that we could draw about the two approaches, we have performed an experiment where we used the same task (searching papers from the scientific literature needed for updating a medical guideline), the same test-case (updating the 2004 Dutch national breast-cancer guideline), the same gold standard (the updated 2012 Dutch national breast-cancer guideline) and the same corpus (PubMed). We then performed this task using two different methods: retrieving papers based on keywords (text-based approach) and retrieving papers based on semantic annotations (semantics-based approach). Based on this experiment, we discuss the insights that we gained from this dual set of experiments.",
  keywords  = "Concept-based search, Document retrieval, Keyword search, Relation-based search, Semantic annotation",
  author    = "Mohammad Shafahi and Qing Hu and Hamideh Afsarmanesh and Z. Huang and {ten Teije}, A.C.M. and {van Harmelen}, F.A.H.",
  year      = "2016",
  volume    = "1613",
  journal   = "CEUR workshop proceedings",
  issn      = "1613-0073",
  publisher = "CEUR Workshop Proceedings",
}


@inbook{91383e82af8f4f5c82634e58bfdbf443,
  title     = "A topic-centric approach to detecting new evidences for evidence-based medical guidelines",
  abstract  = "Evidence-based Medical guidelines are developed based on the best available evidence in biomedical science and clinical practice. Such evidence-based medical guidelines should be regularly updated, so that they can optimally serve medical practice by using the latest evidence from medical research. The usual approach to detect such new evidence is to use a set of terms from a guideline recommendation and to create queries for a biomedical search engine such as PubMed, with a ranking over a selected subset of terms to search for relevant new evidence. However, the terms that appear in a guideline recommendation do not always cover all of the information we need for the search, because the contextual information (e.g. time and location, user profile, topics) is usually missing in a guideline recommendation. Enhancing the search terms with contextual information would improve the quality of the search results. In this paper, we propose a topic-centric approach to detect new evidence for updating evidence-based medical guidelines as a context-aware method to improve the search. Our experiments show that this topic centric approach can find the goal evidence for 12 guideline statements out of 16 in our test set, compared with only 5 guideline statements that were found by using a non-topic centric approach.",
  keywords  = "Context-awareness, Evidence-based medical guidelines, Medical guideline update, Semantic distance, Topic-centric approach",
  author    = "Qing Hu and Zisheng Huang and {ten Teije}, Annette and {van Harmelen}, Frank and Marshall, {M. Scott} and Andre Dekker",
  year      = "2016",
  pages     = "282--289",
  booktitle = "HEALTHINF 2016 - 9th International Conference on Health Informatics, Proceedings; Part of 9th International Joint Conference on Biomedical Engineering Systems and Technologies, BIOSTEC 2016",
  publisher = "SciTePress",
}


@inbook{82b812259aed4390bb18653971bd6591,
  title     = "Generalizing the Detection of Internal and External Interactions in Clinical Guidelines",
  abstract  = "This paper presents a method for formally representing Computer-Interpretable Guidelines to deal with multimorbidity. Although some approaches for merging guidelines exist, improvements are still required for combining several sources of information and coping with possibly conflicting pieces of evidence coming from clinical studies. Our main contribution is twofold: (i) we provide general models and rules for representing guidelines that expresses evidence as causation beliefs; (ii) we introduce a mechanism to exploit external medical knowledge acquired from Linked Open Data (Drugbank, Sider, DIKB) to detect potential interactions between recommendations. We apply this framework to merge three guidelines (Osteoarthritis, Diabetes, and Hypertension) in order to illustrate the capability of this approach for detecting potential conflicts between guidelines and eventually propose alternatives.",
  keywords  = "Clinical guidelines, Knowledge representation, Ontologies, Semantic web",
  author    = "{Carretta Zamborlini}, Veruska and Rinke Hoekstra and {Da Silveira}, Marcos and Cedric Pruski and {ten Teije}, Annette and {van Harmelen}, Frank",
  year      = "2016",
  doi       = "10.5220/0005704101050116",
  pages     = "105--116",
  booktitle = "HEALTHINF 2016 - 9th International Conference on Health Informatics, Proceedings; Part of 9th International Joint Conference on Biomedical Engineering Systems and Technologies, BIOSTEC 2016",
  publisher = "SciTePress",
}


@article{4d178cf0055f4dd1acf70f17b8d01a25,
  title     = "Inferring recommendation interactions in clinical guidelines",
  abstract  = "The formal representation of clinical knowledge is still an open research topic. Classical representation languages for clinical guidelines are used to produce diagnostic and treatment plans. However, they have important limitations, e.g. when looking for ways to re-use, combine, and reason over existing clinical knowledge. These limitations are especially problematic in the context of multimorbidity; patients that suffer from multiple diseases. To overcome these limitations, this paper proposes a model for clinical guidelines (TMR4I) that allows the re-use and combination of knowledge from multiple guidelines. Semantic Web technology is applied to implement the model, allowing us to automatically infer interactions between recommendations, such as recommending the same drug more than once. It relies on an existing Linked Data set, DrugBank, for identifying drug-drug interactions. We evaluate the model by applying it to two realistic case studies on multimorbidity that combine guidelines for two (Duodenal Ulcer and Transient Ischemic Attack) and three diseases (Osteoarthritis, Hypertension and Diabetes) and compare the results with existing methods.",
  keywords  = "Clinical knowledge representation, OWL, SPARQL, SWRL, combining medical guidelines, multimorbidity, reasoning, rules",
  author    = "{Carretta Zamborlini}, Veruska and Rinke Hoekstra and {Da Silveira}, Marcos and Cedric Pruski and {ten Teije}, A.C.M. and {van Harmelen}, F.A.H.",
  year      = "2016",
  doi       = "10.3233/SW-150212",
  volume    = "7",
  pages     = "421--446",
  journal   = "Semantic Web",
  issn      = "1570-0844",
  publisher = "IOS Press",
  number    = "4",
}


@book{8575d1a8fc274801afefe62bcdff596a,
  title     = "Interim report on the disambiguation results: RISIS Deliverable D25.1",
  author    = "Stefan Schlobach and O.A.K. Idrissou and R.J. Hoekstra and A. Khalili and {van Harmelen}, Frank and {van den Besselaar}, P.A.A.",
  year      = "2016",
  publisher = "Vrije Universiteit",
}


@inbook{210cc82a653b4c559a54a863de63cad2,
  title     = "Knowledge Services Using Rule-Based Formalization for Eligibility Criteria of Clinical Trials",
  abstract  = "Rule-based formalization of eligibility criteria in clinical trials have distinguished features such as declaration, easy maintenance, reusability, and expressiveness. In this paper, we present several knowledge services which can be provided by the rule-based formalization of eligibility criteria. The rule-based formalization can be generated automatically by using the logic programming Prolog with the support of NLP tools for the semantic annotation and relation extraction with medical ontologies/terminologies such as UMLS and SNOMED CT. We show how those automatically generated rule-based formalization for eligibility criteria can be used for the patient recruitment service in SemanticCT, a semantically-enabled system for clinical trials.",
  author    = "Z. Huang and Q. Hu and {ten Teije}, A.C.M. and {van Harmelen}, F.A.H. and S. Ait-Mokhtar",
  year      = "2016",
  doi       = "10.1007/978-3-319-48335-1_6",
  isbn      = "9783319483344",
  volume    = "10038 LNCS",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "49--61",
  booktitle = "Health Information Science - 5th International Conference, HIS 2016, Proceedings",
}


@article{62ad2a6cd97a4be38502d5f84e54ec29,
  title     = "SCRY: Extending SPARQL with custom data processing methods for the life sciences",
  keywords  = "customization, data processing, rdf generation, SPARQL, extension",
  author    = "Bas Stringer and Albert Meroño-peñuela and Sanne Abeln and {van Harmelen}, Frank and Jaap Heringa",
  year      = "2016",
  volume    = "1795",
  pages     = "1--10",
  journal   = "CEUR workshop proceedings",
  issn      = "1613-0073",
  publisher = "CEUR Workshop Proceedings",
}


@inbook{6b8910eb91d746ed84e869023557eb2a,
  title     = "SMS: a linked open data infrastructure for science and innovation studies.",
  author    = "{van den Besselaar}, Peter and Ali Khalili and Oladele Idrissou and K.S. Schlobach and {van Harmelen}, F.A.H.",
  year      = "2016",
  pages     = "106--114",
  editor    = "Ismael Rafols",
  booktitle = "Peripheries, Frontiers and Beyond; proceedings of the 21st STI Conference",
  publisher = "University Valencia",
}


@article{aeeb6a77c22d4ecebcef47cedec27955,
  title     = "SWISH for prototyping clinical guideline interactions theory",
  abstract  = "SWISH provides a general purpose collaborative infrastructure for applying Prolog reasoning over an RDF dataset together with features that facilitates prototyping Semantic Web applications. In this paper we report on the use of SWISH for efficiently developing a prototype for detection of clinical guideline interactions. These guidelines are a set of medical recommendations meant for supporting doctors on tackling a single disease. However, often guidelines need to be combined for treating patients that suffer from multiple diseases, and then a number of interactions can occur. The generic interaction rules are implemented in SWI-Prolog and the guideline RDF-data is enriched with clinical Linked Open Data (LOD) (e.g. Drugbank, Sider). We show the implementation of the proposed theory about interaction detection in a case-study on combining three guidelines. The experiment is interactively described using a SWISH notebook and the results are graphical visualised empowered by graphviz.",
  keywords  = "Clinical guideline interactions, Multimorbidity, Prolog, RDF, SWISH",
  author    = "{Carretta Zamborlini}, Veruska and Jan Wielemaker and {Da Silveira}, Marcos and Cedric Pruski and {ten Teije}, Annette and {van Harmelen}, F.A.H.",
  year      = "2016",
  volume    = "1795",
  journal   = "CEUR workshop proceedings",
  issn      = "1613-0073",
  publisher = "CEUR Workshop Proceedings",
}


@misc{7559caac92f74c2381a30ea519bbe559,
  title  = "Towards an open data infrastructure for STI data. OECD Blue Sky Conference, Gent, September 2016",
  author = "{van den Besselaar}, Peter and Ali Khalili and {de Graaf}, {Klaas Andries} and Oladele Idrissou and A. Loizou and K.S. Schlobach and {van Harmelen}, F.A.H.",
  year   = "2016",
}


@article{9254330a91e749cdb89b65db174225b4,
  title     = "LOD Laundromat: Why the Semantic Web needs centralization (even if we don't like it)",
  abstract  = "LOD Laundromat poses a centralized solution for today's Semantic Web problems. This approach adheres more closely to the original vision of a Web of Data, providing uniform access to a large and ever-increasing subcollection of the LOD Cloud.",
  keywords  = "Internet/Web technologies, Linked Data, Linked Open Data, LOD, Semantic Web",
  author    = "Wouter Beek and Laurens Rietveld and Stefan Schlobach and {van Harmelen}, Frank",
  year      = "2016",
  month     = "3",
  doi       = "10.1109/MIC.2016.43",
  volume    = "20",
  pages     = "78--81",
  journal   = "IEEE Internet Computing",
  issn      = "1089-7801",
  publisher = "Institute of Electrical and Electronics Engineers Inc.",
  number    = "2",
}


@article{6e2aeddc45c14074974ab9577f40e126,
  title     = "Interview with Frank van Harmelen on {"}Linked Data and Business Information Systems{"}",
  author    = "Soeren Auer and {van Harmelen}, Frank",
  year      = "2016",
  month     = "10",
  doi       = "10.1007/s12599-016-0448-y",
  volume    = "58",
  pages     = "371--373",
  journal   = "Business & Information Systems Engineering",
  issn      = "2363-7005",
  publisher = "Springer Gabler",
  number    = "5",
}


@inbook{4c7885c87ca2424b84f30e0ce5fbf6b0,
  title     = "An empirical study on how the distribution of ontologies affects reasoning on the web",
  abstract  = "The Web of Data is an inherently distributed environment where ontologies are located in (physically) remote locations and are subject to constant changes. Reasoning is affected by these changes, but the extent and significance of this dependency is not well-studied yet. To address this problem, this paper presents an empirical study on how the distribution of ontological data on the Web affects the outcome of reasoning. We study (1) to what degree datasets depend on external ontologies and (2) to what extent the inclusion of additional ontological information via IRI de-referencing and the owl:imports directive to the input datasets leads to new derivations. We based our study on many RDF datasets and on a large collection of RDFa, and JSON-LD data embedded into HTML pages. We used both Jena and Pellet in order to evaluate the results under different semantics. Our results indicate that remote ontologies are often crucial to obtain non-trivial derivations. Unfortunately, in many cases IRIs were broken and the owl:imports is rarely used. Furthermore, in some cases the inclusion of remote knowledge either did not yield any additional derivation or led to errors. Despite these cases, in general, we found that inclusion of additional ontologies via IRIs de-referencing and owl:imports directive is very effective for producing new derivations. This indicates that the two W3C standards for fetching remote ontologies have found their way into practice.",
  keywords  = "JSON-LD, OWL, RDF, RDFa, Reasoning, Web of data",
  author    = "Bazoobandi, {Hamid R.} and Jacopo Urbani and {van Harmelen}, Frank and Henri Bal",
  year      = "2017",
  doi       = "10.1007/978-3-319-68288-4_5",
  isbn      = "9783319682877",
  volume    = "10587 LNCS",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "69--86",
  booktitle = "The Semantic Web – ISWC 2017 - 16th International Semantic Web Conference, Proceedings",
}


@book{fcb11c81af894034a33d5a6d705166b0,
  title     = "ARIADNE: Final Report on Data Mining",
  abstract  = "Recent years have witnessed a growing interest from archaeological communities in Linked Data. ARIADNE, the Advanced Research Infrastructure for Archaeological Data set Networking in Europe, facilitates a central web portal that provides access to archaeological data from various sources. Parts of these data have been being published as Linked Data, and are currently available in the Linked Open Data cloud. With it, the nature of these data has shifted from unstructured to structured. This presents new opportunities for data mining. While general-purpose software exists, recent studies have revealed the importance of two domain-specific requirements: 1) produce interpretable results, and 2) allow trust in the underlying model. In this work, we investigate to what extend interpretable data mining can contribute to the understanding of linked archaeological data. A case study was held, which involved the mining of semantic association rules over data sets of increasing levels of knowledgegranularity, followed by the qualitative evaluation of these rules by domain experts. Experiments have shown that the approach yielded mostly plausible patterns, some of which were seen as highly relevant.",
  author    = "W.X. Wilcke and {de Boer}, V. and {van Harmelen}, F.A.H. and {de Kleijn}, M.T.M. and M. Wansleeben and Harry Dimitropoulos and Holly Wright",
  year      = "2017",
  series    = "ARIADNE",
  publisher = "Ariadne",
  number    = "D16.3",
}


@inbook{db25399349794a8f86dce8ed0ec8ac69,
  title     = "Constructing disease-centric knowledge graphs: A case study for depression (short version)",
  abstract  = "In this paper we show how we used multiple large knowledge sources to construct a much smaller knowledge graph that is focussed on single disease (in our case major depression disorder). Such a disease-centric knowledge-graph makes it more convenient for doctors (in our case psychiatric doctors) to explore the relationship among various knowledge resources and to answer realistic clinical queries.",
  author    = "Zhisheng Huang and Jie Yang and {van Harmelen}, Frank and Qing Hu",
  year      = "2017",
  doi       = "10.1007/978-3-319-59758-4_5",
  isbn      = "9783319597577",
  volume    = "10259 LNAI",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "48--52",
  booktitle = "Artificial Intelligence in Medicine - 16th Conference on Artificial Intelligence in Medicine, AIME 2017, Proceedings",
}


@inbook{0a3a2ec2685448339a2ca93455e0e36f,
  title     = "Constructing Knowledge Graphs of Depression",
  abstract  = "Knowledge Graphs have been shown to be useful tools for integrating multiple medical knowledge sources, and to support such tasks as medical decision making, literature retrieval, determining healthcare quality indicators, co-morbodity analysis and many others. A large number of medical knowledge sources have by now been converted to knowledge graphs, covering everything from drugs to trials and from vocabularies to gene-disease associations. Such knowledge graphs have typically been generic, covering very large areas of medicine. (e.g. all of internal medicine, or arbitrary drugs, arbitrary trials, etc.). This has had the effect that such knowledge graphs become prohibitively large, hampering both efficiency for machines and usability for people. In this paper we show how we use multiple large knowledge sources to construct a much smaller knowledge graph that is focussed on single disease (in our case major depression disorder). Such a disease-centric knowledge-graph makes it more convenient for doctors (in our case psychiatric doctors) to explore the relationship among various knowledge resources and to answer realistic clinical queries (This paper is an extended version of [1].).",
  author    = "Zhisheng Huang and Jie Yang and {van Harmelen}, Frank and Qing Hu",
  year      = "2017",
  doi       = "10.1007/978-3-319-69182-4_16",
  isbn      = "9783319691817",
  volume    = "10594 LNCS",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "149--161",
  booktitle = "Health Information Science - 6th International Conference, HIS 2017, Proceedings",
}


@inbook{f0a401b2137b45e5b2d32d0149c3160a,
  title     = "Detecting New Evidences for Evidence-Based Medical Guidelines with Journal Filtering",
  abstract  = "Evidence-based medical guidelines are systematically developed recommendations with the aim to assist practitioner and patients decisions regarding appropriate health care for specific clinical circumstances, and are based on evidence described in medical research papers. Evidence-based medical guidelines should be regularly updated, such that they can serve medical practice using based on the latest medical research evidence. A usual approach to detecting new evidences is to use a set of terms which appear in a guideline conclusion or recommendation and create queries over a bio-medical search engine such as PubMed with a ranking over a selected subset of terms to search for relevant new research papers. However, the sizes of the found relevant papers are usually very large (i.e. over a few hundreds, even thousands), which results in a low precision of the search. This makes it for medical professionals quite difficult to find which papers are really interesting and useful for updating the guideline. We propose a filtering step to decrease the number of papers. More exactly we are interested in the question if we can reduce the number of papers with no or a slightly lower recall. A plausible approach is to introduce journal filtering, such that evidence appear in those top journals are preferred. In this paper, we extend our approach of detecting new papers for updating evidence-based medical guideline with a journal filtering step. We report our experiments and show that (1) the method with journal filtering can indeed gain a large reduction of the number of papers (69.73%) with a slightly lower recall (14.29%); (2) we show that the journal filtering method keeps relatively more high level evidence papers (category A) and removes all the low level evidence papers (category D).",
  author    = "Qing Hu and Zisheng Huang and {ten Teije}, Annette and {van Harmelen}, Frank",
  year      = "2017",
  doi       = "10.1007/978-3-319-55014-5_8",
  isbn      = "9783319550138",
  volume    = "10096 LNAI",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer/Verlag",
  pages     = "120--132",
  editor    = "David Riaño and Richard Lenz and Manfred Reichert",
  booktitle = "Knowledge Representation for Health Care: HEC 2016 International Joint Workshop, KR4HC/ProHealth 2016, Munich, Germany, September 2, 2016, Revised Selected Papers",
}


@misc{1db5e48c46a24fdfbe5f7395eeeaccec,
  title  = "Enriching scientometrics with linked data: the SMS platform",
  author = "O.A.K. Idrissou and A. Khalili and {van den Besselaar}, P.A.A. and {de Graaf}, K.A. and A. Loizou and {van Harmelen}, Frank",
  year   = "2017",
}


@inbook{68afba2d8f25485fa3ade85bf8a66553,
  title     = "Generalizing the Detection of Clinical Guideline Interactions Enhanced with LOD",
  author    = "{Carretta Zamborlini}, Veruska and Rinke Hoekstra and {Da Silveira}, Marcos and Cedric Pruski and {ten Teije}, Annette and {van Harmelen}, Frank",
  year      = "2017",
  doi       = "10.1007/978-3-319-54717-6_20",
  isbn      = "978-3-319-54717-6",
  pages     = "360--386",
  editor    = "Ana Fred and Hugo Gamboa",
  booktitle = "Biomedical Engineering Systems and Technologies: 9th International Joint Conference, BIOSTEC 2016, Rome, Italy, February 21--23, 2016, Revised Selected Papers",
  publisher = "Springer International Publishing Switzerland",
}


@misc{a48b60672af940c4a911566a973f08aa,
  title  = "Improving social research using heterogeneous data: the SMS platform",
  author = "{van den Besselaar}, P.A.A. and A. Khalili and {van Harmelen}, Frank and {de Graaf}, K.A.",
  year   = "2017",
}


@inbook{336aadbd02e346dcadb4630b4fe216fa,
  title     = "Is my:sameAs the same as your:sameAs?",
  abstract  = "Linking between entities in different datasets is a crucial element of the Semantic Web architecture, since those links allow us to integrate datasets without having to agree on a uniform vocabulary. However, it is widely acknowledged that the owl:sameAs construct is too blunt a tool for this purpose. It entails full equality between two resources independent of context. But whether or not two resources should be considered equal depends not only on their intrinsic properties, but also on the purpose or task for which the resources are used. We present a system for constructing contextspecific equality links. In a first step, our system generates a set of probable links between two given datasets. These potential links are decorated with rich metadata describing how, why, when and by whom they were generated. In a second step, a user then selects the links which are suited for the current task and context, constructing a context-specific “Lenticular Lens”. Such lenses can be combined using operators such as union, intersection, difference and composition. We illustrate and validate our approach with",
  author    = "Idrissou, {Al Koudous} and Rinke Hoekstra and {van Harmelen}, Frank and Ali Khalili and {van den Besselaar}, Peter",
  year      = "2017",
  booktitle = "The ninth international conference on knowledge capture: k-cap 2017",
}


@inbook{5d2c3965da59435382ceb720c06021ad,
  title     = "Knowledge-Driven Paper Retrieval to Support Updating of Clinical Guidelines",
  author    = "{Carretta Zamborlini}, Veruska and Qing Hu and Z. Huang and {Da Silveira}, Marcos and Cedric Pruski and {ten Teije}, A.C.M. and {van Harmelen}, F.A.H.",
  year      = "2017",
  doi       = "10.1007/978-3-319-55014-5_5",
  isbn      = "978-3-319-55014-5",
  pages     = "71--89",
  editor    = "David Riaño and Richard Lenz and Manfred Reichert",
  booktitle = "Knowledge Representation for Health Care: HEC 2016 International Joint Workshop, KR4HC/ProHealth 2016, Munich, Germany, September 2, 2016, Revised Selected Papers",
  publisher = "Springer International Publishing Switzerland",
}


@book{1539ee1ae92840a1beb2609e405321d0,
  title     = "Opening the SMS platform to users: Deliverable D7.2 - RISIS project",
  abstract  = "In this deliverable we describe the SMS (Semantically Mapping Science) data integration platform (http://sms.risis.eu), the technical core within the RISIS data infrastructure for Science, Technology and Innovation Studies (STI). The aim of the platform is to produce richer data to be used in social research – through the integration of heterogeneous datasets, ranging from tabular statistical data to unstructured data found on the Web. We outline the platform’s architecture and functions. There arealso some example use cases mentioned to show how the platform enables data integration in practice.",
  author    = "{van den Besselaar}, P.A.A. and A. Khalili and {de Graaf}, K.A. and O.A.K. Idrissou and {van Harmelen}, Frank",
  year      = "2017",
  publisher = "RISIS",
}


@inbook{1ea4f94b25b648a69d0416b2d937605c,
  title     = "Semantically Mapping Science (SMS) Platform",
  author    = "Ali Khalili and {van den Besselaar}, Peter and {Al Koudous}, Idrissou and {de Graaf}, {Klaas Andries} and {van Harmelen}, Frank",
  year      = "2017",
  pages     = "1--6",
  booktitle = "SemSci 2017: Enabling Open Semantic Science",
  publisher = "CEUR Workshop Proceedings",
}


@misc{61aa29f69da34a6f832f7f20ba0ac5ea,
  title  = "SMS: a platform for linking and enriching data for science and innovation studies: An example",
  author = "{van den Besselaar}, P.A.A. and A. Khalili and O.A.K. Idrissou and {de Graaf}, K.A. and {van Harmelen}, Frank",
  year   = "2017",
}


@article{c84bd6e3e40c418e85cea116fd2aacbf,
  title     = "Analyzing interactions on combining multiple clinical guidelines",
  abstract  = "Accounting for patients with multiple health conditions is a complex task that requires analysing potential interactions among recommendations meant to address each condition. Although some approaches have been proposed to address this issue, important features still require more investigation, such as (re)usability and scalability. To this end, this paper presents an approach that relies on reusable rules for detecting interactions among recommendations coming from various guidelines. It extends a previously proposed knowledge representation model (TMR) to enhance the detection of interactions and it provides a systematic analysis of relevant interactions in the context of multimorbidity. The approach is evaluated in a case study on rehabilitation of breast cancer patients, developed in collaboration with experts. The results are considered promising to support the experts in this task.",
  keywords  = "Clinical knowledge representation, Combining clinical guidelines, Comorbidity, Interactions among guidelines, Multimorbidity",
  author    = "Veruska Zamborlini and {Da Silveira}, Marcos and Cedric Pruski and {ten Teije}, Annette and Edwin Geleijn and {van der Leeden}, Marike and Martijn Stuiver and {van Harmelen}, Frank",
  year      = "2017",
  month     = "3",
  doi       = "10.1016/j.artmed.2017.03.012",
  journal   = "Artificial Intelligence in Medicine",
  issn      = "0933-3657",
  publisher = "Elsevier",
}
